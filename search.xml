<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[War包二次开发技巧]]></title>
    <url>%2Fjava%2Fwar%2Fjava%2Fwar%2F1%2F</url>
    <content type="text"><![CDATA[War包二次开发技巧概述近期拿到一个war包项目，里面没有认证功能，也没有源码。对于系统安全来说，直接能访问里面的数据，并对数据操作，是相当不安全的。所以想在项目里面增加一个简单的Basic认证功能，并且使用idea工具能够快速热部署开发。 Tomcat模式使用Tomcat来进行Basic认证控制新增config/tomcat-users.xml内容 123456789&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;tomcat-users xmlns="http://tomcat.apache.org/xml" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://tomcat.apache.org/xml tomcat-users.xsd" version="1.0"&gt; &lt;role rolename="test"/&gt; &lt;user username="root" password="000000" roles="test" /&gt;&lt;/tomcat-users&gt; 修改war中的web.xml 12345678910111213&lt;security-constraint&gt; &lt;web-resource-collection&gt; &lt;web-resource-name&gt;protected Resource&lt;/web-resource-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/web-resource-collection&gt; &lt;auth-constraint&gt; &lt;role-name&gt;test&lt;/role-name&gt; &lt;/auth-constraint&gt; &lt;/security-constraint&gt; &lt;login-config&gt; &lt;auth-method&gt;BASIC&lt;/auth-method&gt; &lt;realm-name&gt;Default&lt;/realm-name&gt; &lt;/login-config&gt; 这样就简单实现war项目新增Basic认证了 拦截器模式目录结构123456789101112131415161718report|- out # tomcat编译运行目录| |- artifacts| |- xx| |- xx_war_exploded|- src # 二次开发目录| |- main| |- java| |- com.xx.xx.safe.filter| |- HttpAuthBasicFilter.java| |- resources| |- http-auth-basic.properties|- target # 二次开发编译目录|- tomcat| |- webapps # 源码目录| |- WEB-INF | |- web.xml # 启动配置|- pom.xml # 二次开发pom 代码调整HttpAuthBasicFilter.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384public class HttpAuthBasicFilter implements Filter &#123; private static final String SECURITY_BASIC = "BASIC"; private Properties properties; public void init(FilterConfig filterConfig) throws ServletException &#123; InputStream in = HttpAuthBasicFilter.class.getClassLoader().getResourceAsStream("http-auth-basic.properties"); properties = new Properties(); try &#123; properties.load(in); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException &#123; HttpServletRequest req = (HttpServletRequest) servletRequest; HttpServletResponse resp = (HttpServletResponse) servletResponse; String sessionAuth = (String) req.getSession().getAttribute("auth"); if (!Strings.isNullOrEmpty(sessionAuth)) &#123; filterChain.doFilter(servletRequest, servletResponse); return; &#125; if(!checkHeaderAuth(req)) &#123; resp.setStatus(401); resp.setHeader("Cache-Control", "no-store"); resp.setDateHeader("Expires", 0); resp.setHeader("WWW-authenticate", "Basic Realm=\"test\""); return; &#125; req.getSession().setAttribute("auth", "true"); filterChain.doFilter(servletRequest, servletResponse); &#125; public void destroy() &#123; &#125; /** * 验证Http Header 中的Authorization是否正确 * @param request HttpServletRequest * @return boolean * @throws IOException */ private boolean checkHeaderAuth(HttpServletRequest request) throws IOException &#123; String auth = request.getHeader("Authorization"); if (Strings.isNullOrEmpty(auth)) &#123; return false; &#125; if (!auth.toUpperCase().startsWith(SECURITY_BASIC)) &#123; return false; &#125; String decodeVal = getFromBASE64(auth.substring(6)); String authBasicFormat = String.format("%s:%s" , properties.getProperty("BASIC_ACCOUNT") , properties.getProperty("BASIC_PASS")); if (!decodeVal.startsWith(authBasicFormat)) &#123; return false; &#125; return true; &#125; /** * 解析Authorization中的值 * @param s Authorization * @return Decode Authorization * @throws IOException */ private String getFromBASE64(String s) throws IOException &#123; BASE64Decoder decoder = new BASE64Decoder(); byte[] b = decoder.decodeBuffer(s); return new String(b); &#125;&#125; pom.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.eclipse.jetty.orbit&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet&lt;/artifactId&gt; &lt;version&gt;3.0.0.v201112011016&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;27.0.1-jre&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt; &lt;version&gt;3.1.1&lt;/version&gt; &lt;configuration&gt; &lt;outputDirectory&gt; tomcat/webapps/WEB-INF/lib/ &lt;/outputDirectory&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;!-- 内置打包法 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;2.3&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;createDependencyReducedPom&gt;false&lt;/createDependencyReducedPom&gt; &lt;transformers&gt; &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.AppendingTransformer&quot;&gt; &lt;resource&gt;META-INF/spring.handlers&lt;/resource&gt; &lt;/transformer&gt; &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.AppendingTransformer&quot;&gt; &lt;resource&gt;META-INF/spring.schemas&lt;/resource&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; web.xml 12345678&lt;filter&gt; &lt;filter-name&gt;HttpAuthBasicFilter&lt;/filter-name&gt; &lt;filter-class&gt;com.xxx.xx.safe.filter.HttpAuthBasicFilter&lt;/filter-class&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;HttpAuthBasicFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; IDEA设置现在代码都准备差不多了，我们开始准备idea的一些配置 1、首先将webapps中的lib包加入到你的项目中 2、配置Artifacts 3、配置tomcat和deployment 现在大功告成了，下面进行启动 首先启动tomcat为调试模式，因为我们要使用热部署，都懂 在你的二开工程中，进行package就可以打包到tomcat中进行自动依赖了]]></content>
      <categories>
        <category>java</category>
        <category>war</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>war</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ApiGateway整合Swagger2]]></title>
    <url>%2Fjava%2Fspring%2Fjava%2Fspring%2Fspring-cloud-gateway-1%2F</url>
    <content type="text"><![CDATA[ApiGateway整合Swagger2概述最近在项目中尝试使用Spring Cloud.Greenwich版整合Swagger2。发现Swagger并不支持以WebFlux为底层的Gateway，无法集成，运行报错。下面分享我的解决思路，和关键代码。 引入依赖1234567891011121314151617&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- Swagger2 --&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt; 核心代码配置SwaggerProvider，获取Api-doc，即SwaggerResources。SwaggerProvider.java 1234567891011121314151617181920212223242526272829303132333435363738@Component@Primary@AllArgsConstructorpublic class SwaggerProvider implements SwaggerResourcesProvider &#123; public static final String API_URI = "/v2/api-docs"; private final RouteLocator routeLocator; private final GatewayProperties gatewayProperties; @Override public List&lt;SwaggerResource&gt; get() &#123; List&lt;SwaggerResource&gt; resources = new ArrayList&lt;&gt;(); List&lt;String&gt; routes = new ArrayList&lt;&gt;(); //取出gateway的route routeLocator.getRoutes().subscribe(route -&gt; routes.add(route.getId())); //结合配置的route-路径(Path)，和route过滤，只获取有效的route节点 gatewayProperties.getRoutes() .stream() .filter(routeDefinition -&gt; routes.contains(routeDefinition.getId())) .forEach(routeDefinition -&gt; routeDefinition.getPredicates().stream() .filter(predicateDefinition -&gt; ("Path").equalsIgnoreCase(predicateDefinition.getName())) .forEach(predicateDefinition -&gt; &#123; resources.add(swaggerResource( routeDefinition.getId(), predicateDefinition.getArgs().get(NameUtils.GENERATED_NAME_PREFIX + "0").replace("/**", API_URI) )); &#125;)); return resources; &#125; private SwaggerResource swaggerResource(String name, String location) &#123; SwaggerResource swaggerResource = new SwaggerResource(); swaggerResource.setName(name); swaggerResource.setLocation(location); swaggerResource.setSwaggerVersion("1.0"); return swaggerResource; &#125;&#125; 因为Gateway里没有配置SwaggerConfig，而运行Swagger-ui又需要依赖一些接口，所以我的想法是自己建立相应的swagger-resource端点SwaggerHandler.java 1234567891011121314151617181920212223242526272829303132333435@RestController@ConditionalOnExpression("#&#123;'true'.equals(environment['neusoft.hype.swagger.enable'])&#125;")@RequestMapping("/swagger-resources")public class SwaggerHandler &#123; @Autowired(required = false) private SecurityConfiguration securityConfiguration; @Autowired(required = false) private UiConfiguration uiConfiguration; private final SwaggerResourcesProvider swaggerResources; @Autowired public SwaggerHandler(SwaggerResourcesProvider swaggerResources) &#123; this.swaggerResources = swaggerResources; &#125; @GetMapping("/configuration/security") public Mono&lt;ResponseEntity&lt;SecurityConfiguration&gt;&gt; securityConfiguration() &#123; return Mono.just(new ResponseEntity&lt;&gt;( Optional.ofNullable(securityConfiguration).orElse(SecurityConfigurationBuilder.builder().build()), HttpStatus.OK)); &#125; @GetMapping("/configuration/ui") public Mono&lt;ResponseEntity&lt;UiConfiguration&gt;&gt; uiConfiguration() &#123; return Mono.just(new ResponseEntity&lt;&gt;( Optional.ofNullable(uiConfiguration).orElse(UiConfigurationBuilder.builder().build()), HttpStatus.OK)); &#125; @GetMapping("") public Mono&lt;ResponseEntity&gt; swaggerResources() &#123; return Mono.just((new ResponseEntity&lt;&gt;(swaggerResources.get(), HttpStatus.OK))); &#125;&#125;]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kettle]]></title>
    <url>%2Fdwh%2Fetl%2Fdwh%2Fkettle-1%2F</url>
    <content type="text"><![CDATA[Kettle概述Kettle是一款国外开源的ETL工具，纯java编写，可以在Window、Linux、Unix上运行，绿色无需安装，数据抽取高效稳定。中文名称叫水壶，该项目的主程序员MATT 希望把各种数据放到一个壶里，然后以一种指定的格式流出。 产品家族Kettle家族目前包括4个产品：Spoon、Pan、CHEF、Kitchen SPOON 允许你通过图形界面来设计ETL转换过程（Transformation）。 PAN 允许你批量运行由Spoon设计的ETL转换 (例如使用一个时间调度器)。Pan是一个后台执行的程序，没有图形界面。 CHEF 允许你创建任务（Job）。 任务通过允许每个转换，任务，脚本等等，更有利于自动化更新数据仓库的复杂工作。任务通过允许每个转换，任务，脚本等等。任务将会被检查，看看是否正确地运行了。 KITCHEN 允许你批量使用由Chef设计的任务 (例如使用一个时间调度器)。KITCHEN也是一个后台运行的程序。 手动编译和运行准备工作 JDK1.8+ 下载源码：地址 版本号：pentaho-kettle-8.3.0.4-R Maven version 3+ settings.xml配置 编译在源码的根目录中执行，会等一段时间 1mvn clean install -DskipTests 运行编译成功后，会在assemblies/client/target生成一个pdi-ce-*.zip压缩包文件，解压后双击运行Spoon.bat就启动了。 问题 问题：编译时文件过大，通过maven无法下载文件 解决：可以根据拉取的地址进行直接下载，并发动maven本地仓库位置 问题：Spoon.bat启动直接未响应 解决：修改该文件中的-Xms，-Xmx 1if "%PENTAHO_DI_JAVA_OPTIONS%"=="" set PENTAHO_DI_JAVA_OPTIONS="-Xms512m" "-Xmx512m" "-XX:MaxPermSize=256m"]]></content>
      <categories>
        <category>dwh</category>
        <category>etl</category>
      </categories>
      <tags>
        <tag>dwh</tag>
        <tag>etl</tag>
        <tag>kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据仓库之ETL]]></title>
    <url>%2Fdwh%2Fetl%2Fdwh%2Fetl-1%2F</url>
    <content type="text"><![CDATA[数据仓库之ETL概述ETL，Extraction-Transformation-Loading的缩写，中文名称为数据抽取、转换和加载。一般随着业务的发展扩张，产线也越来越多，产生的数据也越来越多，这些数据的收集方式、原始数据格式、数据量、存储要求、使用场景等方面有很大的差异。作为数据中心，既要保证数据的准确性，存储的安全性，后续的扩展性，以及数据分析的时效性，这是一个很大的挑战。 名词解释 ODS——操作性数据 DW——数据仓库 DM——数据集市 数据抽取数据抽取是指把ODS源数据抽取到DW中，然后处理成展示给相关人员查看的数据 源数据： 用户访问日志 自定义事件日志、操作日志 业务日志 各服务产生的日志 系统日志：操作系统日志，CDN日志等 监控日志 其它日志 抽取频次： 如果没有特殊要求可以一天一次，但是需要避开拉去日志的高峰期 对于有实时性要求的日志，可以一小时一次，或者直接使用kafka等相关工具收集，需要考虑到系统能否承受 抽取策略： 由于数据量较大，一般都是采用增量抽取，但是对于一些特殊场景的数据，比如订单数据，由于订单的状态会发生变化，并且订单的量级是可预知和相对较少的，就需要采用全量拉取的策略 对于增量拉取的日志，如果是文件类型，可以在文件名称上追加日期，例如 server_log_2018082718.log，这样就可以满足按小时拉取的需求 对于源数据的保留，考虑到突发情况，服务器上的源数据至少要保证2天以上的时间 数据转换、清洗顾名思义，就是把不需要的和不符合规范的数据进行处理。数据清洗最好不要放在抽取的环节进行，考虑到有时可能会查原始数据。一般各公司都会有自己的规范，以下列出几点仅供参考 数据清洗主要包括以下几个方面： 空值处理；根据业务需要，可以将空值替换为特定的值或者直接过滤掉； 验证数据正确性；主要是把不符合业务含义的数据做一处理，比如，把一个表示数量的字段中的字符串替换为0，把一个日期字段的非日期字符串过滤掉等等； 规范数据格式；比如，把所有的日期都格式化成yyyy-MM-dd HH:mm:ss的格式等； 数据转码；把一个源数据中用编码表示的字段，通过关联编码表，转换成代表其真实意义的值等等； 数据标准，统一；比如在源数据中表示男女的方式有很多种，在抽取的时候，直接根据模型中定义的值做转化，统一表示男女； 其他业务规则定义的数据清洗… 数据加载数据拉取，清洗完之后，就需要展示了。一般是把清洗好的数据加载到mysql中，然后在各系统中使用，或者使用Tableau直接给相关人员展示 开源工具Talend易用性：有 GUI 图形界面但是以 Eclipse 的插件方式提供 技术支持：主要在美国 速度：需要手工调整，对特定数据源有优化知识 监控：有监控和日志工具 连接性：各种常用 数据库 ，文件， web，service Kettle易用性：有非常容易使用的 GUI，出现问题可以到社区咨询 技术支持：在美国，欧洲（比利时，德国，法国，英国），亚洲（ 中国 ，日本，韩国）都可以找到相关技术支持人员 速度：比 Talend 快，不过也需要手工调整，对 Oracle 和 PostGre 等数据源做了优化，同时也取决于转换任务的设计 监控：有监控和日志工具 连接性：非常广泛的数据库，文件，另外可以通过插件扩展 Informatica易用性：有非常容易使用的 GUI，但是要专门的训练 技术支持：遍布全世界 速度：是最快的 监控：有非常详细的监控和日志工具 连接性：各种数据源]]></content>
      <categories>
        <category>dwh</category>
        <category>etl</category>
      </categories>
      <tags>
        <tag>etl</tag>
        <tag>data-warehouse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis客户端选择]]></title>
    <url>%2Funcategorized%2Fspring%2Fredis-1%2F</url>
    <content type="text"><![CDATA[Redis 客户端的选择概念 Jedis：是老牌的Redis的Java实现客户端，提供了比较全面的Redis命令的支持， Redisson：实现了分布式和可扩展的Java数据结构。 Lettuce：高级Redis客户端，用于线程安全同步，异步和响应使用，支持集群，Sentinel，管道和编码器。 优缺点 Jedis：比较全面的提供了Redis的操作特性 Redisson：促使使用者对Redis的关注分离，提供很多分布式相关操作服务，例如，分布式锁，分布式集合，可通过Redis支持延迟队列 Lettuce：基于Netty框架的事件驱动的通信层，其方法调用是异步的。Lettuce的API是线程安全的，所以可以操作单个Lettuce连接来完成各种操作 可伸缩 Jedis：使用阻塞的I/O，且其方法调用都是同步的，程序流需要等到sockets处理完I/O才能执行，不支持异步。Jedis客户端实例不是线程安全的，所以需要通过连接池来使用Jedis。 Redisson：基于Netty框架的事件驱动的通信层，其方法调用是异步的。Redisson的API是线程安全的，所以可以操作单个Redisson连接来完成各种操作 Lettuce：基于Netty框架的事件驱动的通信层，其方法调用是异步的。Lettuce的API是线程安全的，所以可以操作单个Lettuce连接来完成各种操作，lettuce能够支持redis4，需要java8及以上。lettuce是基于netty实现的与redis进行同步和异步的通信。 比较 jedis使直接连接redis server,如果在多线程环境下是非线程安全的，这个时候只有使用连接池，为每个jedis实例增加物理连接 ； lettuce的连接是基于Netty的，连接实例（StatefulRedisConnection）可以在多个线程间并发访问，StatefulRedisConnection是线程安全的，所以一个连接实例可以满足多线程环境下的并发访问，当然这也是可伸缩的设计，一个连接实例不够的情况也可以按需增加连接实例。 Redisson实现了分布式和可扩展的Java数据结构，和Jedis相比，功能较为简单，不支持字符串操作，不支持排序、事务、管道、分区等Redis特性。Redisson的宗旨是促进使用者对Redis的关注分离，从而让使用者能够将精力更集中地放在处理业务逻辑上。 总结优先使用Lettuce，如果需要分布式锁，分布式集合等分布式的高级特性，添加Redisson结合使用，因为Redisson本身对字符串的操作支持很差。]]></content>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8s搭建-YAPI]]></title>
    <url>%2Fdocker%2Fk8s%2Fdocker%2Fk8s%2Fyapi-1%2F</url>
    <content type="text"><![CDATA[YAPI搭建概述YApi 是一个可本地部署的、打通前后端及QA的、可视化的接口管理平台 准备工作1、准备Yapi镜像 2、准备Mongo镜像 3、创建NFS共享文件 这些准备工作，都在我的博客中有写道，有疑问可以去找一下 http://blog.appydm.com 安装/启动(Mongo)Mongo-K8S配置mongo-nfs-pv.yaml文件内容 123456789101112131415161718192021222324apiVersion: v1kind: PersistentVolumemetadata: name: mongo-nfs-pvspec: capacity: storage: 10Gi accessModes: - ReadWriteOnce nfs: path: /nfs/data/mongo server: 172.24.2.70---apiVersion: v1kind: PersistentVolumeClaimmetadata: name: mongo-nfs-pvcspec: storageClassName: "" accessModes: - ReadWriteOnce resources: requests: storage: 10Gi mongo.yaml文件内容 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960kind: ServiceapiVersion: v1metadata: labels: app: mongo name: mongospec: ports: - port: 19098 targetPort: 27017 selector: app: mongo---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: mongo labels: app: mongospec: replicas: 1 template: metadata: labels: app: mongo spec: initContainers: - name: 'busyboxplus' image: registry.cn-shenzhen.aliyuncs.com/zhouqi-kubernetes/busyboxplus:v1 command: [ "sh", "-c", "nslookup mysql-system.default.svc.cluster.local" ] containers: - name: mongo image: mongo imagePullPolicy: Always resources: limits: cpu: 2 memory: 2Gi requests: memory: "512Mi" cpu: "200m" ports: - containerPort: 27017 env: - name: "MONGO_INITDB_ROOT_USERNAME" value: "root" - name: "MONGO_INITDB_ROOT_PASSWORD" value: "48660960" volumeMounts: - name: datadir mountPath: /data/db subPath: data volumes: - name: datadir persistentVolumeClaim: claimName: mongo-nfs-pvc Mongo-启动1kubectl apply -f mongo-nfs-pv.yaml 1kubectl apply -f mongo.yaml 安装/启动(YAPI)创建YAPI用户和数据库进入mongo容器中，执行下面命令 1234567891011root@mongo-69f8cbc957-xw47b:/# mongoMongoDB shell version v4.0.10connecting to: mongodb://127.0.0.1:27017/?gssapiServiceName=mongodbImplicit session: session &#123; "id" : UUID("d333ace7-d3e7-40bd-9d41-81a72b12771d") &#125;MongoDB server version: 4.0.10&gt; use adminswitched to db admin&gt; db.auth("root", "48660960")1&gt; use yapi&gt; db.createUser(&#123;user:"yapi",pwd:"yapi",roles:[&#123;"role":"readWrite","db":"yapi"&#125;]&#125;) 创建YAPI的config.json配置12345678910111213141516171819202122232425cat &lt;&lt; EOF &gt; config.json&gt; &#123; "port": "3000", "adminAccount": "admin@admin.com", "db": &#123; "servername": "mongo.default.svc.cluster.local", "DATABASE": "yapi", "port": 19098, "user": "yapi", "pass": "yapi", "authSource": "" &#125;, "mail": &#123; "enable": true, "host": "smtp.163.com", "port": 465, "from": "***@163.com", "auth": &#123; "user": "***@163.com", "pass": "*****" &#125; &#125;&#125;&gt; EOF YAPI-K8S配置yapi.yaml文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061kind: ServiceapiVersion: v1metadata: labels: app: yapi name: yapispec: ports: - port: 19080 targetPort: 3000 selector: app: yapi---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: yapi labels: app: yapispec: replicas: 1 template: metadata: labels: app: yapi spec: initContainers: - name: 'busyboxplus' image: registry.cn-shenzhen.aliyuncs.com/zhouqi-kubernetes/busyboxplus:v1 command: [ "sh", "-c", "nslookup mongo.default.svc.cluster.local" ] containers: - name: yapi image: registry.cn-shenzhen.aliyuncs.com/zhouqi-kubernetes/yapi:v1.7.2 imagePullPolicy: Always resources: limits: cpu: 1 memory: 1Gi requests: memory: "512Mi" cpu: "200m" ports: - containerPort: 3000 command: [ "/bin/bash", "-c", "[ ! -e /home/yapi/log/init.lock ] &amp;&amp; npm run install-server &amp;&amp; touch /home/yapi/log/init.lock; npm run start" ] volumeMounts: - name: yapi-nfs mountPath: /home/yapi/config.json subPath: config.json volumes: - name: yapi-nfs nfs: server: 172.24.2.71 path: /nfs/data/yapi YAPI-启动1kubectl apply -f yapi.yaml]]></content>
      <categories>
        <category>docker</category>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8s搭建-CoreDns]]></title>
    <url>%2Fdocker%2Fk8s%2Fdocker%2Fk8s%2FcoreDns-1%2F</url>
    <content type="text"><![CDATA[CoreDNS简介CoreDNS 其实就是一个 DNS 服务，而 DNS 作为一种常见的服务发现手段，所以很多开源项目以及工程师都会使用 CoreDNS 为集群提供服务发现的功能，Kubernetes 就在集群中使用 CoreDNS 解决服务发现的问题。 如果想要在分布式系统实现服务发现的功能，CoreDNS 其实是一个非常好的选择，CoreDNS作为一个已经进入CNCF并且在Kubernetes中作为DNS服务使用的应用，其本身的稳定性和可用性已经得到了证明，同时它基于插件实现的方式非常轻量并且易于使用，插件链的使用也使得第三方插件的定义变得非常的方便。 Coredns 架构整个 CoreDNS 服务都建立在一个使用 Go 编写的 HTTP/2 Web 服务器 Caddy 。 Coredns 项目下载下载地址1： wget https://github.com/coredns/deployment/archive/master.zipunzip master.zip 下载地址2：git clone https://github.com/coredns/deployment.git 安装部署确认是否存在已运行dns服务 1kubectl get pods -o wide -n=kube-system 删除命令1kubectl delete --namespace=kube-system deployment ****-dns 生成安装配置文件12cd /workspace/deployment/kubernetes./deploy.sh -r 10.254.0.0/16 -i 10.254.0.2 -d cluster.local -t coredns.yaml.sed -s &gt;coredns.yaml 验证配置文件核心配置1cat coredns.yaml 执行安装 1kubectl create -f coredns.yaml 验证安装 12345678kubectl get svc -o wide -n=kube-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) heapster ClusterIP 10.109.196.102 &lt;none&gt; 80/TCP kube-dns ClusterIP 10.96.0.2 &lt;none&gt; 53/UDP,53/TCP,9153/TCPkubernetes-dashboard NodePort 10.110.231.202 &lt;none&gt; 443:30001/TCP monitoring-grafana NodePort 10.98.242.145 &lt;none&gt; 80:30108/TCP monitoring-influxdb ClusterIP 10.100.60.54 &lt;none&gt; 8086/TCP 查看coredns详细 设置master和节点DNS使用命令查看kubelet配置的位置 1systemctl status kubelet -l 修改/var/lib/kubelet/config.yaml文件的内容 123clusterDNS:- 10.96.0.2clusterDomain: cluster.local. 重启 12systemctl daemon-reloadsystemctl restart kubelet 验证DNS1、创建一个curl 1kubectl run -it --image=registry.cn-shenzhen.aliyuncs.com/zhouqi-kubernetes/busyboxplus:v1 curl --rm 这个地方如果要验证各节点，可以伸缩多个，会运行在不同的节点中 对节点的验证，只需要进入容器使用nslookup kubernetes命令，就可以显示下图 文献参考：https://blog.51cto.com/michaelkang/2367800]]></content>
      <categories>
        <category>docker</category>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8s搭建-Zookeeper集群]]></title>
    <url>%2Fdocker%2Fk8s%2Fdocker%2Fk8s%2Fzookeeper-1%2F</url>
    <content type="text"><![CDATA[K8S搭建Zookeeper集群服务器资源 服务器地址 k8s NFS 172.24.2.67 k8s-node-3 service 172.24.2.69 k8s-master client 172.24.2.70 k8s-node-2 client 172.24.2.71 k8s-node-1 client 安装NFS1、在67服务器中安装NFS 123456789101112131415161718192021222324252627#master节点安装nfsyum -y install nfs-utils#创建nfs目录mkdir -p /nfs/data/#修改权限chmod -R 777 /nfs/data#编辑export文件,这个文件就是nfs默认的配置文件vi /etc/exports/nfs/data *(rw,no_root_squash,sync)#配置生效exportfs -r#查看生效exportfs#启动rpcbind、nfs服务systemctl restart rpcbind &amp;&amp; systemctl enable rpcbindsystemctl restart nfs &amp;&amp; systemctl enable nfs#查看 RPC 服务的注册状况rpcinfo -p localhost#showmount测试showmount -e 172.24.2.67 2、所有node节点安装客户端，开机启动 12yum -y install nfs-utilssystemctl start nfs &amp;&amp; systemctl enable nfs 部署NSF-Zookeeper-PV1、新建一个zk-nfs-pv.yaml文件，内容如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950apiVersion: v1kind: PersistentVolumemetadata: name: zk-nfs-pv-01 labels: pv: zk-nfs-pv-01spec: capacity: storage: 3Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: zk-nfs nfs: path: /nfs/data/zookeeper/pv-01 server: 172.24.2.67--- apiVersion: v1kind: PersistentVolumemetadata: name: zk-nfs-pv-02 labels: pv: zk-nfs-pv-02spec: capacity: storage: 3Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: zk-nfs nfs: path: /nfs/data/zookeeper/pv-02 server: 172.24.2.67---apiVersion: v1kind: PersistentVolumemetadata: name: zk-nfs-pv-03 labels: pv: zk-nfs-pv-03spec: capacity: storage: 3Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: zk-nfs nfs: path: /nfs/data/zookeeper/pv-03 server: 172.24.2.67 2、启动zk-nfs-pv 1kubectl create -f zk-nfs-pv.yaml 3、查看状态 部署Zookeeper集群1、新建文件zookeeper.yaml，内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183apiVersion: v1kind: Servicemetadata: name: zk-headless labels: app: zk-headless spec: ports: - port: 2888 name: server - port: 3888 name: leader-election clusterIP: None selector: app: zk---apiVersion: v1kind: Servicemetadata: name: zk-cs labels: app: zkspec: ports: - port: 2181 name: client nodePort: 30002 selector: app: zk type: NodePort ---apiVersion: v1kind: ConfigMapmetadata: name: zk-configdata: ensemble: "zk-0;zk-1;zk-2" jvm.heap: "2G" tick: "2000" init: "10" sync: "5" client.cnxns: "60" snap.retain: "3" purge.interval: "1"---apiVersion: policy/v1beta1kind: PodDisruptionBudgetmetadata: name: zk-budgetspec: selector: matchLabels: app: zk minAvailable: 2---apiVersion: apps/v1kind: StatefulSetmetadata: name: zkspec: selector: matchLabels: app: zk serviceName: zk-headless replicas: 3 template: metadata: labels: app: zk spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: "app" operator: In values: - zk topologyKey: "kubernetes.io/hostname" containers: - name: kubernetes-zookeeper imagePullPolicy: Always image: "registry.cn-shenzhen.aliyuncs.com/zhouqi-kubernetes/k8szk:v3" resources: limits: cpu: 1 memory: 1Gi requests: memory: 512Mi cpu: 500m ports: - containerPort: 2181 name: client - containerPort: 2888 name: server - containerPort: 3888 name: leader-election env: - name : ZK_REPLICAS value: "3" - name : ZK_ENSEMBLE valueFrom: configMapKeyRef: name: zk-config key: ensemble - name : ZK_HEAP_SIZE valueFrom: configMapKeyRef: name: zk-config key: jvm.heap - name : ZK_TICK_TIME valueFrom: configMapKeyRef: name: zk-config key: tick - name : ZK_INIT_LIMIT valueFrom: configMapKeyRef: name: zk-config key: init - name : ZK_SYNC_LIMIT valueFrom: configMapKeyRef: name: zk-config key: tick - name : ZK_MAX_CLIENT_CNXNS valueFrom: configMapKeyRef: name: zk-config key: client.cnxns - name: ZK_SNAP_RETAIN_COUNT valueFrom: configMapKeyRef: name: zk-config key: snap.retain - name: ZK_PURGE_INTERVAL valueFrom: configMapKeyRef: name: zk-config key: purge.interval - name: ZK_CLIENT_PORT value: "2181" - name: ZK_SERVER_PORT value: "2888" - name: ZK_ELECTION_PORT value: "3888" command: - sh - -c - zkGenConfig.sh &amp;&amp; zkServer.sh start-foreground readinessProbe: exec: command: - "zkOk.sh" initialDelaySeconds: 10 timeoutSeconds: 5 livenessProbe: exec: command: - "zkOk.sh" initialDelaySeconds: 10 timeoutSeconds: 5 volumeMounts: - name: datadir mountPath: /var/lib/zookeeper subPath: data/ - name: datadir mountPath: /opt/zookeeper/conf subPath: config/ securityContext: runAsUser: 1000 fsGroup: 1000 volumeClaimTemplates: - metadata: name: datadir annotations: volume.beta.kubernetes.io/storage-class: "zk-nfs" spec: accessModes: [ "ReadWriteOnce" ] resources: requests: storage: 3Gi 2、启动zookeeper.yaml 1kubectl create -f zookeeper.yaml 3、查看启动情况 1kubectl get pod -o wide 4、最后来验证Zookeeper集群是否正常，查看集群节点状态 1for i in 0 1 2; do kubectl exec zk-$i zkServer.sh status; done 参考文献：https://www.jianshu.com/p/2633b95c244c]]></content>
      <categories>
        <category>docker</category>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8s-Dashboard安装]]></title>
    <url>%2Fdocker%2Fk8s%2Fdocker%2Fk8s%2Fdashboard-1%2F</url>
    <content type="text"><![CDATA[安装Kubernetes-dashboard概述Kubernetes Dashboard是Kubernetes集群的基于Web的通用UI。它允许用户管理在群集中运行的应用程序并对其进行故障排除，以及管理群集本身。 安装在master节点上进行如下操作 1、创建Dashboard的yaml文件，并设置端口为30001，拉取镜像文件地址 123wget https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yamlsed -i 's/k8s.gcr.io/loveone/g' kubernetes-dashboard.yamlsed -i '/targetPort:/a\ \ \ \ \ \ nodePort: 30001\n\ \ type: NodePort' kubernetes-dashboard.yaml 2、部署Dashboard 1kubectl create -f kubernetes-dashboard.yaml 3、创建完成后，检查相关服务运行状态 1234567kubectl get deployment kubernetes-dashboard -n kube-systemkubectl get pods -n kube-system -o widekubectl get services -n kube-systemnetstat -ntlp|grep 30001 4、查看访问Dashboard的认证令牌 123kubectl create serviceaccount dashboard-admin -n kube-systemkubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-adminkubectl describe secrets -n kube-system $(kubectl -n kube-system get secret | awk '/dashboard-admin/&#123;print $1&#125;') 问题集合 问题：Chrome无法访问 解决：chrome.exe” 在快捷方式中增加 –disable-infobars –ignore-certificate-errors 问题：Master的Token过期 使用命令：kubeadm token create重新生成Token在加入 问题：子节点加入后出现NotReady 在master中重启docker，方可解决]]></content>
      <categories>
        <category>docker</category>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K8s安装]]></title>
    <url>%2Fdocker%2Fk8s%2Fdocker%2Fk8s%2F1%2F</url>
    <content type="text"><![CDATA[K8S安装Kubernetes支持在物理服务器或虚拟机中运行，本次使用虚拟机准备测试环境，硬件配置信息如表所示 IP地址 节点角色 Hostname 172.24.2.69 master k8s-master 172.24.2.71 worker k8s-node-1 172.24.2.70 worker k8s-node-2 环境准备1、设置主机名hostname，管理节点设置主机名为 k8s-master 。 1hostnamectl set-hostname k8s-master 2、关闭防火墙、selinux和swap 123456systemctl stop firewalldsystemctl disable firewalldsetenforce 0sed -i "s/^SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/configswapoff -ased -i 's/.*swap.*/#&amp;/' /etc/fstab 3、配置内核参数，将桥接的IPv4流量传递到iptables的链 123456cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt;EOFnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl --system 4、配置国内yum源 123456789yum install -y wgetmkdir /etc/yum.repos.d/bak &amp;&amp; mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/bakwget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repowget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repoyum clean all &amp;&amp; yum makecache 5、配置国内Kubernetes源 12345678910cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF 6、配置 docker 源 1wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo 软件安装1、安装docker 12345yum install docker-ce-18.06.1.ce-3.el7systemctl enable docker &amp;&amp; systemctl start dockerdocker --version 2、安装kubeadm、kubelet、kubectl 123yum install kubelet-1.14.3-0 kubeadm-1.14.3-0 kubectl-1.14.3-0systemctl enable kubelet 部署启动1、在master进行Kubernetes集群初始化 1kubeadm init --kubernetes-version=1.14.3 --image-repository registry.aliyuncs.com/google_containers --pod-network-cidr=10.244.0.0/16 2、配置kubectl工具 12345mkdir -p /root/.kubecp /etc/kubernetes/admin.conf /root/.kube/configkubectl get nodeskubectl get cs 3、部署flannel网络 1kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml 子节点部署1、设置子节点的名称 1hostnamectl set-hostname k8s-node-1 2、子节点可以继续执行主节点中（环境准备、软件安装） 3、将子节点加入到主节点中 12kubeadm join 172.24.2.69:6443 --token 94zry6.lzob4h452am02f0d \ --discovery-token-ca-cert-hash sha256:bf8e4fcdbf4001cafe5ab5325e429dcb3f8a52a836d0a6b80f683db506611043 4、如果当token过期了，可以使用下面命令重新生成token1kubeadm token create 集群状态检查在master节点输入命令检查集群状态，返回如下结果则集群状态正常 123456kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-master Ready master 7d6h v1.14.3k8s-node-1 Ready &lt;none&gt; 7d5h v1.14.3k8s-node-2 Ready &lt;none&gt; 100s v1.14.3 参考文献：https://www.kubernetes.org.cn/5462.html]]></content>
      <categories>
        <category>docker</category>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring OAuth2基础]]></title>
    <url>%2Fjava%2Fspring%2Fspring%2Fspring-oauth2%2F</url>
    <content type="text"><![CDATA[Spring OAuth2.0 提供者实现原理Spring OAuth2.0提供者实际上分为 授权服务 Authorization Service. 资源服务 Resource Service. 虽然这两个提供者有时候可能存在同一个应用程序中，但在Spring Security OAuth中你可以把他它们各自放在不同的应用上，而且你可以有多个资源服务，它们共享同一个中央授权服务。 配置一个授权服务必须要实现的endpoints： AuthorizationEndpoint：用来作为请求者获得授权的服务，默认的URL是/oauth/authorize. TokenEndpoint：用来作为请求者获得令牌（Token）的服务，默认的URL是/oauth/token. 配置一个资源服务必须要实现的过滤器： OAuth2AuthenticationProcessingFilter：用来作为认证令牌（Token）的一个处理流程过滤器。只有当过滤器通过之后，请求者才能获得受保护的资源 授权服务配置配置一个授权服务，你需要考虑几种授权类型（Grant Type），不同的授权类型为客户端（Client）提供了不同的获取令牌（Token）方式，为了实现并确定这几种授权，需要配置使用 ClientDetailsService 和 TokenService 来开启或者禁用这几种授权机制。到这里就请注意了，不管你使用什么样的授权类型（Grant Type），每一个客户端（Client）都能够通过明确的配置以及权限来实现不同的授权访问机制。这也就是说，假如你提供了一个支持”client_credentials”的授权方式，并不意味着客户端就需要使用这种方式来获得授权。 authorization_code：授权码类型。 implicit：隐式授权类型。 password：资源所有者（即用户）密码类型。 client_credentials：客户端凭据（客户端ID以及Key）类型。 refresh_token：通过以上授权获得的刷新令牌来获取新的令牌。 配置客户端详情信息ClientDetailsServiceConfigurer (AuthorizationServerConfigurer 的一个回调配置项，见上的概述) 能够使用内存或者JDBC来实现客户端详情服务（ClientDetailsService），有几个重要的属性如下列表： clientId：（必须的）用来标识客户的Id。 secret：（需要值得信任的客户端）客户端安全码，如果有的话。 scope：用来限制客户端的访问范围，如果为空（默认）的话，那么客户端拥有全部的访问范围。 authorizedGrantTypes：此客户端可以使用的授权类型，默认为空。 authorities：此客户端可以使用的权限（基于Spring Security authorities）。 配置授权类型授权是使用 AuthorizationEndpoint 这个端点来进行控制的，你能够使用 AuthorizationServerEndpointsConfigurer 这个对象的实例来进行配置(AuthorizationServerConfigurer 的一个回调配置项，见上的概述) ，如果你不进行设置的话，默认是除了资源所有者密码（password）授权类型以外，支持其余所有标准授权类型的（RFC6749），我们来看一下这个配置对象有哪些属性可以设置吧，如下列表： authenticationManager：认证管理器，当你选择了资源所有者密码（password）授权类型的时候，请设置这个属性注入一个 AuthenticationManager 对象。 userDetailsService：如果啊，你设置了这个属性的话，那说明你有一个自己的 UserDetailsService 接口的实现，或者你可以把这个东西设置到全局域上面去（例如 GlobalAuthenticationManagerConfigurer 这个配置对象），当你设置了这个之后，那么 “refresh_token” 即刷新令牌授权类型模式的流程中就会包含一个检查，用来确保这个账号是否仍然有效，假如说你禁用了这个账户的话。 authorizationCodeServices：这个属性是用来设置授权码服务的（即 AuthorizationCodeServices 的实例对象），主要用于 “authorization_code” 授权码类型模式。 implicitGrantService：这个属性用于设置隐式授权模式，用来管理隐式授权模式的状态。 tokenGranter：这个属性就很牛B了，当你设置了这个东西（即 TokenGranter 接口实现），那么授权将会交由你来完全掌控，并且会忽略掉上面的这几个属性，这个属性一般是用作拓展用途的，即标准的四种授权模式已经满足不了你的需求的时候，才会考虑使用这个。 配置授权端点的URLAuthorizationServerEndpointsConfigurer 这个配置对象(AuthorizationServerConfigurer 的一个回调配置项，见上的概述) 有一个叫做 pathMapping() 的方法用来配置端点URL链接，它有两个参数： 第一个参数：String 类型的，这个端点URL的默认链接。 第二个参数：String 类型的，你要进行替代的URL链接。 以上的参数都将以 “/“ 字符为开始的字符串，框架的默认URL链接如下列表，可以作为这个 pathMapping() 方法的第一个参数： /oauth/authorize：授权端点。 /oauth/token：令牌端点。 /oauth/confirm_access：用户确认授权提交端点。 /oauth/error：授权服务错误信息端点。 /oauth/check_token：用于资源服务访问的令牌解析端点。 资源服务配置一个资源服务（可以和授权服务在同一个应用中，当然也可以分离开成为两个不同的应用程序）提供一些受token令牌保护的资源，Spring OAuth提供者是通过Spring Security authentication filter 即验证过滤器来实现的保护，你可以通过 @EnableResourceServer 注解到一个 @Configuration 配置类上，并且必须使用 ResourceServerConfigurer 这个配置对象来进行配置（可以选择继承自 ResourceServerConfigurerAdapter 然后覆写其中的方法，参数就是这个对象的实例），下面是一些可以配置的属性： tokenServices：ResourceServerTokenServices 类的实例，用来实现令牌服务。 resourceId：这个资源服务的ID，这个属性是可选的，但是推荐设置并在授权服务中进行验证。 其他的拓展属性例如 tokenExtractor 令牌提取器用来提取请求中的令牌。 请求匹配器，用来设置需要进行保护的资源路径，默认的情况下是受保护资源服务的全部路径。 受保护资源的访问规则，默认的规则是简单的身份验证（plain authenticated）。 其他的自定义权限保护规则通过 HttpSecurity 来进行配置。]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>oauth2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security基础]]></title>
    <url>%2Fjava%2Fspring%2Fspring%2Fspring-security-1%2F</url>
    <content type="text"><![CDATA[最近公司项目使用了Spring Security，下面整理一下基础知识 概述一个能够为基于Spring的企业应用系统提供声明式的安全訪问控制解决方式的安全框架（简单说是对访问权限进行控制嘛），应用的安全性包括用户认证（Authentication）和用户授权（Authorization）两个部分。用户认证指的是验证某个用户是否为系统中的合法主体，也就是说用户能否访问该系统。用户认证一般要求用户提供用户名和密码。系统通过校验用户名和密码来完成认证过程。用户授权指的是验证某个用户是否有权限执行某个操作。在一个系统中，不同用户所具有的权限是不同的。比如对一个文件来说，有的用户只能进行读取，而有的用户可以进行修改。一般来说，系统会为不同的用户分配不同的角色，而每个角色则对应一系列的权限。 spring security的主要核心功能为 认证和授权，所有的架构也是基于这两个核心功能去实现的。 框架原理众所周知 想要对对Web资源进行保护，最好的办法莫过于Filter，要想对方法调用进行保护，最好的办法莫过于AOP。所以springSecurity在我们进行用户认证以及授予权限的时候，通过各种各样的拦截器来控制权限的访问，从而实现安全。 如下为其主要过滤器 名称 功能 ChannelProcessingFilter 可根据配置进行协议的重定向（HTTP与HTTPS） SecurityContextPersistenceFilter 针对每个web请求开始时加载SecurityContext，并在web请求结束时将SecurityContext保存到HttpSession中 ConcurrentSessionFilter 功能一，刷新session中最后更新时间；功能二，取session信息看是否过期，如果过期执行登出操作。 UsernamePasswordAuthenticationFilter、CasAuthenticationFilter、BasicAuthenticationFilter、其他 根据需要选择一个合适的认证过滤器 SecurityContextHolderAwareRequestFilter servlet相关实现的过滤器 JaasApiIntegrationFilter Jaas相关过滤器 RememberMeAuthenticationFilter 提供“记住我”功能的过滤器 AnonymousAuthenticationFilter 匿名授权过滤器 ExceptionTranslationFilter 捕获（Spring Security）产生的异常 FilterSecurityInterceptor URL的控制及访问拒绝时产生异常 框架的核心组件 名称 功能 SecurityContextHolder 提供对SecurityContext的访问 SecurityContext 持有Authentication对象和其他可能需要的信息 AuthenticationManager 其中可以包含多个AuthenticationProvider ProviderManager 对象为AuthenticationManager接口的实现类 AuthenticationProvider 主要用来进行认证操作的类 调用其中的authenticate()方法去进行认证操作 Authentication Spring Security方式的认证主体 GrantedAuthority 对认证主题的应用层面的授权，含当前用户的权限信息，通常使用角色表示 UserDetails 构建Authentication对象必须的信息，可以自定义，可能需要访问DB得到 UserDetailsService 通过username构建UserDetails对象，通过loadUserByUsername根据userName获取UserDetail对象 （可以在这里基于自身业务进行自定义的实现 如通过数据库，xml,缓存获取等） HttpSecurity使用 名称 功能 antMatchers(“/resources/**”, “/signup”, “/about”).permitAll() 任何用户都可以访问以”/resources/“,”/signup”, 或者 “/about”开头的URL。 antMatchers(“/admin/**”).hasRole(“ADMIN”) 以 “/admin/“ 开头的URL只能让拥有 “ROLE_ADMIN”角色的用户访问。 antMatchers(“/db/**”).access(“hasRole(‘ADMIN’) and hasRole(‘DBA’)”) 任何以”/db/“ 开头的URL需要同时具有 “ROLE_ADMIN” 和 “ROLE_DBA”权限的用户才可以访问。 antMatchers(“/db/**”).hasAnyRole(“ADMIN”, “DBA”) 任何以”/db/“ 开头的URL只需要拥有 “ROLE_ADMIN” 和 “ROLE_DBA”其中一个权限的用户才可以访问。 anyRequest().authenticated() 尚未匹配的任何URL都要求用户进行身份验证 logout().logoutUrl(“/api/user/logout”) 指定登出的url defaultSuccessUrl(“/index”) 指定登录成功后跳转到/index页面 failureUrl(“/login?error”) 指定登录失败后跳转到/login?error页面 rememberMe().tokenValiditySeconds(1209600).key(“mykey”) 开启cookie储存用户信息，并设置有效期为14天，指定cookie中的密钥 formLogin().loginPage(“/api/user/login”) 通过formlogin方法登录，并设置登录url为/api/user/login]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security + OAuth2（一）]]></title>
    <url>%2Fjava%2Fspring%2Fspring%2Fspring-security-oauth2-1%2F</url>
    <content type="text"><![CDATA[前言经过前段时间对SSO单点登录的研究发现，有以下几类方案可以实现 Spring Session + Redis Spring Security + JWT Spring Security + OAuth2 + Redis 本文章将讲述Spring Security + OAuth2 + Redis，其他方式不做更多说明，直接上干货，下面是大概的流程。 目录结构12345678910111213neusoft-sso-login 授权服务├── AuthApplication└── security ├── WebSecurityConfigurer ├── WebResponseException └── auth ├── AuthFailHandler ├── AuthRequestFilter ├── AuthServerConfigurerAdapter └── AuthSuccessHandler └── service ├── ClientDetailsServiceImpl └── UserDetailsServiceImpl SQL脚本1234567891011121314151617181920212223242526272829CREATE TABLE `neusoft_sr_oper` ( `OPER_ID` varchar(64) NOT NULL COMMENT '用户ID', `OPER_ACCOUNT` varchar(20) DEFAULT NULL COMMENT '账号', `OPER_PWD` varchar(128) DEFAULT NULL COMMENT '密码', `OPER_NAME` varchar(64) DEFAULT NULL COMMENT '账号名称', `CREATOR_ID` varchar(64) DEFAULT '0' COMMENT '创建者ID（0-注册用户，super-超级管理员，其他-子用户）', `LAST_LOGIN_IP` varchar(20) DEFAULT NULL COMMENT '最后登录IP', `LAST_LOGIN_TIME` datetime DEFAULT NULL COMMENT '最后登录时间', `LOGIN_COUNT` int(11) DEFAULT '0' COMMENT '登录次数', `STATUS` varchar(2) DEFAULT '0' COMMENT '0-正常，1-停用，2-锁定，99-注销（删除）', `PWD_MODIFY_DATE` datetime DEFAULT NULL COMMENT '密码更新时间', `STATUS_MODIFY_DATE` datetime DEFAULT NULL COMMENT '状态更新时间', `CREATE_TIME` datetime DEFAULT NULL COMMENT '创建时间', `UPDATE_TIME` datetime DEFAULT NULL COMMENT '更新时间', PRIMARY KEY (`OPER_ID`), UNIQUE KEY `OPER_ACCOUNT` (`OPER_ACCOUNT`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='用户基础信息表';CREATE TABLE `neusoft_sys_oauth_client` ( `CLIENT_ID` varchar(64) NOT NULL COMMENT '客户端ID', `CLIENT_NAME` varchar(32) DEFAULT NULL COMMENT '客户端名称', `CLIENT_SECRET` varchar(128) DEFAULT NULL COMMENT '客户端密匙', `CLIENT_SECRET_PLAIN` varchar(128) DEFAULT NULL COMMENT '客户端密匙明码', `SCOPE` varchar(32) DEFAULT NULL COMMENT '限定范围（read，write）', `AUTHORIZED_GRANT_TYPES` varchar(32) DEFAULT NULL COMMENT '授权类型（client_credentials,password,refresh_token）', `CREATE_TIME` datetime DEFAULT NULL COMMENT '创建时间', `UPDATE_TIME` datetime DEFAULT NULL COMMENT '更新时间', PRIMARY KEY (`CLIENT_ID`) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='OAuth客户端'; 自定义和服务实现对Spring Security + OAuth2，自定义和服务实现主要是以下几个类 WebSecurityConfigurerAdapter，授权拦截器配置 WebResponseExceptionTranslator，异常捕获处理 UsernamePasswordAuthenticationFilter，请求拦截器 UserDetailsService，获取用户信息 ClientDetailsService，获取客户端信息 AuthenticationSuccessHandler，认证成功处理 AuthenticationFailureHandler，认证失败处理 AuthorizationServerConfigurerAdapter 认证服务配置 CODINGAuthApplication.java启动文件，因我使用的是Redis Cluster集群模式，所以要把自带的Redis加载去掉12345678@EnableEurekaClient@SpringBootApplication(exclude = &#123;RedisAutoConfiguration.class&#125;)public class AuthApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(AuthApplication.class); &#125;&#125; pom.xml123456789101112131415161718192021&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-security&lt;/artifactId&gt; &lt;exclusions&gt; &lt;!--旧版本 redis操作有问题--&gt; &lt;exclusion&gt; &lt;artifactId&gt;spring-security-oauth2&lt;/artifactId&gt; &lt;groupId&gt;org.springframework.security.oauth&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.security.oauth&lt;/groupId&gt; &lt;artifactId&gt;spring-security-oauth2&lt;/artifactId&gt; &lt;version&gt;2.3.5.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; WebSecurityConfigurer.java自定义WebSecurityConfigurerAdapter，过滤MATCHER_URL路径为不用鉴权，登录地址设置为LOGIN_URL，在UsernamePasswordAuthenticationFilter拦截器前面新增AuthRequestFilter，设定认证密码模式为BCryptPasswordEncoder12345678910111213141516171819202122232425262728293031323334353637383940414243444546@Configurationpublic class WebSecurityConfigurer extends WebSecurityConfigurerAdapter &#123; @Autowired private AuthFailHandler authFailHandler; @Autowired private AuthSuccessHandler authSuccessHandler; private final static String LOGIN_URL = "/sso/login"; private final static String[] MATCHER_URL = &#123;"/info", "/sso/*"&#125;; @Override protected void configure(HttpSecurity http) throws Exception &#123; http .authorizeRequests() .antMatchers(MATCHER_URL).permitAll() .anyRequest().authenticated() .and().csrf().disable(); http.addFilterAt(authRequestFilter(), UsernamePasswordAuthenticationFilter.class); &#125; @Bean public AuthRequestFilter authRequestFilter() &#123; AuthRequestFilter authRequestFilter = new AuthRequestFilter(); authRequestFilter.setAuthenticationManager(authenticationManagerBean()); authRequestFilter.setRequiresAuthenticationRequestMatcher(new AntPathRequestMatcher(LOGIN_URL, HttpMethod.POST.toString())); authRequestFilter.setAuthenticationFailureHandler(authFailHandler); authRequestFilter.setAuthenticationSuccessHandler(authSuccessHandler); return authRequestFilter; &#125; @Bean @Override @SneakyThrows public AuthenticationManager authenticationManagerBean() &#123; return super.authenticationManagerBean(); &#125; @Bean public PasswordEncoder passwordEncoder() &#123; return new BCryptPasswordEncoder(); &#125;&#125; AuthRequestFilter.java自定义UsernamePasswordAuthenticationFilter，将默认的Form模式变更为JSON模式12345678910111213141516171819202122232425262728public class AuthRequestFilter extends UsernamePasswordAuthenticationFilter &#123; private final static Logger LOGGER = LoggerFactory.getLogger(AuthRequestFilter.class); private final static String ACCOUNT_STR = "account"; private final static String PASSWORD_STR = "password"; @Override public Authentication attemptAuthentication(HttpServletRequest request, HttpServletResponse response) throws AuthenticationException &#123; if (!request.getContentType().equals(MediaType.APPLICATION_JSON_UTF8_VALUE) &amp;&amp; !request.getContentType().equals(MediaType.APPLICATION_JSON_VALUE)) &#123; return super.attemptAuthentication(request, response); &#125; ObjectMapper mapper = new ObjectMapper(); UsernamePasswordAuthenticationToken authRequest = null; try (InputStream is = request.getInputStream()) &#123; Map authenticationBean = mapper.readValue(is, Map.class); authRequest = new UsernamePasswordAuthenticationToken(authenticationBean.get(ACCOUNT_STR), authenticationBean.get(PASSWORD_STR)); &#125; catch (IOException e) &#123; LOGGER.error("JsonAuthenticationFilter form to json error!"); authRequest = new UsernamePasswordAuthenticationToken("", ""); &#125; setDetails(request, authRequest); return this.getAuthenticationManager().authenticate(authRequest); &#125;&#125; AuthFailHandler.java自定义AuthenticationFailureHandler,根据不同的异常信息进行返回处理1234567891011121314151617181920212223242526272829303132333435@Componentpublic class AuthFailHandler implements AuthenticationFailureHandler &#123; @Autowired private ObjectMapper objectMapper; @Override public void onAuthenticationFailure(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, AuthenticationException e) throws IOException, ServletException &#123; String code = null; // 锁 if (e instanceof LockedException) &#123; code = ERROR_AUTH_SSO_LOGIN_LOCK; &#125; // 停用 if (e instanceof DisabledException) &#123; code = ERROR_AUTH_SSO_LOGIN_EXPIRED; &#125; // 账号密码错误 if (e instanceof BadCredentialsException) &#123; code = ERROR_AUTH_SSO_LOGIN; &#125; if (code == null) &#123; return; &#125; httpServletResponse.setStatus(HttpStatus.OK.value()); httpServletResponse.setContentType(MediaType.APPLICATION_JSON_UTF8_VALUE); httpServletResponse.getWriter().write(objectMapper.writeValueAsString(ResponseUtil.fail(code))); &#125;&#125; AuthSuccessHandler.java自定义AuthenticationSuccessHandler，读取header中的AUTHORIZATION，校验客户端secret，校验客户端scope，生成Token并自动存储到Redis中123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687@Componentpublic class AuthSuccessHandler implements AuthenticationSuccessHandler &#123; @Autowired private ObjectMapper objectMapper; @Autowired private PasswordEncoder passwordEncoder; @Autowired private ClientDetailsService clientDetailsService; @Autowired private AuthorizationServerTokenServices defaultAuthorizationServerTokenServices; private final static Logger LOGGER = LoggerFactory.getLogger(AuthSuccessHandler.class); private static final String BASIC_ = "Basic "; @Override public void onAuthenticationSuccess(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Authentication authentication) throws IOException &#123; String header = httpServletRequest.getHeader(HttpHeaders.AUTHORIZATION); if (header == null || !header.startsWith(BASIC_)) &#123; HttpUtil.writeJson(httpServletResponse, ResponseUtil.fail(ERROR_AUTH_BASIC_CLIENT), objectMapper); return; &#125; String[] tokens; try &#123; tokens = extractAndDecodeHeader(header); &#125; catch (Exception e) &#123; HttpUtil.writeJson(httpServletResponse, ResponseUtil.fail(ERROR_AUTH_BASIC_CLIENT), objectMapper); return; &#125; assert tokens.length == 2; String clientId = tokens[0]; ClientDetails clientDetails = clientDetailsService.loadClientByClientId(clientId); //校验secret if (!passwordEncoder.matches(tokens[1], clientDetails.getClientSecret())) &#123; HttpUtil.writeJson(httpServletResponse, ResponseUtil.fail(ERROR_AUTH_CLIENT), objectMapper); return; &#125; TokenRequest tokenRequest = new TokenRequest(Maps.newConcurrentMap(), clientId, clientDetails.getScope(), "all"); //校验scope new DefaultOAuth2RequestValidator().validateScope(tokenRequest, clientDetails); OAuth2Request oAuth2Request = tokenRequest.createOAuth2Request(clientDetails); OAuth2Authentication oAuth2Authentication = new OAuth2Authentication(oAuth2Request, authentication); OAuth2AccessToken oAuth2AccessToken = defaultAuthorizationServerTokenServices.createAccessToken(oAuth2Authentication); LOGGER.info("token builder [&#123;&#125;]", oAuth2AccessToken.getValue()); Map&lt;String, String&gt; result = Maps.newConcurrentMap(); result.put(HttpHeaders.ACCESS_TOKEN, oAuth2AccessToken.getValue()); HttpUtil.writeJson(httpServletResponse, ResponseUtil.success(result), objectMapper); &#125; @SneakyThrows private String[] extractAndDecodeHeader(String header) &#123; byte[] base64Token = header.substring(6).getBytes(UTF_8); byte[] decoded; try &#123; decoded = Base64.getDecoder().decode(base64Token); &#125; catch (IllegalArgumentException e) &#123; throw new RuntimeException( "Failed to decode basic authentication token"); &#125; String token = new String(decoded, UTF_8); int delim = token.indexOf(":"); if (delim == -1) &#123; throw new RuntimeException("Invalid basic authentication token"); &#125; return new String[]&#123;token.substring(0, delim), token.substring(delim + 1)&#125;; &#125;&#125; ClientDetailsServiceImpl.java接口ClientDetailsService实现类，获取客户端信息，并生成ClientDetails12345678910111213141516171819202122232425@Servicepublic class ClientDetailsServiceImpl implements ClientDetailsService &#123; @Autowired private OAuthClientService clientService; private final static String SCOPE_LIMIT = ","; @Override public ClientDetails loadClientByClientId(String s) throws ClientRegistrationException &#123; OAuthClient client = clientService.clientByName(s); if (null == client) &#123; return new BaseClientDetails(); &#125; BaseClientDetails clientDetails = new BaseClientDetails(); clientDetails.setClientId(client.getClientName()); clientDetails.setClientSecret(client.getSecret()); clientDetails.setScope(Arrays.asList(client.getScope().split(SCOPE_LIMIT))); clientDetails.setAuthorizedGrantTypes(Arrays.asList(client.getAuthorizedGrantTypes().split(SCOPE_LIMIT))); return clientDetails; &#125;&#125; UserDetailsServiceImpl.java接口UserDetailsService实现类，获取用户信息，并生成UserDetails123456789101112131415161718192021222324252627282930313233343536373839404142@Servicepublic class UserDetailsServiceImpl implements UserDetailsService &#123; @Autowired private AuthService authService; @Override public UserDetails loadUserByUsername(String account) throws UsernameNotFoundException &#123; if (Strings.isNullOrEmpty(account)) &#123; throw new UsernameNotFoundException(ResponseCodeSso.ERROR_AUTH_SSO_LOGIN_ACCOUNT); &#125; if (!authService.accountExtis(account)) &#123; throw new UsernameNotFoundException(ResponseCodeSso.ERROR_AUTH_SSO_LOGIN_ACCOUNT); &#125; return getUserDetails(account); &#125; private UserDetails getUserDetails(String account) &#123; Oper oper = authService.infoByWhere(account); if (null == oper) &#123; throw new UsernameNotFoundException(ResponseCodeSso.ERROR_AUTH_SSO_LOGIN_ACCOUNT); &#125; // 获取角色，未进行处理 Collection&lt;? extends GrantedAuthority&gt; authorities = AuthorityUtils.createAuthorityList("ROLE_USER"); User user = new User( oper.getOperAccont(), oper.getOperPwd(), oper.getStatus().startsWith("0"), // 是否正常 !oper.getStatus().startsWith("1"), // 是否停用 true, !oper.getStatus().startsWith("2"), // 是否锁定 authorities ); return user; &#125;&#125; AuthServerConfigurerAdapter.javaToken存储模式设置为Redis Cluster存储，将Lettuce替代Jedis，重定义/oauth/check_token路径为/sso/check，嵌入自定义授权异常1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889@Configurationpublic class AuthServerConfigurerAdapter &#123; @Autowired private RedisCacheConfig redisCacheConfig; @Autowired private CacheConfig cacheConfig; @Bean public LettuceConnectionFactory lettuceConnectionFactory() &#123; RedisClusterConfig clusterConfig = new RedisClusterConfig(redisCacheConfig); RedisClusterConfiguration clusterConfiguration = new RedisClusterConfiguration(clusterConfig.getClusterNodes()); return new LettuceConnectionFactory(clusterConfiguration); &#125; @Bean(name = "redisTemplate") public RedisTemplate&lt;String, Serializable&gt; redisCacheTemplate() &#123; RedisTemplate&lt;String, Serializable&gt; template = new RedisTemplate&lt;&gt;(); template.setKeySerializer(new StringRedisSerializer()); template.setValueSerializer(new GenericJackson2JsonRedisSerializer()); template.setConnectionFactory(lettuceConnectionFactory()); return template; &#125; @Bean public TokenStore tokenStore() &#123; RedisTokenStore tokenStore = new RedisTokenStore(redisCacheTemplate().getConnectionFactory()); tokenStore.setPrefix(cacheConfig.getRoot() + ":Token:"); return tokenStore; &#125; @Bean public TokenEnhancer tokenEnhancer() &#123; return (accessToken, authentication) -&gt; &#123; final Map&lt;String, Object&gt; additionalInfo = new HashMap&lt;&gt;(1); additionalInfo.put("license", "web"); ((DefaultOAuth2AccessToken) accessToken).setAdditionalInformation(additionalInfo); return accessToken; &#125;; &#125; @EnableAuthorizationServer protected static class AuthorizationServerConfig extends AuthorizationServerConfigurerAdapter &#123; @Autowired private AuthenticationManager authenticationManager; @Autowired private UserDetailsService userDetailsService; @Autowired private ClientDetailsServiceImpl clientDetailsService; @Autowired private TokenStore tokenStore; @Autowired private TokenEnhancer tokenEnhancer; @Autowired private WebResponseExceptionTranslator webResponseException; @Override public void configure(ClientDetailsServiceConfigurer clients) throws Exception &#123; clients.withClientDetails(clientDetailsService); &#125; @Override public void configure(AuthorizationServerSecurityConfigurer oauthServer) throws Exception &#123; oauthServer .allowFormAuthenticationForClients() .checkTokenAccess("permitAll()"); &#125; @Override public void configure(AuthorizationServerEndpointsConfigurer endpoints) throws Exception &#123; endpoints .allowedTokenEndpointRequestMethods(HttpMethod.GET, HttpMethod.POST) .pathMapping("/oauth/check_token", "/sso/check") .tokenStore(tokenStore) .tokenEnhancer(tokenEnhancer) .userDetailsService(userDetailsService) .authenticationManager(authenticationManager) .reuseRefreshTokens(false) .exceptionTranslator(webResponseException); &#125; &#125;&#125;]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>oauth2</tag>
        <tag>security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nexus搭建Docker私库]]></title>
    <url>%2Fdocker%2Fdocker%2F4%2F</url>
    <content type="text"><![CDATA[Nexus搭建Docker私库Nexus是用于Maven私服的，不过在官网上发现最新的Nexus 3.x还支持Docker仓库了，所以使用docker来搭建一下Nexus 前面安装docker，docker-compose就不说了，直接来文件 docker-compose.yml12345678910nexus: image: registry.cn-shenzhen.aliyuncs.com/zhouqi/nexus:3.0 restart: always ports: - 8081:8081 - 5000:5000 volumes: - ~/docker/nexus/nexus-data:/nexus-data:Z container_name: nexus 注意：端口5000，是为了让docker能登录的端口地址和nexus上要设置一样 创建一个docker本地仓库 在docker所在服务上进行设置 12345vi /etc/docker/daemon.json&#123; "insecure-registries":["172.24.2.65:5000"]&#125; 重启docker并登录 1234systemctl daemon-reloadsystemctl restart dockerdocker login -u admin -p admin123 172.24.2.65:5000]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>nexus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[携程Apollo配置中心]]></title>
    <url>%2Funcategorized%2Fapollo-1%2F</url>
    <content type="text"><![CDATA[携程 Apollo 配置中心在Spring Boot 2.0 整合携程Apollo配置中心一文中，我们在本地快速部署试用了Apollo。本文将介绍如何按照分布式部署（采用Docker部署）的方式编译、打包、部署Apollo配置中心，从而可以在开发、测试、生产等环境分别部署运行。 部署准备 docker 安装 docker-compose 安装 mysql 安装 Apollo工程 工程名 优先级 说明 端口 sql apollo-configservice 1 服务 8090 apolloconfigdb.sql apollo-adminservice 2 管理 8091 apolloconfigdb.sql apollo-portal 3 界面 8092 apolloportaldb.sql 部署策略Apollo目前支持以下环境： DEV 开发环境 FAT 测试环境，相当于alpha环境(功能测试) UAT 集成环境，相当于beta环境（回归测试） PRO 生产环境 网络策略（官网）分布式部署的时候，apollo-configservice和apollo-adminservice需要把自己的IP和端口注册到Meta Server（apollo-configservice本身）。Apollo客户端和Portal会从Meta Server获取服务的地址（IP+端口），然后通过服务地址直接访问。所以如果实际部署的机器有多块网卡（如docker），或者存在某些网卡的IP是Apollo客户端和Portal无法访问的（如网络安全限制），那么我们就需要在apollo-configservice和apollo-adminservice中做相关限制以避免Eureka将这些网卡的IP注册到Meta Server。如下面这个例子就是对于apollo-configservice，把docker0和veth.* 的网卡在注册到Eureka时忽略掉。 12345678910spring: application: name: apollo-configservice profiles: active: $&#123;apollo_profile&#125; cloud: inetutils: ignoredInterfaces: - docker0 - veth.* 另外一种方式是直接指定要注册的IP，可以修改startup.sh，通过JVM System Property在运行时传入，如-Deureka.instance.ip-address=${指定的IP}，或者也可以修改apollo-adminservice或apollo-configservice 的bootstrap.yml文件，加入以下配置 123eureka: instance: ip-address: $&#123;指定的IP&#125; 最后一种方式是直接指定要注册的IP+PORT，可以修改startup.sh，通过JVM System Property在运行时传入，如-Deureka.instance.homePageUrl=http://${指定的IP}:${指定的Port}，或者也可以修改apollo-adminservice或apollo-configservice 的bootstrap.yml文件，加入以下配置 1234eureka: instance: homePageUrl: http://$&#123;指定的IP&#125;:$&#123;指定的Port&#125; preferIpAddress: false 如果Apollo部署在公有云上，本地开发环境无法连接，但又需要做开发测试的话，客户端可以升级到0.11.0版本及以上，然后通过-Dapollo.configService=http://config-service的公网IP:端口来跳过meta service的服务发现 部署步骤部署步骤共四步： 创建数据库，所有Apollo服务端都依赖于MySQL数据库，所以在启动时，应先配置数据才能启动服务； 获取安装包：通过源码构建 构建docker镜像：为apollo-configservice, apollo-adminservice, apollo-portal构建Docker镜像 部署Apollo服务端：构建镜像后通过docker compose就可以部署到公司的测试和生产环境了 创建数据库Apollo服务端共需要两个数据库：ApolloPortalDB和ApolloConfigDB，官网把数据库、表的创建和样例数据都分别准备了sql文件，只需要导入数据库即可。 服务器 数据库 端口 环境 172.24.2.65 ApolloConfigDB 3306 dev 172.24.2.65 ApolloPortalDB 3306 dev 调整服务端配置Apollo自身的一些配置是放在数据库里面的，所以需要针对实际情况做一些调整 ApolloPortalDB 配置项统一存储在ServerConfig表中，也可以通过管理员工具 - 系统参数页面进行配置。 apollo.portal.envs - 可支持的环境列表，默认值是dev，如果portal需要管理多个环境的话，以逗号分隔即可（大小写不敏感） ApolloConfigDB 配置项统一存储在ServerConfig表中，需要注意每个环境的ApolloConfigDB.ServerConfig都需要单独配置。 eureka.service.url - Eureka服务Url，不管是apollo-configservice还是apollo-adminservice都需要向eureka服务注册，所以需要配置eureka服务地址。 按照目前的实现，apollo-configservice本身就是一个eureka服务，所以只需要填入apollo-configservice的地址即可，如有多个，用逗号分隔（注意不要忘了/eureka/后缀）。这里我填写http://172.24.2.63:8090/eureka。 获取安装包到github上进行源码下载，如果github下载比较慢，可以去ipaddress把CDN信息搜索出来，添加到hosts中 151.101.185.194 github.global.ssl.fastly.net 192.30.253.113 github.com 192.30.253.120 codeload.github.com 调整源码配置数据库连接信息scripts/build.bat 12345678910111213# configset apollo_config_db_url="jdbc:mysql://172.24.2.65:3306/ApolloConfigDB?characterEncoding=utf8"set apollo_config_db_username="apollo-config"set apollo_config_db_password="apollo-config"# portalset apollo_portal_db_url="jdbc:mysql://172.24.2.65:3306/ApolloPortalDB?characterEncoding=utf8"set apollo_portal_db_username="apollo-portal"set apollo_portal_db_password="apollo-portal"# dev_metaset dev_meta="http://172.24.2.63:8090"set META_SERVERS_OPTS=-Ddev_meta=%dev_meta% 调整apollo-configservice工程 将所有端口改为8090 bootstrap.yml调整defaultZone: http://${eureka.instance.hostname}:8090/eureka/ startup.sh 新增 export JAVA_OPTS=&quot;-Deureka.instance.ip-address=172.24.2.63&quot; 调整apollo-configservice工程 将所有端口改为8091 bootstrap.yml调整 defaultZone: http://${eureka.instance.hostname}:8090/eureka/ startup.sh 新增 export JAVA_OPTS=&quot;-Deureka.instance.ip-address=172.24.2.63&quot; 调整apollo-portal工程 将所有端口改为8092 apollo-env.properties调整local.meta=http://172.24.2.63:8090 执行编译、打包做完上述配置后，就可以执行编译和打包了。执行/scripts目录下build.sh脚本，该脚本会依次打包apollo-configservice, apollo-adminservice, apollo-portal。 构建docker镜像将target下的apollo-*-github.zip和Dockerfile上传到服务器，形成结构 构建镜像 docker build -t apollo-configservice:1.0.0 . docker build -t apollo-adminservice:1.0.0 . docker build -t apollo-portal:1.0.0 . 部署Apollo服务端 在配置文件目录执行如下命令启动服务：1docker-compose up]]></content>
      <tags>
        <tag>spring cloud</tag>
        <tag>apollo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker搭建Redis集群]]></title>
    <url>%2Fdocker%2Fredis%2Fdocker%2Fredis%2F1%2F</url>
    <content type="text"><![CDATA[前沿redis是最常用的缓存服务，下面我就使用docker来搭建一套redis的集群。 创建redis容器 首先我们需要新建一个redis配置模板，用于批量生成redis配置文件redis-cluster.tmpl 123mkdir -p ~/docker/redis-cluster;cd ~/docker/redis-cluster;touch redis-cluster.tmpl; 将一下内容写入到redis-cluster.tmpl文件中 123456789port $&#123;PORT&#125;protected-mode nocluster-enabled yescluster-config-file nodes.confcluster-node-timeout 5000cluster-announce-ip 192.168.3.60cluster-announce-port $&#123;PORT&#125;cluster-announce-bus-port 1$&#123;PORT&#125;appendonly yes 注意: cluster-announce-ip 是本机ip，而不是127.0.0.1 在docker中创建一个自定义网络，可以使用docker network create命令来创建，docker network ls命令来查看是否创建成功。 1234567docker network create redis-net[root@localhost ~/docker/redis-cluster]# docker network lsNETWORK ID NAME DRIVER SCOPEe98dcf06eec6 bridge bridge local845f1245483b host host localf72ff3f690a0 none null local72aa0ec39b32 redis-net bridge local 现在我们来批量执行生成配置文件和数据存储目录 12345for port in `seq 7000 7005`; do \ mkdir -p ~/docker/redis-cluster/$&#123;port&#125;/conf \ &amp;&amp; PORT=$&#123;port&#125; envsubst &lt; ~/docker/redis-cluster/redis-cluster.tmpl &gt; ~/docker/redis-cluster/$&#123;port&#125;/conf/redis.conf \ &amp;&amp; mkdir -p ~/docker/redis-cluster/$&#123;port&#125;/data; \done 共生成6个文件夹，从7000到7005，每个文件夹下包含data和conf文件夹，同时conf里面有redis.conf配置文件 创建6个redis容器，并启动registry.cn-shenzhen.aliyuncs.com/zhouqi/redis:4.0是我自己的docker镜像，当然也可以直接使用官方镜像。1234567for port in `seq 7000 7005`; do \ docker run -d -ti -p $&#123;port&#125;:$&#123;port&#125; -p 1$&#123;port&#125;:1$&#123;port&#125; \ -v ~/docker/redis-cluster/$&#123;port&#125;/conf/redis.conf:/usr/local/etc/redis/redis.conf:z \ -v ~/docker/redis-cluster/$&#123;port&#125;/data:/data:z \ --restart always --name redis-$&#123;port&#125; --net redis-net \ --sysctl net.core.somaxconn=1024 registry.cn-shenzhen.aliyuncs.com/zhouqi/redis:4.0 redis-server /usr/local/etc/redis/redis.conf; \done 我们查看一下是否启动成功 建立Redis集群关系12345678echo yes | docker run -i --rm --net redis-net ruby sh -c '\ gem install redis \ &amp;&amp; wget http://download.redis.io/releases/redis-4.0.10.tar.gz \ &amp;&amp; tar -xzvf redis-4.0.10.tar.gz \ &amp;&amp; ruby redis-4.0.10/src/redis-trib.rb create --replicas 1 \ '"$(for port in `seq 7000 7005`; do \ echo -n "$(docker inspect --format '&#123;&#123; (index .NetworkSettings.Networks "redis-net").IPAddress &#125;&#125;' "redis-$&#123;port&#125;")":$&#123;port&#125; ' ' ; \ done)" 最后我们在看一下redis的集群信息 问题 centos7创建redis集群一直卡在Waiting for the cluster to join 问题：机器没有开放redis集群总线端口。 解决方法：开放redis集群总线端口。集群总线端口是redis客户端连接的端口+10000。如：redis客户端端口是7001。则：集群总线端口就是17001。注意、所以redis集群机器都要放开对应的redis客户端端口和集群总线端口。如果你是在虚拟机上搭建redis集群、那么你直接关闭防火墙即可解决 centos7创建redis集群一直卡在Waiting for the cluster to join 问题:也许你的redis集群的ip设置成了127.0.0.1或loclahost 解决方法：将redis-cluster.tmpl中的ip设置成本机ip WARNING: redis-trib.rb is not longer available! 问题：只有redis 5.0以下的版本才支持 解决方法：去http://download.redis.io/releases中下载你对应的版本号 批量处理命令 Redis停止 123for port in `seq 7000 7005`; do \docker stop redis-$&#123;port&#125;; \done Redis删除 1234for port in `seq 7000 7005`; do \docker stop redis-$&#123;port&#125;; \docker rm redis-$&#123;port&#125;; \done Redis启动 123for port in `seq 7000 7005`; do \docker start redis-$&#123;port&#125;; \done]]></content>
      <categories>
        <category>docker</category>
        <category>redis</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker使用国内镜像]]></title>
    <url>%2Fdocker%2Fdocker%2F2%2F</url>
    <content type="text"><![CDATA[为什么要为docker配置国内镜像在正常情况下，docker有一个默认连接的国外官方镜像，在国外的网友访问该官方镜像自然不成问题，但是国内毕竟不是国外，由于国情不同，中国的网络访问国外官方镜像网速一向很慢，而且往往还会遭遇断网的窘境，所以说我们要想正常使用docker的镜像，那么我们就不得不配置相应的国内镜像。 可以使用的国内镜像有哪些Docker可以配置的国内镜像有很多可供选择，比如说：阿里云，网易蜂巢，DaoCloud，Docker中国区官方镜像等，这些都是可以提供给大家随意选择的不错的镜像仓库。 配置Docker中国区官方镜像Docker中国区官方镜像简介在国内，可以通过registry.docker-cn.com访问官方镜像库，目前该镜像库只包含流行的公有镜像，而私有镜像仍需要从美国镜像库中拉取。 配置Docker中国区官方镜像使用vi修改 /etc/docker/daemon.json 文件并添加上”registry-mirrors”: [“https://registry.docker-cn.com“]，如下：12345[neusoft@172 ~]$ sudo vi /etc/docker/daemon.json &#123;"registry-mirrors": ["https://registry.docker-cn.com"]&#125; 重启Docker配置完之后执行下面的命令，以使docker的配置文件生效12systemctl daemon-reload systemctl restart docker]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker使用非root用户]]></title>
    <url>%2Fdocker%2Fdocker%2F1%2F</url>
    <content type="text"><![CDATA[Docker使用非root用户通常我们使用Docker的时候都是使用的root，官方说法如下： The docker daemon binds to a Unix socket instead of a TCP port. By default that Unix socket is owned by the user root and other users can access it with sudo. For this reason, docker daemon always runs as the root user.To avoid having to use sudo when you use the docker command, create a Unix group called docker and add users to it. When the docker daemon starts, it makes the ownership of the Unix socket read/writable by the docker group. 下面是使用非root用户操作的步骤 创建docker组 sudo groupadd docker 将当前用户加入docker组 sudo gpasswd -a ${USER} docker 重新启动docker服务（下面是CentOS7的命令） sudo systemctl restart docker 当前用户退出系统重新登陆运行docker命令 docker ps]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker集群(二)]]></title>
    <url>%2Fdocker%2Fdocker%2Fjq-2%2F</url>
    <content type="text"><![CDATA[Docker使用Portainer管理Swarm集群在上一节中已经将了，如果将子节点加入到主节点中，本节将使用portainer来对swarm集群进行管理 安装portainer管理工具下载portainer工具，并启用docker服务123456789[neusoft@localhost ~/docker]$ docker pull registry.cn-shenzhen.aliyuncs.com/zhouqi/portainer:1.20.0[neusoft@localhost ~/docker]$ docker service create \&gt; --name portainer \&gt; --publish 9000:9000 \&gt; --constraint 'node.role == manager' \&gt; --mount type=bind,src=//var/run/docker.sock,dst=/var/run/docker.sock \&gt; registry.cn-shenzhen.aliyuncs.com/zhouqi/portainer:1.20.0 \&gt; -H unix:///var/run/docker.sock 查看docker的服务，已经启用成功123[neusoft@localhost ~/docker]$ docker service lsID NAME MODE REPLICAS IMAGEy6ydozgujzjo portainer replicated 1/1 registry.cn-shenzhen.aliyuncs.com/zhouqi/portainer:1.20.0 如果想停用该服务可以使用以下命令：12[neusoft@localhost ~/docker]$ docker service rm portainerportainer 查看portainer并进行集群设置访问http://172.24.2.63:9000地址，首先进行设置密码 登录进去后，能查看到首页的信息 添加一个子节点到portainer中 现在可以查看添加进去的节点信息了]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker集群(一)]]></title>
    <url>%2Fdocker%2Fdocker%2Fjq-1%2F</url>
    <content type="text"><![CDATA[Docker使用Swarm搭建集群selinux查看selinux是否关闭12[root@localhost ~]# /usr/sbin/sestatus -vSELinux status: disabled 如果没有关闭，可以直接修改/etc/selinux/config中的SELINUX为disabled：12345678910111213[neusoft@localhost ~/docker]$ vi /etc/selinux/config# This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:# enforcing - SELinux security policy is enforced.# permissive - SELinux prints warnings instead of enforcing.# disabled - No SELinux policy is loaded.#SELINUX=enforcingSELINUX=disabled# SELINUXTYPE= can take one of three two values:# targeted - Targeted processes are protected,# minimum - Modification of targeted policy. Only selected processes are protected. # mls - Multi Level Security protection.SELINUXTYPE=targeted 注意：修改xelinux后需重启服务器 打开docker.service中的2375端口12345678910111213141516171819202122232425262728293031323334[neusoft@localhost ~]$ sudo vi /usr/lib/systemd/system/docker.service[Unit]Description=Docker Application Container EngineDocumentation=http://docs.docker.comAfter=network.target rhel-push-plugin.socket registries.serviceWants=docker-storage-setup.serviceRequires=docker-cleanup.timer[Service]Type=notifyNotifyAccess=allEnvironmentFile=-/run/containers/registries.confEnvironmentFile=-/etc/sysconfig/dockerEnvironmentFile=-/etc/sysconfig/docker-storageEnvironmentFile=-/etc/sysconfig/docker-networkEnvironment=GOTRACEBACK=crashEnvironment=DOCKER_HTTP_HOST_COMPAT=1Environment=PATH=/usr/libexec/docker:/usr/bin:/usr/sbinExecStart=/usr/bin/dockerd-current \ --add-runtime docker-runc=/usr/libexec/docker/docker-runc-current \ --default-runtime=docker-runc \ --exec-opt native.cgroupdriver=systemd \ --userland-proxy-path=/usr/libexec/docker/docker-proxy-current \ --init-path=/usr/libexec/docker/docker-init-current \ --seccomp-profile=/etc/docker/seccomp.json \ $OPTIONS \ $DOCKER_STORAGE_OPTIONS \ $DOCKER_NETWORK_OPTIONS \ $ADD_REGISTRY \ $BLOCK_REGISTRY \ $INSECURE_REGISTRY \ $REGISTRIES \ -H unix:///var/run/docker.sock -H 0.0.0.0:2375 重启docker 创建swarm管理节点下面开始创建swarm。登录到centos7主机上，执行如下命令：12345678910[neusoft@localhost ~/docker]$ docker swarm init --advertise-addr 172.24.2.63Swarm initialized: current node (5levgh6t0qizmgwyn789dcuqx) is now a manager.To add a worker to this swarm, run the following command: docker swarm join \ --token SWMTKN-1-5g3hxk4q7eg3atnuw45ijrqei21y2aopiqha31v04jwmnz5svt-bsudrrah0zgdx7qzjcm6xvfy3 \ 172.24.2.63:2377To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions. 如果你不知道或者忘记了Swarm Manager节点的Token信息， 你可以在Manager节点上执行以下命令查看Worker节点连接所需要的Token信息。123456[neusoft@localhost ~/docker]$ docker swarm join-token workerTo add a worker to this swarm, run the following command: docker swarm join \ --token SWMTKN-1-5g3hxk4q7eg3atnuw45ijrqei21y2aopiqha31v04jwmnz5svt-bsudrrah0zgdx7qzjcm6xvfy3 \ 172.24.2.63:2377 使用docker info和docker node ls查看集群中的相关信息：123[neusoft@localhost ~/docker]$ docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS5levgh6t0qizmgwyn789dcuqx * localhost.localdomain Ready Active Leader 把docker中的selinux去掉，然后重启docker123456789101112[root@localhost ~]# vi /etc/sysconfig/docker# /etc/sysconfig/docker# Modify these options if you want to change the way the docker daemon runs#OPTIONS='--selinux-enabled --log-driver=journald --signature-verification=false'OPTIONS='--log-driver=journald --signature-verification=false'if [ -z "$&#123;DOCKER_CERT_PATH&#125;" ]; then DOCKER_CERT_PATH=/etc/dockerfi[root@localhost ~]# systemctl restart docker 将Worker节点加入swarm集群登录到172.24.2.62主机上，执行前面创建swarm时输出的命令：1234[root@localhost ~]# docker swarm join \&gt; --token SWMTKN-1-5g3hxk4q7eg3atnuw45ijrqei21y2aopiqha31v04jwmnz5svt-bsudrrah0zgdx7qzjcm6xvfy3 \&gt; 172.24.2.63:2377This node joined a swarm as a worker. 去master查看节点信息1234[neusoft@localhost ~/docker]$ docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS5levgh6t0qizmgwyn789dcuqx * localhost.localdomain Ready Active Leaderi8x5wur7ux01rewkffavfyw3x localhost.localdomain Ready Active 这个时候能看见有2个节点了，但是hostname无法识别，所以我们可以使用命令来修改hostname的内容12[root@localhost ~]# hostnamectl set-hostname 172.24.2.62[root@localhost ~]# systemctl restart docker 我们在去master中查看节点信息1234[neusoft@localhost ~/docker]$ docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS5levgh6t0qizmgwyn789dcuqx * localhost.localdomain Ready Active Leaderi8x5wur7ux01rewkffavfyw3x 172.24.2.62 Ready Active 那么节点加入成功了。]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM运行原理]]></title>
    <url>%2Fjava%2Fjava%2Fjvm%2F2%2F</url>
    <content type="text"><![CDATA[JVM运行原理详解JVM简析作为一名Java使用者，掌握JVM的体系结构也是很有必要的。说起Java，我们首先想到的是Java编程语言，然而事实上，Java是一种技术，它由四方面组成：Java编程语言、Java类文件格式、Java虚拟机和Java应用程序接口(Java API)。它们的关系如下图所示： Java平台由Java虚拟机和Java应用程序接口搭建，Java语言则是进入这个平台的通道，用Java语言编写并编译的程序可以运行在这个平台上。这个平台的结构如下图所示： 运行期环境代表着Java平台，开发人员编写Java代码(.java文件)，然后将之编译成字节码(.class文件)，再然后字节码被装入内存，一旦字节码进入虚拟机，它就会被解释器解释执行，或者是被即时代码发生器有选择的转换成机器码执行。 JVM在它的生存周期中有一个明确的任务，那就是运行Java程序，因此当Java程序启动的时候，就产生JVM的一个实例；当程序运行结束的时候，该实例也跟着消失了。 在Java平台的结构中, 可以看出，Java虚拟机(JVM) 处在核心的位置，是程序与底层操作系统和硬件无关的关键。它的下方是移植接口，移植接口由两部分组成：适配器和Java操作系统, 其中依赖于平台的部分称为适配器；JVM 通过移植接口在具体的平台和操作系统上实现；在JVM 的上方是Java的基本类库和扩展类库以及它们的API， 利用Java API编写的应用程序(application) 和小程序(Java applet) 可以在任何Java平台上运行而无需考虑底层平台, 就是因为有Java虚拟机(JVM)实现了程序与操作系统的分离，从而实现了Java 的平台无关性。 下面我们从JVM的基本概念和运过程程这两个方面入手来对它进行深入的研究。 JVM基本概念基本概念JVM是可运行Java代码的假想计算机 ，包括一套字节码指令集、一组寄存器、一个栈、一个垃圾回收，堆 和 一个存储方法域。JVM是运行在操作系统之上的，它与硬件没有直接的交互。 运行过程我们都知道Java源文件，通过编译器，能够生产相应的.Class文件，也就是字节码文件，而字节码文件又通过Java虚拟机中的解释器，编译成特定机器上的机器码 。也就是如下： Java源文件—&gt;编译器—&gt;字节码文件 字节码文件-&gt;JVM—&gt;机器码 每一种平台的解释器是不同的，但是实现的虚拟机是相同的，这也就是Java为什么能够跨平台的原因了 ，当一个程序从开始运行，这时虚拟机就开始实例化了，多个程序启动就会存在多个虚拟机实例。程序退出或者关闭，则虚拟机实例消亡，多个虚拟机实例之间数据不能共享。 三种JVM Sun公司的HotSpot BEA公司的JRockit IBM公司的J9 JVM 在JDK1.7及其以前我们所使用的都是Sun公司的HotSpot，但由于Sun公司和BEA公司都被oracle收购，jdk1.8将采用Sun公司的HotSpot和BEA公司的JRockit两个JVM中精华形成jdk1.8的JVM。 JVM的体系结构 Class Loader类加载器负责加载 .class文件，class文件在文件开头有特定的文件标示，并且ClassLoader负责class文件的加载等，至于它是否可以运行，则由Execution Engine决定。 定位和导入二进制class文件 验证导入类的正确性 为类分配初始化内存 帮助解析符号引用 Native Interface本地接口本地接口的作用是融合不同的编程语言为Java所用，它的初衷是融合C/C++程序，Java诞生的时候C/C++横行的时候，要想立足，必须有调用C/C++程序，于是就在内存中专门开辟了一块区域处理标记为native的代码，它的具体作法是Native Method Stack中登记native方法，在Execution Engine执行时加载native libraies。 目前该方法使用的越来越少了，除非是与硬件有关的应用，比如通过Java程序驱动打印机，或者Java系统管理生产设备，在企业级应用中已经比较少见。 因为现在的异构领域间的通信很发达，比如可以使用Socket通信，也可以使用Web Service等。 Execution Engine 执行引擎：执行包在装载类的方法中的指令，也就是方法Runtime data area 运行数据区虚拟机内存或者Jvm内存，冲整个计算机内存中开辟一块内存存储Jvm需要用到的对象，变量等，运行区数据有分很多小区，分别为：方法区，虚拟机栈，本地方法栈，堆，程序计数器 JVM数据运行区详解（栈管运行，堆管存储）JVM调优主要就是优化 Heap堆 和 Method Area 方法区 Native Method Stack本地方法栈它的具体做法是Native Method Stack中登记native方法，在Execution Engine执行时加载native libraies PC Register程序计数器每个线程都有一个程序计算器，就是一个指针，指向方法区中的方法字节码（下一个将要执行的指令代码），由执行引擎读取下一条指令，是一个非常小的内存空间，几乎可以忽略不记 Method Area方法区方法区是被所有线程共享，所有字段和方法字节码，以及一些特殊方法如构造函数，接口代码也在此定义。简单说，所有定义的方法的信息都保存在该区域，此区域属于共享区间 静态变量+常量+类信息+运行时常量池存在方法区中，实例变量存在堆内存中 Stack 栈 栈是什么 栈也叫栈内存，主管Java程序的运行，是在线程创建时创建，它的生命期是跟随线程的生命期，线程结束栈内存也就释放，对于栈来说不存在垃圾回收问题，只要线程一结束该栈就Over，生命周期和线程一致，是线程私有的。 基本类型的变量和对象的引用变量都是在函数的栈内存中分配 栈存储什么？ 栈帧中主要保存3类数据 本地变量（Local Variables）：输入参数和输出参数以及方法内的变量； 栈操作（Operand Stack）：记录出栈、入栈的操作； 栈帧数据（Frame Data）：包括类文件、方法等等； 栈运行原理 栈中的数据都是以栈帧（Stack Frame）的格式存在，栈帧是一个内存区块，是一个数据集，是一个有关方法和运行期数据的数据集，当一个方法A被调用时就产生了一个栈帧F1，并被压入到栈中，A方法又调用了B方法，于是产生栈帧F2也被压入栈，B方法又调用了C方法，于是产生栈帧F3也被压入栈…… 依次执行完毕后，先弹出后进……F3栈帧，再弹出F2栈帧，再弹出F1栈帧。 遵循“先进后出”/“后进先出”原则 Heap 堆堆这块区域是JVM中最大的，应用的对象和数据都是存在这个区域，这块区域也是线程共享的，也是gc主要的回收区，一个 JVM 实例只存在一个堆类存，堆内存的大小是可以调节的。类加载器读取了类文件后，需要把类、方法、常变量放到堆内存中，以方便执行器执行，堆内存分为三部分： 新生区 新生区是类的诞生、成长、消亡的区域，一个类在这里产生，应用，最后被垃圾回收器收集，结束生命。新生区又分为两部分：伊甸区（Eden space）和幸存者区（Survivor pace），所有的类都是在伊甸区被new出来的。幸存区有两个：0区（Survivor 0 space）和1区（Survivor 1 space）。当伊甸园的空间用完时，程序又需要创建对象，JVM的垃圾回收器将对伊甸园进行垃圾回收（Minor GC）,将伊甸园中的剩余对象移动到幸存0区。若幸存0区也满了，再对该区进行垃圾回收，然后移动到1区。那如果1去也满了呢？再移动到养老区。若养老区也满了，那么这个时候将产生Major GC（FullGCC），进行养老区的内存清理。若养老区执行Full GC 之后发现依然无法进行对象的保存，就会产生OOM异常“OutOfMemoryError”。 如果出现java.lang.OutOfMemoryError: Java heap space异常，说明Java虚拟机的堆内存不够。 原因有二 Java虚拟机的堆内存设置不够，可以通过参数-Xms、-Xmx来调整 代码中创建了大量大对象，并且长时间不能被垃圾收集器收集（存在被引用） 养老区 养老区用于保存从新生区筛选出来的 JAVA 对象，一般池对象都在这个区域活跃 永久区 永久存储区是一个常驻内存区域，用于存放JDK自身所携带的 Class,Interface 的元数据，也就是说它存储的是运行环境必须的类信息，被装载进此区域的数据是不会被垃圾回收器回收掉的，关闭 JVM 才会释放此区域所占用的内存 如果出现java.lang.OutOfMemoryError: PermGen space，说明是Java虚拟机对永久代Perm内存设置不够。 原因有二 程序启动需要加载大量的第三方jar包。例如：在一个Tomcat下部署了太多的应用 大量动态反射生成的类不断被加载，最终导致Perm区被占满 说明： Jdk1.6及之前：常量池分配在永久代 。 Jdk1.7：有，但已经逐步“去永久代” 。 Jdk1.8及之后：无(java.lang.OutOfMemoryError: PermGen space,这种错误将不会出现在JDK1.8中)。 说明： 方法区和堆内存的异议，实际而言，方法区和堆一样，是各个线程共享的内存区域，它用于存储虚拟机加载的：类信息+普通常量+静态常量+编译器编译后的代码等等，虽然JVM规范将方法区描述为堆的一个逻辑部分，但它却还有一个别名叫做Non-Heap（非堆），目的就是要和堆分开。 对于HotSpot虚拟机，很多开发者习惯将方法区称之为“永久代（Parmanent Gen）”,但严格本质上说两者不同，或者说使用永久代来实现方法区而已，永久代是方法区的一个实现，jdk1.7的版本中，已经将原本放在永久代的字符串常量池移走。 常量池（Constant Pool）是方法区的一部分，Class文件除了有类的版本、字段、方法、接口等描述信息外，还有一项信息就是常量池，这部分内容将在类加载后进入方法区的运行时常量池中存放。 堆内存调优简介 代码测试： 12345678 public class JVMTest &#123; public static void main(String[] args)&#123; long maxMemory = Runtime.getRuntime().maxMemory();//返回Java虚拟机试图使用的最大内存量。 Long totalMemory = Runtime. getRuntime().totalMemory();//返回Java虚拟机中的内存总量。 System.out.println("MAX_MEMORY ="+maxMemory +"(字节)、"+(maxMemory/(double)1024/1024) + "MB"); System.out.println("TOTAL_ MEMORY = "+totalMemory +"(字节)"+(totalMemory/(double)1024/1024) + "MB"); &#125; &#125; 在Run as -&gt;Run Configurations中输入”-XX:+PrintGCDetails”可以查看堆内存运行原理图 在jdk1.7中 在jdk1.8中 通过参数设置自动触发垃圾回收123456789101112public class JVMTest &#123; public static void main(String[] args)&#123; long maxMemory = Runtime.getRuntime().maxMemory();//返回Java虚拟机试图使用的最大内存量。 Long totalMemory = Runtime. getRuntime().totalMemory();//返回Java虚拟机中的内存总量。 System.out.println("MAX_MEMORY ="+maxMemory +"(字节)、"+(maxMemory/(double)1024/1024) + "MB"); System.out.println("TOTAL_ MEMORY = "+totalMemory +"(字节)"+(totalMemory/(double)1024/1024) + "MB"); String str = "www.baidu.com"; while(true)&#123; str += str + new Random().nextInt(88888888) + new Random().nextInt(99999999); &#125; &#125;&#125; 在Run as -&gt;Run Configurations中输入设置“-Xmx8m –Xms8m –xx:+PrintGCDetails”可以参看垃圾回收机制原理]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA调优之JVM-GC]]></title>
    <url>%2Fjava%2Fjava%2Fjvm%2F1%2F</url>
    <content type="text"><![CDATA[JAVA调优之JVM-GC在分享之前需要给大家说一下，JVM的堆一般分为新生代,老年代，永久代这三个区间，下面我们就来说明根据各种情况来进行排查，并对这三个区进行分配合理的区间。 服务器 应用：dal-service-5.1.4.0-SNAPSHOT.jar CPU：24核 内存：125G JDK：8.144 启动应用1java -jar ./dal-service-5.1.4.0-SNAPSHOT.jar --spring.profiles.active=test 使用上面的命令，我们的应用就启动完成来，然后我们来看一下应用的各种数据。 查看应用的PID进程号，使用JPS 查看CPU和内存情况，可以使用TOP，并按下1 现在应用的基本情况就知道来，但这只是一个基础的信息，怎样能看详细的资源，比如GC怎么回收的，GC触发时间，GC频率等。现在我们把应用停了，加载GC的一些参数和GC的日志，再次启动 1234567java \-verbose:gc \-XX:+PrintGCDetails \-XX:+PrintGCTimeStamps \-XX:+PrintHeapAtGC \-Xloggc:/home/iot/dal-service-gc.log \-jar /home/iot/portal/dal-service-5.1.4.0-SNAPSHOT.jar --spring.profiles.active=test 查看GC情况查看GC情况，有2种方法可以查看 1、直接查看dal-service-gc.log日志文件 2、使用jstatd进行监听，然后通过jvisualvm来查看 1jstatd -J-Djava.security.policy=/home/iot/jdk1.8.0_131/bin/jstatd.all.policy -J-Djava.rmi.server.hostname=192.168.156.22 -p 1099 -J-Djava.rmi.server.logCalls=true 其实，我们在日志中就看见当应用启动的时候执行来3次FULL GC。 新生代情况： PSYoungGen total 3185152K, used 3185120K eden space 3150848K, 100% used from space 34304K, 99% used 老年代情况： ParOldGen total 1429504K, used 56198K object space 1429504K, 3% used 持久代情况： Metaspace used 74104K, capacity 74936K, committed 75544K, reserved 1116160K class space used 8951K, capacity 9137K, committed 9256K, reserved 1048576K 优化JVM大家都知道FULL GC一次，将会降低TPS，并且会暂用线程时间。本次优化让应用尽量启动后不再产生FULL GC，我们调整一下启动命令。 调整后，我们虽然进行来GC，但是没有再次执行FULL GC了，这也算调整完成了。 启动命令参数– GC 1234-XX:+PrintGCDetails GC内容-XX:+PrintGCTimeStamps 输出GC的时间戳（以基准时间的形式）-XX:+PrintHeapAtGC 在进行GC的前后打印出堆的信息-Xloggc:../logs/gc.log 日志文件的输出路径 – JVM 12345-Xmn 新生代空间大小，此处的大小是(eden+2 survivor space) -XX:NewSize 新生代空间大小初始值-XX:MaxNewSize 新生代空间大小最大值-XX:MetaspaceSize 持久代初始值-XX:MaxMetaspaceSize 最大持久代值 Linux调优修改/etc/sysctl.conf文件，增加 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_tw_recycle = 1]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mobaxterm session stopped]]></title>
    <url>%2Funcategorized%2Flinux%2Fmobaxterm-1%2F</url>
    <content type="text"><![CDATA[Mobaxterm Session Stopped 点击当前的Session 编辑Edit Session 点击Telnet设置Remote host地址为你的SSH地址 在Advanced Telnet settings选项卡的Telnet Client中选择Busybox telnet 尝试连接（肯定没效果） 切换到SSH，重新输入Remote host，点击连接，输入账号密码，ok连上了！]]></content>
      <tags>
        <tag>linux</tag>
        <tag>mobaxterm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 阅读源码笔记]]></title>
    <url>%2Fjava%2Fjava%2Fspring%2F5%2F</url>
    <content type="text"><![CDATA[Spring 阅读源码笔记（原创）什么是控制反转所谓控制反转，就是把原先我们代码里面需要实现的对象创建、依赖的代码，反转给容器来帮忙实现。那么必然的我们需要创建一个容器，同时需要一种描述来让容器知道需要创建的对象与对象的关系。这个描述最具体表现就是我们可配置的文件。 对象和对象关系怎么表示？可以用 xml ， properties 文件等语义化配置文件表示。 描述对象关系的文件存放在哪里？可能是 classpath ， filesystem ，或者是 URL 网络资源， servletContext 等。 有了配置文件，还需要对配置文件解析不同的配置文件对对象的描述不一样，如标准的，自定义声明式的，如何统一？ 在内部需要有一个统一的关于对象的定义，所有外部的描述都必须转化成统一的描述定义。如何对不同的配置文件进行解析？需要对不同的配置文件语法，采用不同的解析器 IOC容器接口BeanFactory 最顶层的一个接口类，它定义了IOC容器的基本功能规范，他下面有三个子类分别是（ListableBeanFactory，HierarchicalBeanFactory，AutowireCapableBeanFactory），这四个接口共同定义了 Bean 的集合、Bean 之间的关系、以及 Bean 行为。 ListableBeanFactory表示这些 Bean 是可列表的 HierarchicalBeanFactory表示这些 Bean 是有继承关系的 AutowireCapableBeanFactory表示这些 Bean 的自动装配规则 IOC容器实现 XmlBeanFactory最基础的容器实现，这个IOC容器可以读取XML文件定义XML文件中对bean的描述（BeanDefinition） ClasspathXmlApplicationContext高级的IOC容器实现 ApplicationContext高级的IoC容器实现，并附加了（国际化MessageSource，访问资源ResourcePatternResolver，应用事件ApplicationEventPublisher）的实现 SpringIOC容器管理了我们定义的各种Bean对象及其相互的关系，Bean对象在Spring实现中是以BeanDefinition来描述的 IOC容器初始化 BeanDefinition的Resource定位 BeanDefinition的Resource载入 BeanDefinition的Resource注册 IOC容器创建过程IOC容器创建分为两种 XmlBeanFactoryFileSystemXmlApplicationContext XmlBeanFactory创建12345678//根据Xml配置文件创建Resource资源对象，该对象中包含了BeanDefinition的信息 ClassPathResource resource =new ClassPathResource("application-context.xml");//创建DefaultListableBeanFactory DefaultListableBeanFactory factory =new DefaultListableBeanFactory();//创建XmlBeanDefinitionReader读取器，用于载入BeanDefinition。之所以需要BeanFactory作为参数，是因为会将读取的信息回调配置给factory XmlBeanDefinitionReader reader =new XmlBeanDefinitionReader(factory);//XmlBeanDefinitionReader执行载入BeanDefinition的方法，最后会完成Bean的载入和注册。完成后Bean就成功的放置到IOC容器当中，以后我们就可以从中取得Bean来使用 reader.loadBeanDefinitions(resource); FileSystemXmlApplicationContext创建 执行FileSystemXmlApplicationContext的构造函数 设置资源加载器（父类super）和资源定位（AbstractRefreshableConfigApplicationContext中的setConfigLocations） AbstractApplicationContext的refresh函数载入Bean AbstractRefreshableApplicationContext实现载入Bean AbstractBeanDefinitionReader读取Bean定义资源 资源加载器获取要读入的资源 XmlBeanDefinitionReader加载Bean定义资源 DocumentLoader将Bean定义资源转换为Document对象 XmlBeanDefinitionReader解析载入的Bean定义资源文件 DefaultBeanDefinitionDocumentReader对Bean定义的Document对象解析 BeanDefinitionParserDelegate解析&lt;Bean&gt;定义资源文件中的Bean元素 BeanDefinitionParserDelegate解析&lt;property&gt;元素 解析&lt;property&gt;元素的子元素 解析&lt;list&gt;子元素 解析过后的BeanDefinition在IoC容器中的注册]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Idea中整合Docker]]></title>
    <url>%2Fidea%2Fidea%2F1%2F</url>
    <content type="text"><![CDATA[IDEA在Windows上结合Docker在网上找了很多文章，却没有一篇文章明确说明如何整合，如何使用Idea+Docker，下面来一一说下整合。 虚拟机（192.168.3.60）在虚拟机上安装Docker，我这里安装的是Docker version 1.13.1, build 092cba3。编辑Docker运行文件 1# vi /etc/systemd/system/docker.service 在ExecStart节点上增加unix:///var/run/docker.sock -H 0.0.0.0:2375，打开Docker外部访问，保存后重启Docker服务 12# systemctl daemon-reload# systemctl start docker 查看Docker中是否开启2375端口1# netstat -anp|grep 2375 在虚拟机上的配置就完毕了。 Windows在windows系统环境变量中新建DOCKER_HOST,值为tcp://192.168.3.60:2375 打开Idea，安装Docker插件 设置Docker远程访问地址为tcp://192.168.3.60:2375 在Idea的运行中，运行Docker，就可查看到远程Docker的服务了 我们现在在POM.xml中增加插件并设置要编译后的文件 1234567891011121314151617&lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;configuration&gt; &lt;imageName&gt;test/test:1.0&lt;/imageName&gt; &lt;baseImage&gt;java&lt;/baseImage&gt; &lt;entryPoint&gt;["java", "-jar", "/$&#123;project.build.finalName&#125;.jar"]&lt;/entryPoint&gt; &lt;resources&gt; &lt;resource&gt; &lt;targetPath&gt;/&lt;/targetPath&gt; &lt;directory&gt;$&#123;project.build.directory&#125;&lt;/directory&gt; &lt;include&gt;$&#123;project.build.finalName&#125;.jar&lt;/include&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/configuration&gt; &lt;/plugin&gt; 执行mvn clean package docker:build，就可以进行远程编译了。编译后在会target下生成一个Dockerfile 123FROM javaADD /demo-1.0-SNAPSHOT.jar //ENTRYPOINT ["java", "-jar", "/demo-1.0-SNAPSHOT.jar"] 查看远程编译后的包]]></content>
      <categories>
        <category>idea</category>
      </categories>
      <tags>
        <tag>idea</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[源码解读Spring IOC原理（四）]]></title>
    <url>%2Fjava%2Fspring%2Fjava%2Fspring%2F4%2F</url>
    <content type="text"><![CDATA[源码解读Spring IOC原理（四）IOC容器的高级特性介绍通过前面4篇文章对Spring IoC容器的源码分析，我们已经基本上了解了Spring IoC容器对Bean定义资源的定位、读入和解析过程，同时也清楚了当用户通过getBean方法向IoC容器获取被管理的Bean时，IoC容器对Bean进行的初始化和依赖注入过程，这些是Spring IoC容器的基本功能特性。Spring IoC容器还有一些高级特性，如使用lazy-init属性对Bean预初始化、FactoryBean产生或者修饰Bean对象的生成、IoC容器初始化Bean过程中使用BeanPostProcessor后置处理器对Bean声明周期事件管理和IoC容器的autowiring自动装配功能等。 Spring IoC容器的lazy-init属性实现预实例化通过前面我们对IoC容器的实现和工作原理分析，我们知道IoC容器的初始化过程就是对Bean定义资源的定位、载入和注册，此时容器对Bean的依赖注入并没有发生，依赖注入主要是在应用程序第一次向容器索取Bean时，通过getBean方法的调用完成。 当Bean定义资源的元素中配置了lazy-init属性时，容器将会在初始化的时候对所配置的Bean进行预实例化，Bean的依赖注入在容器初始化的时候就已经完成。这样，当应用程序第一次向容器索取被管理的Bean时，就不用再初始化和对Bean进行依赖注入了，直接从容器中获取已经完成依赖注入的现成Bean，可以提高应用第一次向容器获取Bean的性能。 下面我们通过代码分析容器预实例化的实现过程： (1). refresh()先从IoC容器的初始会过程开始，通过前面文章分析，我们知道IoC容器读入已经定位的Bean定义资源是从refresh方法开始的，我们首先从AbstractApplicationContext类的refresh方法入手分析，源码如下： 12345678910111213141516171819202122232425262728293031323334353637383940//容器初始化的过程，读入Bean定义资源，并解析注册 public void refresh() throws BeansException, IllegalStateException &#123; synchronized (this.startupShutdownMonitor) &#123; //调用容器准备刷新的方法，获取容器的当时时间，同时给容器设置同步标识 prepareRefresh(); //告诉子类启动refreshBeanFactory()方法，Bean定义资源文件的载入从 //子类的refreshBeanFactory()方法启动 ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); //为BeanFactory配置容器特性，例如类加载器、事件处理器等 prepareBeanFactory(beanFactory); try &#123; //为容器的某些子类指定特殊的BeanPost事件处理器 postProcessBeanFactory(beanFactory); //调用所有注册的BeanFactoryPostProcessor的Bean invokeBeanFactoryPostProcessors(beanFactory); //为BeanFactory注册BeanPost事件处理器. //BeanPostProcessor是Bean后置处理器，用于监听容器触发的事件 registerBeanPostProcessors(beanFactory); //初始化信息源，和国际化相关. initMessageSource(); //初始化容器事件传播器. initApplicationEventMulticaster(); //调用子类的某些特殊Bean初始化方法 onRefresh(); //为事件传播器注册事件监听器. registerListeners(); //这里是对容器lazy-init属性进行处理的入口方法 finishBeanFactoryInitialization(beanFactory); //初始化容器的生命周期事件处理器，并发布容器的生命周期事件 finishRefresh(); &#125; catch (BeansException ex) &#123; //销毁以创建的单态Bean destroyBeans(); //取消refresh操作，重置容器的同步标识. cancelRefresh(ex); throw ex; &#125; &#125; &#125; 在refresh方法中ConfigurableListableBeanFactorybeanFactory = obtainFreshBeanFactory();启动了Bean定义资源的载入、注册过程，而finishBeanFactoryInitialization方法是对注册后的Bean定义中的预实例化(lazy-init=false，Spring默认就是预实例化，即为true)的Bean进行处理的地方。 (2). finishBeanFactoryInitialization处理预实例化Bean 当Bean定义资源被载入IoC容器之后，容器将Bean定义资源解析为容器内部的数据结构BeanDefinition注册到容器中，AbstractApplicationContext类中的finishBeanFactoryInitialization方法对配置了预实例化属性的Bean进行预初始化过程，源码如下： 12345678910111213141516//对配置了lazy-init属性的Bean进行预实例化处理 protected void finishBeanFactoryInitialization(ConfigurableListableBeanFactory beanFactory) &#123; //这是Spring3以后新加的代码，为容器指定一个转换服务(ConversionService) //在对某些Bean属性进行转换时使用 if (beanFactory.containsBean(CONVERSION_SERVICE_BEAN_NAME) &amp;&amp; beanFactory.isTypeMatch(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)) &#123; beanFactory.setConversionService( beanFactory.getBean(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)); &#125; //为了类型匹配，停止使用临时的类加载器 beanFactory.setTempClassLoader(null); //缓存容器中所有注册的BeanDefinition元数据，以防被修改 beanFactory.freezeConfiguration(); //对配置了lazy-init属性的单态模式Bean进行预实例化处理 beanFactory.preInstantiateSingletons(); &#125; ConfigurableListableBeanFactory是一个接口，其preInstantiateSingletons方法由其子类DefaultListableBeanFactory提供。 (3). DefaultListableBeanFactory对配置lazy-init属性单态Bean的预实例化1234567891011121314151617181920212223242526272829303132333435363738394041424344//对配置lazy-init属性单态Bean的预实例化 public void preInstantiateSingletons() throws BeansException &#123; if (this.logger.isInfoEnabled()) &#123; this.logger.info("Pre-instantiating singletons in " + this); &#125; //在对配置lazy-init属性单态Bean的预实例化过程中，必须多线程同步，以确保数据一致性 synchronized (this.beanDefinitionMap) &#123; for (String beanName : this.beanDefinitionNames) &#123; //获取指定名称的Bean定义 RootBeanDefinition bd = getMergedLocalBeanDefinition(beanName); //Bean不是抽象的，是单态模式的，且lazy-init属性配置为false if (!bd.isAbstract() &amp;&amp; bd.isSingleton() &amp;&amp; !bd.isLazyInit()) &#123; //如果指定名称的bean是创建容器的Bean if (isFactoryBean(beanName)) &#123; //FACTORY_BEAN_PREFIX=”&amp;”，当Bean名称前面加”&amp;”符号 //时，获取的是产生容器对象本身，而不是容器产生的Bean. //调用getBean方法，触发容器对Bean实例化和依赖注入过程 final FactoryBean factory = (FactoryBean) getBean(FACTORY_BEAN_PREFIX + beanName); //标识是否需要预实例化 boolean isEagerInit; if (System.getSecurityManager() != null &amp;&amp; factory instanceof SmartFactoryBean) &#123; //一个匿名内部类 isEagerInit = AccessController.doPrivileged(new PrivilegedAction&lt;Boolean&gt;() &#123; public Boolean run() &#123; return ((SmartFactoryBean) factory).isEagerInit(); &#125; &#125;, getAccessControlContext()); &#125; else &#123; isEagerInit = factory instanceof SmartFactoryBean &amp;&amp; ((SmartFactoryBean) factory).isEagerInit(); &#125; if (isEagerInit) &#123; //调用getBean方法，触发容器对Bean实例化和依赖注入过程 getBean(beanName); &#125; &#125; else &#123; //调用getBean方法，触发容器对Bean实例化和依赖注入过程 getBean(beanName); &#125; &#125; &#125; &#125; &#125; 通过对lazy-init处理源码的分析，我们可以看出，如果设置了lazy-init属性，则容器在完成Bean定义的注册之后，会通过getBean方法，触发对指定Bean的初始化和依赖注入过程，这样当应用第一次向容器索取所需的Bean时，容器不再需要对Bean进行初始化和依赖注入，直接从已经完成实例化和依赖注入的Bean中取一个线程的Bean，这样就提高了第一次获取Bean的性能。 FactoryBean的实现在Spring中，有两个很容易混淆的类：BeanFactory和FactoryBean。BeanFactory：Bean工厂，是一个工厂(Factory)，我们Spring IoC容器的最顶层接口就是这个BeanFactory，它的作用是管理Bean，即实例化、定位、配置应用程序中的对象及建立这些对象间的依赖。 FactoryBean：工厂Bean，是一个Bean，作用是产生其他bean实例。通常情况下，这种bean没有什么特别的要求，仅需要提供一个工厂方法，该方法用来返回其他bean实例。通常情况下，bean无须自己实现工厂模式，Spring容器担任工厂角色；但少数情况下，容器中的bean本身就是工厂，其作用是产生其它bean实例。 当用户使用容器本身时，可以使用转义字符”&amp;”来得到FactoryBean本身，以区别通过FactoryBean产生的实例对象和FactoryBean对象本身。在BeanFactory中通过如下代码定义了该转义字符： StringFACTORY_BEAN_PREFIX = “&amp;”; 如果myJndiObject是一个FactoryBean，则使用&amp;myJndiObject得到的是myJndiObject对象，而不是myJndiObject产生出来的对象。 (1).FactoryBean的源码如下： 12345678910//工厂Bean，用于产生其他对public interface FactoryBean&lt;T&gt; &#123; //获取容器管理的对象实例 T getObject() throws Exception; //获取Bean工厂创建的对象的类型 Class getObjectType(); //Bean工厂创建的对象是否是单态模式，如果是单态模式，则整个容器中只有一个实例 //对象，每次请求都返回同一个实例对象 boolean isSingleton();&#125; (2). AbstractBeanFactory的getBean方法调用FactoryBean在前面我们分析Spring Ioc容器实例化Bean并进行依赖注入过程的源码时，提到在getBean方法触发容器实例化Bean的时候会调用AbstractBeanFactory的doGetBean方法来进行实例化的过程，源码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970//真正实现向IoC容器获取Bean的功能，也是触发依赖注入功能的地方 @SuppressWarnings("unchecked") protected &lt;T&gt; T doGetBean( final String name, final Class&lt;T&gt; requiredType, final Object[] args, boolean typeCheckOnly) throws BeansException &#123; //根据指定的名称获取被管理Bean的名称，剥离指定名称中对容器的相关依赖 //如果指定的是别名，将别名转换为规范的Bean名称 final String beanName = transformedBeanName(name); Object bean; //先从缓存中取是否已经有被创建过的单态类型的Bean，对于单态模式的Bean整 //个IoC容器中只创建一次，不需要重复创建 Object sharedInstance = getSingleton(beanName); //IoC容器创建单态模式Bean实例对象 if (sharedInstance != null &amp;&amp; args == null) &#123; if (logger.isDebugEnabled()) &#123; //如果指定名称的Bean在容器中已有单态模式的Bean被创建，直接返回 //已经创建的Bean if (isSingletonCurrentlyInCreation(beanName)) &#123; logger.debug("Returning eagerly cached instance of singleton bean '" + beanName + "' that is not fully initialized yet - a consequence of a circular reference"); &#125; else &#123; logger.debug("Returning cached instance of singleton bean '" + beanName + "'"); &#125; &#125; //获取给定Bean的实例对象，主要是完成FactoryBean的相关处理 bean = getObjectForBeanInstance(sharedInstance, name, beanName, null); &#125; …… &#125; //获取给定Bean的实例对象，主要是完成FactoryBean的相关处理 protected Object getObjectForBeanInstance( Object beanInstance, String name, String beanName, RootBeanDefinition mbd) &#123; //容器已经得到了Bean实例对象，这个实例对象可能是一个普通的Bean，也可能是 //一个工厂Bean，如果是一个工厂Bean，则使用它创建一个Bean实例对象，如果 //调用本身就想获得一个容器的引用，则指定返回这个工厂Bean实例对象 //如果指定的名称是容器的解引用(dereference，即是对象本身而非内存地址)， //且Bean实例也不是创建Bean实例对象的工厂Bean if (BeanFactoryUtils.isFactoryDereference(name) &amp;&amp; !(beanInstance instanceof FactoryBean)) &#123; throw new BeanIsNotAFactoryException(transformedBeanName(name), beanInstance.getClass()); &#125; //如果Bean实例不是工厂Bean，或者指定名称是容器的解引用，调用者向获取对 //容器的引用，则直接返回当前的Bean实例 if (!(beanInstance instanceof FactoryBean) || BeanFactoryUtils.isFactoryDereference(name)) &#123; return beanInstance; &#125; //处理指定名称不是容器的解引用，或者根据名称获取的Bean实例对象是一个工厂Bean //使用工厂Bean创建一个Bean的实例对象 Object object = null; if (mbd == null) &#123; //从Bean工厂缓存中获取给定名称的Bean实例对象 object = getCachedObjectForFactoryBean(beanName); &#125; //让Bean工厂生产给定名称的Bean对象实例 if (object == null) &#123; FactoryBean factory = (FactoryBean) beanInstance; //如果从Bean工厂生产的Bean是单态模式的，则缓存 if (mbd == null &amp;&amp; containsBeanDefinition(beanName)) &#123; //从容器中获取指定名称的Bean定义，如果继承基类，则合并基类相关属性 mbd = getMergedLocalBeanDefinition(beanName); &#125; //如果从容器得到Bean定义信息，并且Bean定义信息不是虚构的，则让工厂 //Bean生产Bean实例对象 boolean synthetic = (mbd != null &amp;&amp; mbd.isSynthetic()); //调用FactoryBeanRegistrySupport类的getObjectFromFactoryBean //方法，实现工厂Bean生产Bean对象实例的过程 object = getObjectFromFactoryBean(factory, beanName, !synthetic); &#125; return object; &#125; (3)、AbstractBeanFactory生产Bean实例对象： AbstractBeanFactory类中生产Bean实例对象的主要源码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172//Bean工厂生产Bean实例对象 protected Object getObjectFromFactoryBean(FactoryBean factory, String beanName, boolean shouldPostProcess) &#123; //Bean工厂是单态模式，并且Bean工厂缓存中存在指定名称的Bean实例对象 if (factory.isSingleton() &amp;&amp; containsSingleton(beanName)) &#123; //多线程同步，以防止数据不一致 synchronized (getSingletonMutex()) &#123; //直接从Bean工厂缓存中获取指定名称的Bean实例对象 Object object = this.factoryBeanObjectCache.get(beanName); //Bean工厂缓存中没有指定名称的实例对象，则生产该实例对象 if (object == null) &#123; //调用Bean工厂的getObject方法生产指定Bean的实例对象 object = doGetObjectFromFactoryBean(factory, beanName, shouldPostProcess); //将生产的实例对象添加到Bean工厂缓存中 this.factoryBeanObjectCache.put(beanName, (object != null ? object : NULL_OBJECT)); &#125; return (object != NULL_OBJECT ? object : null); &#125; &#125; //调用Bean工厂的getObject方法生产指定Bean的实例对象 else &#123; return doGetObjectFromFactoryBean(factory, beanName, shouldPostProcess); &#125; &#125; //调用Bean工厂的getObject方法生产指定Bean的实例对象 private Object doGetObjectFromFactoryBean( final FactoryBean factory, final String beanName, final boolean shouldPostProcess) throws BeanCreationException &#123; Object object; try &#123; if (System.getSecurityManager() != null) &#123; AccessControlContext acc = getAccessControlContext(); try &#123; //实现PrivilegedExceptionAction接口的匿名内置类 //根据JVM检查权限，然后决定BeanFactory创建实例对象 object = AccessController.doPrivileged(new PrivilegedExceptionAction&lt;Object&gt;() &#123; public Object run() throws Exception &#123; //调用BeanFactory接口实现类的创建对象方法 return factory.getObject(); &#125; &#125;, acc); &#125; catch (PrivilegedActionException pae) &#123; throw pae.getException(); &#125; &#125; else &#123; //调用BeanFactory接口实现类的创建对象方法 object = factory.getObject(); &#125; &#125; catch (FactoryBeanNotInitializedException ex) &#123; throw new BeanCurrentlyInCreationException(beanName, ex.toString()); &#125; catch (Throwable ex) &#123; throw new BeanCreationException(beanName, "FactoryBean threw exception on object creation", ex); &#125; //创建出来的实例对象为null，或者因为单态对象正在创建而返回null if (object == null &amp;&amp; isSingletonCurrentlyInCreation(beanName)) &#123; throw new BeanCurrentlyInCreationException( beanName, "FactoryBean which is currently in creation returned null from getObject"); &#125; //为创建出来的Bean实例对象添加BeanPostProcessor后置处理器 if (object != null &amp;&amp; shouldPostProcess) &#123; try &#123; object = postProcessObjectFromFactoryBean(object, beanName); &#125; catch (Throwable ex) &#123; throw new BeanCreationException(beanName, "Post-processing of the FactoryBean's object failed", ex); &#125; &#125; return object; &#125; 从上面的源码分析中，我们可以看出，BeanFactory接口调用其实现类的getObject方法来实现创建Bean实例对象的功能。 (4).工厂Bean的实现类getObject方法创建Bean实例对象：FactoryBean的实现类有非常多，比如：Proxy、RMI、JNDI、ServletContextFactoryBean等等，FactoryBean接口为Spring容器提供了一个很好的封装机制，具体的getObject有不同的实现类根据不同的实现策略来具体提供，我们分析一个最简单的AnnotationTestFactoryBean的实现源码： 12345678910111213141516public class AnnotationTestBeanFactory implements FactoryBean&lt;IJmxTestBean&gt; &#123; private final FactoryCreatedAnnotationTestBean instance = new FactoryCreatedAnnotationTestBean(); public AnnotationTestBeanFactory() &#123; this.instance.setName("FACTORY"); &#125; //AnnotationTestBeanFactory产生Bean实例对象的实现 public IJmxTestBean getObject() throws Exception &#123; return this.instance; &#125; public Class&lt;? extends IJmxTestBean&gt; getObjectType() &#123; return FactoryCreatedAnnotationTestBean.class; &#125; public boolean isSingleton() &#123; return true; &#125; &#125; 其他的Proxy，RMI，JNDI等等，都是根据相应的策略提供getObject的实现。这里不做一一分析，这已经不是Spring的核心功能，有需要的时候再去深入研究。 BeanPostProcessor后置处理器的实现BeanPostProcessor后置处理器是Spring IoC容器经常使用到的一个特性，这个Bean后置处理器是一个监听器，可以监听容器触发的Bean声明周期事件。后置处理器向容器注册以后，容器中管理的Bean就具备了接收IoC容器事件回调的能力。 BeanPostProcessor的使用非常简单，只需要提供一个实现接口BeanPostProcessor的实现类，然后在Bean的配置文件中设置即可。 (1).BeanPostProcessor的源码如下： 12345678package org.springframework.beans.factory.config; import org.springframework.beans.BeansException; public interface BeanPostProcessor &#123; //为在Bean的初始化前提供回调入口 Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException; //为在Bean的初始化之后提供回调入口 Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException; &#125; 这两个回调的入口都是和容器管理的Bean的生命周期事件紧密相关，可以为用户提供在Spring IoC容器初始化Bean过程中自定义的处理操作。 (2).AbstractAutowireCapableBeanFactory类对容器生成的Bean添加后置处理器：BeanPostProcessor后置处理器的调用发生在Spring IoC容器完成对Bean实例对象的创建和属性的依赖注入完成之后，在对Spring依赖注入的源码分析过程中我们知道，当应用程序第一次调用getBean方法(lazy-init预实例化除外)向Spring IoC容器索取指定Bean时触发Spring IoC容器创建Bean实例对象并进行依赖注入的过程，其中真正实现创建Bean对象并进行依赖注入的方法是AbstractAutowireCapableBeanFactory类的doCreateBean方法，主要源码如下： 123456789101112131415161718192021//真正创建Bean的方法 protected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, final Object[] args) &#123; //创建Bean实例对象 …… try &#123; //对Bean属性进行依赖注入 populateBean(beanName, mbd, instanceWrapper); if (exposedObject != null) &#123; //在对Bean实例对象生成和依赖注入完成以后，开始对Bean实例对象 //进行初始化 ，为Bean实例对象应用BeanPostProcessor后置处理器 exposedObject = initializeBean(beanName, exposedObject, mbd); &#125; &#125; catch (Throwable ex) &#123; if (ex instanceof BeanCreationException &amp;&amp; beanName.equals(((BeanCreationException) ex).getBeanName())) &#123; throw (BeanCreationException) ex; &#125; …… //为应用返回所需要的实例对象 return exposedObject; &#125; 从上面的代码中我们知道，为Bean实例对象添加BeanPostProcessor后置处理器的入口的是initializeBean方法。 (3).initializeBean方法为容器产生的Bean实例对象添加BeanPostProcessor后置处理器：同样在AbstractAutowireCapableBeanFactory类中，initializeBean方法实现为容器创建的Bean实例对象添加BeanPostProcessor后置处理器，源码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869//初始容器创建的Bean实例对象，为其添加BeanPostProcessor后置处理器 protected Object initializeBean(final String beanName, final Object bean, RootBeanDefinition mbd) &#123; //JDK的安全机制验证权限 if (System.getSecurityManager() != null) &#123; //实现PrivilegedAction接口的匿名内部类 AccessController.doPrivileged(new PrivilegedAction&lt;Object&gt;() &#123; public Object run() &#123; invokeAwareMethods(beanName, bean); return null; &#125; &#125;, getAccessControlContext()); &#125; else &#123; //为Bean实例对象包装相关属性，如名称，类加载器，所属容器等信息 invokeAwareMethods(beanName, bean); &#125; Object wrappedBean = bean; //对BeanPostProcessor后置处理器的postProcessBeforeInitialization //回调方法的调用，为Bean实例初始化前做一些处理 if (mbd == null || !mbd.isSynthetic()) &#123; wrappedBean = applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName); &#125; //调用Bean实例对象初始化的方法，这个初始化方法是在Spring Bean定义配置 //文件中通过init-method属性指定的 try &#123; invokeInitMethods(beanName, wrappedBean, mbd); &#125; catch (Throwable ex) &#123; throw new BeanCreationException( (mbd != null ? mbd.getResourceDescription() : null), beanName, "Invocation of init method failed", ex); &#125; //对BeanPostProcessor后置处理器的postProcessAfterInitialization //回调方法的调用，为Bean实例初始化之后做一些处理 if (mbd == null || !mbd.isSynthetic()) &#123; wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); &#125; return wrappedBean; &#125; //调用BeanPostProcessor后置处理器实例对象初始化之前的处理方法 public Object applyBeanPostProcessorsBeforeInitialization(Object existingBean, String beanName) throws BeansException &#123; Object result = existingBean; //遍历容器为所创建的Bean添加的所有BeanPostProcessor后置处理器 for (BeanPostProcessor beanProcessor : getBeanPostProcessors()) &#123; //调用Bean实例所有的后置处理中的初始化前处理方法，为Bean实例对象在 //初始化之前做一些自定义的处理操作 result = beanProcessor.postProcessBeforeInitialization(result, beanName); if (result == null) &#123; return result; &#125; &#125; return result; &#125; //调用BeanPostProcessor后置处理器实例对象初始化之后的处理方法 public Object applyBeanPostProcessorsAfterInitialization(Object existingBean, String beanName) throws BeansException &#123; Object result = existingBean; //遍历容器为所创建的Bean添加的所有BeanPostProcessor后置处理器 for (BeanPostProcessor beanProcessor : getBeanPostProcessors()) &#123; //调用Bean实例所有的后置处理中的初始化后处理方法，为Bean实例对象在 //初始化之后做一些自定义的处理操作 result = beanProcessor.postProcessAfterInitialization(result, beanName); if (result == null) &#123; return result; &#125; &#125; return result; &#125; BeanPostProcessor是一个接口，其初始化前的操作方法和初始化后的操作方法均委托其实现子类来实现，在Spring中，BeanPostProcessor的实现子类非常的多，分别完成不同的操作，如：AOP面向切面编程的注册通知适配器、Bean对象的数据校验、Bean继承属性/方法的合并等等，我们以最简单的AOP切面织入来简单了解其主要的功能。 (4).AdvisorAdapterRegistrationManager在Bean对象初始化后注册通知适配器：AdvisorAdapterRegistrationManager是BeanPostProcessor的一个实现类，其主要的作用为容器中管理的Bean注册一个面向切面编程的通知适配器，以便在Spring容器为所管理的Bean进行面向切面编程时提供方便，其源码如下：123456789101112131415161718192021//为容器中管理的Bean注册一个面向切面编程的通知适配器 public class AdvisorAdapterRegistrationManager implements BeanPostProcessor &#123; //容器中负责管理切面通知适配器注册的对象 private AdvisorAdapterRegistry advisorAdapterRegistry = GlobalAdvisorAdapterRegistry.getInstance(); public void setAdvisorAdapterRegistry(AdvisorAdapterRegistry advisorAdapterRegistry) &#123; this.advisorAdapterRegistry = advisorAdapterRegistry; &#125; //BeanPostProcessor在Bean对象初始化前的操作 public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; //没有做任何操作，直接返回容器创建的Bean对象 return bean; &#125; //BeanPostProcessor在Bean对象初始化后的操作 public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; if (bean instanceof AdvisorAdapter)&#123; //如果容器创建的Bean实例对象是一个切面通知适配器，则向容器的注册 this.advisorAdapterRegistry.registerAdvisorAdapter((AdvisorAdapter) bean); &#125; return bean; &#125; &#125; 其他的BeanPostProcessor接口实现类的也类似，都是对Bean对象使用到的一些特性进行处理，或者向IoC容器中注册，为创建的Bean实例对象做一些自定义的功能增加，这些操作是容器初始化Bean时自动触发的，不需要认为的干预。 Spring IoC容器autowiring实现原理Spring IoC容器提供了两种管理Bean依赖关系的方式： 显式管理：通过BeanDefinition的属性值和构造方法实现Bean依赖关系管理。 autowiring：Spring IoC容器的依赖自动装配功能，不需要对Bean属性的依赖关系做显式的声明，只需要在配置好autowiring属性，IoC容器会自动使用反射查找属性的类型和名称，然后基于属性的类型或者名称来自动匹配容器中管理的Bean，从而自动地完成依赖注入。 通过对autowiring自动装配特性的理解，我们知道容器对Bean的自动装配发生在容器对Bean依赖注入的过程中。在前面对Spring IoC容器的依赖注入过程源码分析中，我们已经知道了容器对Bean实例对象的属性注入的处理发生在AbstractAutoWireCapableBeanFactory类中的populateBean方法中，我们通过程序流程分析autowiring的实现原理： (1). AbstractAutoWireCapableBeanFactory对Bean实例进行属性依赖注入：应用第一次通过getBean方法(配置了lazy-init预实例化属性的除外)向IoC容器索取Bean时，容器创建Bean实例对象，并且对Bean实例对象进行属性依赖注入，AbstractAutoWireCapableBeanFactory的populateBean方法就是实现Bean属性依赖注入的功能，其主要源码如下：1234567891011121314151617181920protected void populateBean(String beanName, AbstractBeanDefinition mbd, BeanWrapper bw) &#123; //获取Bean定义的属性值，并对属性值进行处理 PropertyValues pvs = mbd.getPropertyValues(); …… //对依赖注入处理，首先处理autowiring自动装配的依赖注入 if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_NAME || mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_TYPE) &#123; MutablePropertyValues newPvs = new MutablePropertyValues(pvs); //根据Bean名称进行autowiring自动装配处理 if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_NAME) &#123; autowireByName(beanName, mbd, bw, newPvs); &#125; //根据Bean类型进行autowiring自动装配处理 if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_TYPE) &#123; autowireByType(beanName, mbd, bw, newPvs); &#125; &#125; //对非autowiring的属性进行依赖注入处理 …… &#125; (2).Spring IoC容器根据Bean名称或者类型进行autowiring自动依赖注入：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475//根据名称对属性进行自动依赖注入 protected void autowireByName( String beanName, AbstractBeanDefinition mbd, BeanWrapper bw, MutablePropertyValues pvs) &#123; //对Bean对象中非简单属性(不是简单继承的对象，如8中原始类型，字符串，URL等//都是简单属性)进行处理 String[] propertyNames = unsatisfiedNonSimpleProperties(mbd, bw); for (String propertyName : propertyNames) &#123; //如果Spring IoC容器中包含指定名称的Bean if (containsBean(propertyName)) &#123; //调用getBean方法向IoC容器索取指定名称的Bean实例，迭代触发属性的//初始化和依赖注入 Object bean = getBean(propertyName); //为指定名称的属性赋予属性值 pvs.add(propertyName, bean); //指定名称属性注册依赖Bean名称，进行属性依赖注入 registerDependentBean(propertyName, beanName); if (logger.isDebugEnabled()) &#123; logger.debug("Added autowiring by name from bean name '" + beanName + "' via property '" + propertyName + "' to bean named '" + propertyName + "'"); &#125; &#125; else &#123; if (logger.isTraceEnabled()) &#123; logger.trace("Not autowiring property '" + propertyName + "' of bean '" + beanName + "' by name: no matching bean found"); &#125; &#125; &#125; &#125; //根据类型对属性进行自动依赖注入 protected void autowireByType( String beanName, AbstractBeanDefinition mbd, BeanWrapper bw, MutablePropertyValues pvs) &#123; //获取用户定义的类型转换器 TypeConverter converter = getCustomTypeConverter(); if (converter == null) &#123; converter = bw; &#125; //存放解析的要注入的属性 Set&lt;String&gt; autowiredBeanNames = new LinkedHashSet&lt;String&gt;(4); //对Bean对象中非简单属性(不是简单继承的对象，如8中原始类型，字符 //URL等都是简单属性)进行处理 String[] propertyNames = unsatisfiedNonSimpleProperties(mbd, bw); for (String propertyName : propertyNames) &#123; try &#123; //获取指定属性名称的属性描述器 PropertyDescriptor pd = bw.getPropertyDescriptor(propertyName); //不对Object类型的属性进行autowiring自动依赖注入 if (!Object.class.equals(pd.getPropertyType())) &#123; //获取属性的setter方法 MethodParameter methodParam = BeanUtils.getWriteMethodParameter(pd); //检查指定类型是否可以被转换为目标对象的类型 boolean eager = !PriorityOrdered.class.isAssignableFrom(bw.getWrappedClass()); //创建一个要被注入的依赖描述 DependencyDescriptor desc = new AutowireByTypeDependencyDescriptor(methodParam, eager); //根据容器的Bean定义解析依赖关系，返回所有要被注入的Bean对象 Object autowiredArgument = resolveDependency(desc, beanName, autowiredBeanNames, converter); if (autowiredArgument != null) &#123; //为属性赋值所引用的对象 pvs.add(propertyName, autowiredArgument); &#125; for (String autowiredBeanName : autowiredBeanNames) &#123; //指定名称属性注册依赖Bean名称，进行属性依赖注入 registerDependentBean(autowiredBeanName, beanName); if (logger.isDebugEnabled()) &#123; logger.debug("Autowiring by type from bean name '" + beanName + "' via property '" + propertyName + "' to bean named '" + autowiredBeanName + "'"); &#125; &#125; //释放已自动注入的属性 autowiredBeanNames.clear(); &#125; &#125; catch (BeansException ex) &#123; throw new UnsatisfiedDependencyException(mbd.getResourceDescription(), beanName, propertyName, ex); &#125; &#125; &#125; 通过上面的源码分析，我们可以看出来通过属性名进行自动依赖注入的相对比通过属性类型进行自动依赖注入要稍微简单一些，但是真正实现属性注入的是DefaultSingletonBeanRegistry类的registerDependentBean方法。 (3).DefaultSingletonBeanRegistry的registerDependentBean方法对属性注入：12345678910111213141516171819202122232425262728293031//为指定的Bean注入依赖的Bean public void registerDependentBean(String beanName, String dependentBeanName) &#123; //处理Bean名称，将别名转换为规范的Bean名称 String canonicalName = canonicalName(beanName); //多线程同步，保证容器内数据的一致性 //先从容器中：bean名称--&gt;全部依赖Bean名称集合找查找给定名称Bean的依赖Bean synchronized (this.dependentBeanMap) &#123; //获取给定名称Bean的所有依赖Bean名称 Set&lt;String&gt; dependentBeans = this.dependentBeanMap.get(canonicalName); if (dependentBeans == null) &#123; //为Bean设置依赖Bean信息 dependentBeans = new LinkedHashSet&lt;String&gt;(8); this.dependentBeanMap.put(canonicalName, dependentBeans); &#125; //向容器中：bean名称--&gt;全部依赖Bean名称集合添加Bean的依赖信息 //即，将Bean所依赖的Bean添加到容器的集合中 dependentBeans.add(dependentBeanName); &#125; //从容器中：bean名称--&gt;指定名称Bean的依赖Bean集合找查找给定名称 //Bean的依赖Bean synchronized (this.dependenciesForBeanMap) &#123; Set&lt;String&gt; dependenciesForBean = this.dependenciesForBeanMap.get(dependentBeanName); if (dependenciesForBean == null) &#123; dependenciesForBean = new LinkedHashSet&lt;String&gt;(8); this.dependenciesForBeanMap.put(dependentBeanName, dependenciesForBean); &#125; //向容器中：bean名称--&gt;指定Bean的依赖Bean名称集合添加Bean的依赖信息 //即，将Bean所依赖的Bean添加到容器的集合中 dependenciesForBean.add(canonicalName); &#125; &#125; 通过对autowiring的源码分析，我们可以看出，autowiring的实现过程： 对Bean的属性迭代调用getBean方法，完成依赖Bean的初始化和依赖注入。 将依赖Bean的属性引用设置到被依赖的Bean属性上。 将依赖Bean的名称和被依赖Bean的名称存储在IoC容器的集合中。 Spring IoC容器的autowiring属性自动依赖注入是一个很方便的特性，可以简化开发时的配置，但是凡是都有两面性，自动属性依赖注入也有不足，首先，Bean的依赖关系在配置文件中无法很清楚地看出来，对于维护造成一定困难。其次，由于自动依赖注入是Spring容器自动执行的，容器是不会智能判断的，如果配置不当，将会带来无法预料的后果，所以自动依赖注入特性在使用时还是综合考虑。]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[源码解读Spring IOC原理（三）]]></title>
    <url>%2Fjava%2Fspring%2Fjava%2Fspring%2F3%2F</url>
    <content type="text"><![CDATA[源码解读Spring IOC原理（三）IOC容器的依赖注入依赖注入发生的时间当Spring IoC容器完成了Bean定义资源的定位、载入和解析注册以后，IoC容器中已经管理类Bean定义的相关数据，但是此时IoC容器还没有对所管理的Bean进行依赖注入，依赖注入在以下两种情况发生： 用户第一次通过getBean方法向IoC容索要Bean时，IoC容器触发依赖注入。 当用户在Bean定义资源中为元素配置了lazy-init属性，即让容器在解析注册Bean定义时进行预实例化，触发依赖注入。 BeanFactory接口定义了Spring IoC容器的基本功能规范，是Spring IoC容器所应遵守的最底层和最基本的编程规范。BeanFactory接口中定义了几个getBean方法，就是用户向IoC容器索取管理的Bean的方法，我们通过分析其子类的具体实现，理解Spring IoC容器在用户索取Bean时如何完成依赖注入。 在BeanFactory中我们看到getBean（String…）函数，它的具体实现在AbstractBeanFactory中 AbstractBeanFactory通过getBean向IoC容器获取被管理的BeanAbstractBeanFactory的getBean相关方法的源码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169//获取IoC容器中指定名称的Bean public Object getBean(String name) throws BeansException &#123; //doGetBean才是真正向IoC容器获取被管理Bean的过程 return doGetBean(name, null, null, false); &#125; //获取IoC容器中指定名称和类型的Bean public &lt;T&gt; T getBean(String name, Class&lt;T&gt; requiredType) throws BeansException &#123; //doGetBean才是真正向IoC容器获取被管理Bean的过程 return doGetBean(name, requiredType, null, false); &#125; //获取IoC容器中指定名称和参数的Bean public Object getBean(String name, Object... args) throws BeansException &#123; //doGetBean才是真正向IoC容器获取被管理Bean的过程 return doGetBean(name, null, args, false); &#125; //获取IoC容器中指定名称、类型和参数的Bean public &lt;T&gt; T getBean(String name, Class&lt;T&gt; requiredType, Object... args) throws BeansException &#123; //doGetBean才是真正向IoC容器获取被管理Bean的过程 return doGetBean(name, requiredType, args, false); &#125; //真正实现向IoC容器获取Bean的功能，也是触发依赖注入功能的地方 @SuppressWarnings("unchecked") protected &lt;T&gt; T doGetBean( final String name, final Class&lt;T&gt; requiredType, final Object[] args, boolean typeCheckOnly) throws BeansException &#123; //根据指定的名称获取被管理Bean的名称，剥离指定名称中对容器的相关依赖 //如果指定的是别名，将别名转换为规范的Bean名称 final String beanName = transformedBeanName(name); Object bean; //先从缓存中取是否已经有被创建过的单态类型的Bean，对于单态模式的Bean整 //个IoC容器中只创建一次，不需要重复创建 Object sharedInstance = getSingleton(beanName); //IoC容器创建单态模式Bean实例对象 if (sharedInstance != null &amp;&amp; args == null) &#123; if (logger.isDebugEnabled()) &#123; //如果指定名称的Bean在容器中已有单态模式的Bean被创建，直接返回 //已经创建的Bean if (isSingletonCurrentlyInCreation(beanName)) &#123; logger.debug("Returning eagerly cached instance of singleton bean '" + beanName + "' that is not fully initialized yet - a consequence of a circular reference"); &#125; else &#123; logger.debug("Returning cached instance of singleton bean '" + beanName + "'"); &#125; &#125; //获取给定Bean的实例对象，主要是完成FactoryBean的相关处理 //注意：BeanFactory是管理容器中Bean的工厂，而FactoryBean是 //创建创建对象的工厂Bean，两者之间有区别 bean = getObjectForBeanInstance(sharedInstance, name, beanName, null); &#125; else &#123;//缓存没有正在创建的单态模式Bean //缓存中已经有已经创建的原型模式Bean，但是由于循环引用的问题导致实 //例化对象失败 if (isPrototypeCurrentlyInCreation(beanName)) &#123; throw new BeanCurrentlyInCreationException(beanName); &#125; //对IoC容器中是否存在指定名称的BeanDefinition进行检查，首先检查是否 //能在当前的BeanFactory中获取的所需要的Bean，如果不能则委托当前容器 //的父级容器去查找，如果还是找不到则沿着容器的继承体系向父级容器查找 BeanFactory parentBeanFactory = getParentBeanFactory(); //当前容器的父级容器存在，且当前容器中不存在指定名称的Bean if (parentBeanFactory != null &amp;&amp; !containsBeanDefinition(beanName)) &#123; //解析指定Bean名称的原始名称 String nameToLookup = originalBeanName(name); if (args != null) &#123; //委派父级容器根据指定名称和显式的参数查找 return (T) parentBeanFactory.getBean(nameToLookup, args); &#125; else &#123; //委派父级容器根据指定名称和类型查找 return parentBeanFactory.getBean(nameToLookup, requiredType); &#125; &#125; //创建的Bean是否需要进行类型验证，一般不需要 if (!typeCheckOnly) &#123; //向容器标记指定的Bean已经被创建 markBeanAsCreated(beanName); &#125; //根据指定Bean名称获取其父级的Bean定义，主要解决Bean继承时子类 //合并父类公共属性问题 final RootBeanDefinition mbd = getMergedLocalBeanDefinition(beanName); checkMergedBeanDefinition(mbd, beanName, args); //获取当前Bean所有依赖Bean的名称 String[] dependsOn = mbd.getDependsOn(); //如果当前Bean有依赖Bean if (dependsOn != null) &#123; for (String dependsOnBean : dependsOn) &#123; //递归调用getBean方法，获取当前Bean的依赖Bean getBean(dependsOnBean); //把被依赖Bean注册给当前依赖的Bean registerDependentBean(dependsOnBean, beanName); &#125; &#125; //创建单态模式Bean的实例对象 if (mbd.isSingleton()) &#123; //这里使用了一个匿名内部类，创建Bean实例对象，并且注册给所依赖的对象 sharedInstance = getSingleton(beanName, new ObjectFactory() &#123; public Object getObject() throws BeansException &#123; try &#123; //创建一个指定Bean实例对象，如果有父级继承，则合并子//类和父类的定义 return createBean(beanName, mbd, args); &#125; catch (BeansException ex) &#123; //显式地从容器单态模式Bean缓存中清除实例对象 destroySingleton(beanName); throw ex; &#125; &#125; &#125;); //获取给定Bean的实例对象 bean = getObjectForBeanInstance(sharedInstance, name, beanName, mbd); &#125; //IoC容器创建原型模式Bean实例对象 else if (mbd.isPrototype()) &#123; //原型模式(Prototype)是每次都会创建一个新的对象 Object prototypeInstance = null; try &#123; //回调beforePrototypeCreation方法，默认的功能是注册当前创//建的原型对象 beforePrototypeCreation(beanName); //创建指定Bean对象实例 prototypeInstance = createBean(beanName, mbd, args); &#125; finally &#123; //回调afterPrototypeCreation方法，默认的功能告诉IoC容器指//定Bean的原型对象不再创建了 afterPrototypeCreation(beanName); &#125; //获取给定Bean的实例对象 bean = getObjectForBeanInstance(prototypeInstance, name, beanName, mbd); &#125; //要创建的Bean既不是单态模式，也不是原型模式，则根据Bean定义资源中 //配置的生命周期范围，选择实例化Bean的合适方法，这种在Web应用程序中 //比较常用，如：request、session、application等生命周期 else &#123; String scopeName = mbd.getScope(); final Scope scope = this.scopes.get(scopeName); //Bean定义资源中没有配置生命周期范围，则Bean定义不合法 if (scope == null) &#123; throw new IllegalStateException("No Scope registered for scope '" + scopeName + "'"); &#125; try &#123; //这里又使用了一个匿名内部类，获取一个指定生命周期范围的实例 Object scopedInstance = scope.get(beanName, new ObjectFactory() &#123; public Object getObject() throws BeansException &#123; beforePrototypeCreation(beanName); try &#123; return createBean(beanName, mbd, args); &#125; finally &#123; afterPrototypeCreation(beanName); &#125; &#125; &#125;); //获取给定Bean的实例对象 bean = getObjectForBeanInstance(scopedInstance, name, beanName, mbd); &#125; catch (IllegalStateException ex) &#123; throw new BeanCreationException(beanName, "Scope '" + scopeName + "' is not active for the current thread; " + "consider defining a scoped proxy for this bean if you intend to refer to it from a singleton", ex); &#125; &#125; &#125; //对创建的Bean实例对象进行类型检查 if (requiredType != null &amp;&amp; bean != null &amp;&amp; !requiredType.isAssignableFrom(bean.getClass())) &#123; throw new BeanNotOfRequiredTypeException(name, requiredType, bean.getClass()); &#125; return (T) bean; &#125; 通过上面对向IoC容器获取Bean方法的分析，我们可以看到在Spring中，如果Bean定义的单态模式(Singleton)，则容器在创建之前先从缓存中查找，以确保整个容器中只存在一个实例对象。如果Bean定义的是原型模式(Prototype)，则容器每次都会创建一个新的实例对象。除此之外，Bean定义还可以扩展为指定其生命周期范围。 上面的源码只是定义了根据Bean定义的模式，采取的不同创建Bean实例对象的策略，具体的Bean实例对象的创建过程由实现了ObejctFactory接口的匿名内部类的createBean方法完成，ObejctFactory使用委派模式，具体的Bean实例创建过程交由其实现类AbstractAutowireCapableBeanFactory完成，我们继续分析AbstractAutowireCapableBeanFactory的createBean方法的源码，理解其创建Bean实例的具体实现过程。 AbstractAutowireCapableBeanFactory创建Bean实例对象AbstractAutowireCapableBeanFactory类实现了ObejctFactory接口，创建容器指定的Bean实例对象，同时还对创建的Bean实例对象进行初始化处理。其创建Bean实例对象的方法源码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131//创建Bean实例对象 protected Object createBean(final String beanName, final RootBeanDefinition mbd, final Object[] args) throws BeanCreationException &#123; if (logger.isDebugEnabled()) &#123; logger.debug("Creating instance of bean '" + beanName + "'"); &#125; //判断需要创建的Bean是否可以实例化，即是否可以通过当前的类加载器加载 resolveBeanClass(mbd, beanName); //校验和准备Bean中的方法覆盖 try &#123; mbd.prepareMethodOverrides(); &#125; catch (BeanDefinitionValidationException ex) &#123; throw new BeanDefinitionStoreException(mbd.getResourceDescription(), beanName, "Validation of method overrides failed", ex); &#125; try &#123; //如果Bean配置了初始化前和初始化后的处理器，则试图返回一个需要创建//Bean的代理对象 Object bean = resolveBeforeInstantiation(beanName, mbd); if (bean != null) &#123; return bean; &#125; &#125; catch (Throwable ex) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, "BeanPostProcessor before instantiation of bean failed", ex); &#125; //创建Bean的入口 Object beanInstance = doCreateBean(beanName, mbd, args); if (logger.isDebugEnabled()) &#123; logger.debug("Finished creating instance of bean '" + beanName + "'"); &#125; return beanInstance; &#125; //真正创建Bean的方法 protected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, final Object[] args) &#123; //封装被创建的Bean对象 BeanWrapper instanceWrapper = null; if (mbd.isSingleton())&#123;//单态模式的Bean，先从容器中缓存中获取同名Bean instanceWrapper = this.factoryBeanInstanceCache.remove(beanName); &#125; if (instanceWrapper == null) &#123; //创建实例对象 instanceWrapper = createBeanInstance(beanName, mbd, args); &#125; final Object bean = (instanceWrapper != null ? instanceWrapper.getWrappedInstance() : null); //获取实例化对象的类型 Class beanType = (instanceWrapper != null ? instanceWrapper.getWrappedClass() : null); //调用PostProcessor后置处理器 synchronized (mbd.postProcessingLock) &#123; if (!mbd.postProcessed) &#123; applyMergedBeanDefinitionPostProcessors(mbd, beanType, beanName); mbd.postProcessed = true; &#125; &#125; // Eagerly cache singletons to be able to resolve circular references //向容器中缓存单态模式的Bean对象，以防循环引用 boolean earlySingletonExposure = (mbd.isSingleton() &amp;&amp; this.allowCircularReferences &amp;&amp; isSingletonCurrentlyInCreation(beanName)); if (earlySingletonExposure) &#123; if (logger.isDebugEnabled()) &#123; logger.debug("Eagerly caching bean '" + beanName + "' to allow for resolving potential circular references"); &#125; //这里是一个匿名内部类，为了防止循环引用，尽早持有对象的引用 addSingletonFactory(beanName, new ObjectFactory() &#123; public Object getObject() throws BeansException &#123; return getEarlyBeanReference(beanName, mbd, bean); &#125; &#125;); &#125; //Bean对象的初始化，依赖注入在此触发 //这个exposedObject在初始化完成之后返回作为依赖注入完成后的Bean Object exposedObject = bean; try &#123; //将Bean实例对象封装，并且Bean定义中配置的属性值赋值给实例对象 populateBean(beanName, mbd, instanceWrapper); if (exposedObject != null) &#123; //初始化Bean对象 exposedObject = initializeBean(beanName, exposedObject, mbd); &#125; &#125; catch (Throwable ex) &#123; if (ex instanceof BeanCreationException &amp;&amp; beanName.equals(((BeanCreationException) ex).getBeanName())) &#123; throw (BeanCreationException) ex; &#125; else &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, "Initialization of bean failed", ex); &#125; &#125; if (earlySingletonExposure) &#123; //获取指定名称的已注册的单态模式Bean对象 Object earlySingletonReference = getSingleton(beanName, false); if (earlySingletonReference != null) &#123; //根据名称获取的以注册的Bean和正在实例化的Bean是同一个 if (exposedObject == bean) &#123; //当前实例化的Bean初始化完成 exposedObject = earlySingletonReference; &#125; //当前Bean依赖其他Bean，并且当发生循环引用时不允许新创建实例对象 else if (!this.allowRawInjectionDespiteWrapping &amp;&amp; hasDependentBean(beanName)) &#123; String[] dependentBeans = getDependentBeans(beanName); Set&lt;String&gt; actualDependentBeans = new LinkedHashSet&lt;String&gt;(dependentBeans.length); //获取当前Bean所依赖的其他Bean for (String dependentBean : dependentBeans) &#123; //对依赖Bean进行类型检查 if (!removeSingletonIfCreatedForTypeCheckOnly(dependentBean)) &#123; actualDependentBeans.add(dependentBean); &#125; &#125; if (!actualDependentBeans.isEmpty()) &#123; throw new BeanCurrentlyInCreationException(beanName, "Bean with name '" + beanName + "' has been injected into other beans [" + StringUtils.collectionToCommaDelimitedString(actualDependentBeans) + "] in its raw version as part of a circular reference, but has eventually been " + "wrapped. This means that said other beans do not use the final version of the " + "bean. This is often the result of over-eager type matching - consider using " + "'getBeanNamesOfType' with the 'allowEagerInit' flag turned off, for example."); &#125; &#125; &#125; &#125; //注册完成依赖注入的Bean try &#123; registerDisposableBeanIfNecessary(beanName, bean, mbd); &#125; catch (BeanDefinitionValidationException ex) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, "Invalid destruction signature", ex); &#125; return exposedObject; &#125; 通过对方法源码的分析，我们看到具体的依赖注入实现在以下两个方法中： createBeanInstance：生成Bean所包含的java对象实例。 populateBean ：对Bean属性的依赖注入进行处理。 下面继续分析这两个方法的代码实现。 createBeanInstance方法创建Bean的java实例对象在createBeanInstance方法中，根据指定的初始化策略，使用静态工厂、工厂方法或者容器的自动装配特性生成java实例对象，创建对象的源码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172//创建Bean的实例对象 protected BeanWrapper createBeanInstance(String beanName, RootBeanDefinition mbd, Object[] args) &#123; //检查确认Bean是可实例化的 Class beanClass = resolveBeanClass(mbd, beanName); //使用工厂方法对Bean进行实例化 if (beanClass != null &amp;&amp; !Modifier.isPublic(beanClass.getModifiers()) &amp;&amp; !mbd.isNonPublicAccessAllowed()) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, "Bean class isn't public, and non-public access not allowed: " + beanClass.getName()); &#125; if (mbd.getFactoryMethodName() != null) &#123; //调用工厂方法实例化 return instantiateUsingFactoryMethod(beanName, mbd, args); &#125; //使用容器的自动装配方法进行实例化 boolean resolved = false; boolean autowireNecessary = false; if (args == null) &#123; synchronized (mbd.constructorArgumentLock) &#123; if (mbd.resolvedConstructorOrFactoryMethod != null) &#123; resolved = true; autowireNecessary = mbd.constructorArgumentsResolved; &#125; &#125; &#125; if (resolved) &#123; if (autowireNecessary) &#123; //配置了自动装配属性，使用容器的自动装配实例化 //容器的自动装配是根据参数类型匹配Bean的构造方法 return autowireConstructor(beanName, mbd, null, null); &#125; else &#123; //使用默认的无参构造方法实例化 return instantiateBean(beanName, mbd); &#125; &#125; //使用Bean的构造方法进行实例化 Constructor[] ctors = determineConstructorsFromBeanPostProcessors(beanClass, beanName); if (ctors != null || mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_CONSTRUCTOR || mbd.hasConstructorArgumentValues() || !ObjectUtils.isEmpty(args)) &#123; //使用容器的自动装配特性，调用匹配的构造方法实例化 return autowireConstructor(beanName, mbd, ctors, args); &#125; //使用默认的无参构造方法实例化 return instantiateBean(beanName, mbd); &#125; //使用默认的无参构造方法实例化Bean对象 protected BeanWrapper instantiateBean(final String beanName, final RootBeanDefinition mbd) &#123; try &#123; Object beanInstance; final BeanFactory parent = this; //获取系统的安全管理接口，JDK标准的安全管理API if (System.getSecurityManager() != null) &#123; //这里是一个匿名内置类，根据实例化策略创建实例对象 beanInstance = AccessController.doPrivileged(new PrivilegedAction&lt;Object&gt;() &#123; public Object run() &#123; return getInstantiationStrategy().instantiate(mbd, beanName, parent); &#125; &#125;, getAccessControlContext()); &#125; else &#123; //将实例化的对象封装起来 beanInstance = getInstantiationStrategy().instantiate(mbd, beanName, parent); &#125; BeanWrapper bw = new BeanWrapperImpl(beanInstance); initBeanWrapper(bw); return bw; &#125; catch (Throwable ex) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, "Instantiation of bean failed", ex); &#125; &#125; 经过对上面的代码分析，我们可以看出，对使用工厂方法和自动装配特性的Bean的实例化相当比较清楚，调用相应的工厂方法或者参数匹配的构造方法即可完成实例化对象的工作，但是对于我们最常使用的默认无参构造方法就需要使用相应的初始化策略(JDK的反射机制或者CGLIB)来进行初始化了，在方法getInstantiationStrategy().instantiate中就具体实现类使用初始策略实例化对象。 SimpleInstantiationStrategy类使用默认的无参构造方法创建Bean实例化对象在使用默认的无参构造方法创建Bean的实例化对象时，方法getInstantiationStrategy().instantiate调用了SimpleInstantiationStrategy类中的实例化Bean的方法，其源码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142//使用初始化策略实例化Bean对象 public Object instantiate(RootBeanDefinition beanDefinition, String beanName, BeanFactory owner) &#123; //如果Bean定义中没有方法覆盖，则就不需要CGLIB父类类的方法 if (beanDefinition.getMethodOverrides().isEmpty()) &#123; Constructor&lt;?&gt; constructorToUse; synchronized (beanDefinition.constructorArgumentLock) &#123; //获取对象的构造方法或工厂方法 constructorToUse = (Constructor&lt;?&gt;) beanDefinition.resolvedConstructorOrFactoryMethod; //如果没有构造方法且没有工厂方法 if (constructorToUse == null) &#123; //使用JDK的反射机制，判断要实例化的Bean是否是接口 final Class clazz = beanDefinition.getBeanClass(); if (clazz.isInterface()) &#123; throw new BeanInstantiationException(clazz, "Specified class is an interface"); &#125; try &#123; if (System.getSecurityManager() != null) &#123; //这里是一个匿名内置类，使用反射机制获取Bean的构造方法 constructorToUse = AccessController.doPrivileged(new PrivilegedExceptionAction&lt;Constructor&gt;() &#123; public Constructor run() throws Exception &#123; return clazz.getDeclaredConstructor((Class[]) null); &#125; &#125;); &#125; else &#123; constructorToUse = clazz.getDeclaredConstructor((Class[]) null); &#125; beanDefinition.resolvedConstructorOrFactoryMethod = constructorToUse; &#125; catch (Exception ex) &#123; throw new BeanInstantiationException(clazz, "No default constructor found", ex); &#125; &#125; &#125; //使用BeanUtils实例化，通过反射机制调用”构造方法.newInstance(arg)”来进行实例化 return BeanUtils.instantiateClass(constructorToUse); &#125; else &#123; //使用CGLIB来实例化对象 return instantiateWithMethodInjection(beanDefinition, beanName, owner); &#125; &#125; 通过上面的代码分析，我们看到了如果Bean有方法被覆盖了，则使用JDK的反射机制进行实例化，否则，使用CGLIB进行实例化。 instantiateWithMethodInjection方法调用SimpleInstantiationStrategy的子类CglibSubclassingInstantiationStrategy使用CGLIB来进行初始化，其源码如下：1234567891011121314151617//使用CGLIB进行Bean对象实例化 public Object instantiate(Constructor ctor, Object[] args) &#123; //CGLIB中的类 Enhancer enhancer = new Enhancer(); //将Bean本身作为其基类 enhancer.setSuperclass(this.beanDefinition.getBeanClass()); enhancer.setCallbackFilter(new CallbackFilterImpl()); enhancer.setCallbacks(new Callback[] &#123; NoOp.INSTANCE, new LookupOverrideMethodInterceptor(), new ReplaceOverrideMethodInterceptor() &#125;); //使用CGLIB的create方法生成实例对象 return (ctor == null) ? enhancer.create() : enhancer.create(ctor.getParameterTypes(), args); &#125; CGLIB是一个常用的字节码生成器的类库，它提供了一系列API实现java字节码的生成和转换功能。我们在学习JDK的动态代理时都知道，JDK的动态代理只能针对接口，如果一个类没有实现任何接口，要对其进行动态代理只能使用CGLIB。 populateBean方法对Bean属性的依赖注入在第3步的分析中我们已经了解到Bean的依赖注入分为以下两个过程： createBeanInstance：生成Bean所包含的java对象实例 populateBean ：对Bean属性的依赖注入进行处理 第4、5步中我们已经分析了容器初始化生成Bean所包含的Java实例对象的过程，现在我们继续分析生成对象后，Spring IoC容器是如何将Bean的属性依赖关系注入Bean实例对象中并设置好的，属性依赖注入的代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175//将Bean属性设置到生成的实例对象上 protected void populateBean(String beanName, AbstractBeanDefinition mbd, BeanWrapper bw) &#123; //获取容器在解析Bean定义资源时为BeanDefiniton中设置的属性值 PropertyValues pvs = mbd.getPropertyValues(); //实例对象为null if (bw == null) &#123; //属性值不为空 if (!pvs.isEmpty()) &#123; throw new BeanCreationException( mbd.getResourceDescription(), beanName, "Cannot apply property values to null instance"); &#125; else &#123; //实例对象为null，属性值也为空，不需要设置属性值，直接返回 return; &#125; &#125; //在设置属性之前调用Bean的PostProcessor后置处理器 boolean continueWithPropertyPopulation = true; if (!mbd.isSynthetic() &amp;&amp; hasInstantiationAwareBeanPostProcessors()) &#123; for (BeanPostProcessor bp : getBeanPostProcessors()) &#123; if (bp instanceof InstantiationAwareBeanPostProcessor) &#123; InstantiationAwareBeanPostProcessor ibp = (InstantiationAwareBeanPostProcessor) bp; if (!ibp.postProcessAfterInstantiation(bw.getWrappedInstance(), beanName)) &#123; continueWithPropertyPopulation = false; break; &#125; &#125; &#125; &#125; if (!continueWithPropertyPopulation) &#123; return; &#125; //依赖注入开始，首先处理autowire自动装配的注入 if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_NAME || mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_TYPE) &#123; MutablePropertyValues newPvs = new MutablePropertyValues(pvs); //对autowire自动装配的处理，根据Bean名称自动装配注入 if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_NAME) &#123; autowireByName(beanName, mbd, bw, newPvs); &#125; //根据Bean类型自动装配注入 if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_TYPE) &#123; autowireByType(beanName, mbd, bw, newPvs); &#125; pvs = newPvs; &#125; //检查容器是否持有用于处理单态模式Bean关闭时的后置处理器 boolean hasInstAwareBpps = hasInstantiationAwareBeanPostProcessors(); //Bean实例对象没有依赖，即没有继承基类 boolean needsDepCheck = (mbd.getDependencyCheck() != RootBeanDefinition.DEPENDENCY_CHECK_NONE); if (hasInstAwareBpps || needsDepCheck) &#123; //从实例对象中提取属性描述符 PropertyDescriptor[] filteredPds = filterPropertyDescriptorsForDependencyCheck(bw); if (hasInstAwareBpps) &#123; for (BeanPostProcessor bp : getBeanPostProcessors()) &#123; if (bp instanceof InstantiationAwareBeanPostProcessor) &#123; InstantiationAwareBeanPostProcessor ibp = (InstantiationAwareBeanPostProcessor) bp; //使用BeanPostProcessor处理器处理属性值 pvs = ibp.postProcessPropertyValues(pvs, filteredPds, bw.getWrappedInstance(), beanName); if (pvs == null) &#123; return; &#125; &#125; &#125; &#125; if (needsDepCheck) &#123; //为要设置的属性进行依赖检查 checkDependencies(beanName, mbd, filteredPds, pvs); &#125; &#125; //对属性进行注入 applyPropertyValues(beanName, mbd, bw, pvs); &#125; //解析并注入依赖属性的过程 protected void applyPropertyValues(String beanName, BeanDefinition mbd, BeanWrapper bw, PropertyValues pvs) &#123; if (pvs == null || pvs.isEmpty()) &#123; return; &#125; //封装属性值 MutablePropertyValues mpvs = null; List&lt;PropertyValue&gt; original; if (System.getSecurityManager()!= null) &#123; if (bw instanceof BeanWrapperImpl) &#123; //设置安全上下文，JDK安全机制 ((BeanWrapperImpl) bw).setSecurityContext(getAccessControlContext()); &#125; &#125; if (pvs instanceof MutablePropertyValues) &#123; mpvs = (MutablePropertyValues) pvs; //属性值已经转换 if (mpvs.isConverted()) &#123; try &#123; //为实例化对象设置属性值 bw.setPropertyValues(mpvs); return; &#125; catch (BeansException ex) &#123; throw new BeanCreationException( mbd.getResourceDescription(), beanName, "Error setting property values", ex); &#125; &#125; //获取属性值对象的原始类型值 original = mpvs.getPropertyValueList(); &#125; else &#123; original = Arrays.asList(pvs.getPropertyValues()); &#125; //获取用户自定义的类型转换 TypeConverter converter = getCustomTypeConverter(); if (converter == null) &#123; converter = bw; &#125; //创建一个Bean定义属性值解析器，将Bean定义中的属性值解析为Bean实例对象 //的实际值 BeanDefinitionValueResolver valueResolver = new BeanDefinitionValueResolver(this, beanName, mbd, converter); //为属性的解析值创建一个拷贝，将拷贝的数据注入到实例对象中 List&lt;PropertyValue&gt; deepCopy = new ArrayList&lt;PropertyValue&gt;(original.size()); boolean resolveNecessary = false; for (PropertyValue pv : original) &#123; //属性值不需要转换 if (pv.isConverted()) &#123; deepCopy.add(pv); &#125; //属性值需要转换 else &#123; String propertyName = pv.getName(); //原始的属性值，即转换之前的属性值 Object originalValue = pv.getValue(); //转换属性值，例如将引用转换为IoC容器中实例化对象引用 Object resolvedValue = valueResolver.resolveValueIfNecessary(pv, originalValue); //转换之后的属性值 Object convertedValue = resolvedValue; //属性值是否可以转换 boolean convertible = bw.isWritableProperty(propertyName) &amp;&amp; !PropertyAccessorUtils.isNestedOrIndexedProperty(propertyName); if (convertible) &#123; //使用用户自定义的类型转换器转换属性值 convertedValue = convertForProperty(resolvedValue, propertyName, bw, converter); &#125; //存储转换后的属性值，避免每次属性注入时的转换工作 if (resolvedValue == originalValue) &#123; if (convertible) &#123; //设置属性转换之后的值 pv.setConvertedValue(convertedValue); &#125; deepCopy.add(pv); &#125; //属性是可转换的，且属性原始值是字符串类型，且属性的原始类型值不是 //动态生成的字符串，且属性的原始值不是集合或者数组类型 else if (convertible &amp;&amp; originalValue instanceof TypedStringValue &amp;&amp; !((TypedStringValue) originalValue).isDynamic() &amp;&amp; !(convertedValue instanceof Collection || ObjectUtils.isArray(convertedValue))) &#123; pv.setConvertedValue(convertedValue); deepCopy.add(pv); &#125; else &#123; resolveNecessary = true; //重新封装属性的值 deepCopy.add(new PropertyValue(pv, convertedValue)); &#125; &#125; &#125; if (mpvs != null &amp;&amp; !resolveNecessary) &#123; //标记属性值已经转换过 mpvs.setConverted(); &#125; //进行属性依赖注入 try &#123; bw.setPropertyValues(new MutablePropertyValues(deepCopy)); &#125; catch (BeansException ex) &#123; throw new BeanCreationException( mbd.getResourceDescription(), beanName, "Error setting property values", ex); &#125; &#125; 分析上述代码，我们可以看出，对属性的注入过程分以下两种情况： 属性值类型不需要转换时，不需要解析属性值，直接准备进行依赖注入。 属性值需要进行类型转换时，如对其他对象的引用等，首先需要解析属性值，然后对解析后的属性值进行依赖注入。 对属性值的解析是在BeanDefinitionValueResolver类中的resolveValueIfNecessary方法中进行的，对属性值的依赖注入是通过bw.setPropertyValues方法实现的，在分析属性值的依赖注入之前，我们先分析一下对属性值的解析过程。 BeanDefinitionValueResolver解析属性值当容器在对属性进行依赖注入时，如果发现属性值需要进行类型转换，如属性值是容器中另一个Bean实例对象的引用，则容器首先需要根据属性值解析出所引用的对象，然后才能将该引用对象注入到目标实例对象的属性上去，对属性进行解析的由resolveValueIfNecessary方法实现，其源码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188//解析属性值，对注入类型进行转换 public Object resolveValueIfNecessary(Object argName, Object value) &#123; //对引用类型的属性进行解析 if (value instanceof RuntimeBeanReference) &#123; RuntimeBeanReference ref = (RuntimeBeanReference) value; //调用引用类型属性的解析方法 return resolveReference(argName, ref); &#125; //对属性值是引用容器中另一个Bean名称的解析 else if (value instanceof RuntimeBeanNameReference) &#123; String refName = ((RuntimeBeanNameReference) value).getBeanName(); refName = String.valueOf(evaluate(refName)); //从容器中获取指定名称的Bean if (!this.beanFactory.containsBean(refName)) &#123; throw new BeanDefinitionStoreException( "Invalid bean name '" + refName + "' in bean reference for " + argName); &#125; return refName; &#125; //对Bean类型属性的解析，主要是Bean中的内部类 else if (value instanceof BeanDefinitionHolder) &#123; BeanDefinitionHolder bdHolder = (BeanDefinitionHolder) value; return resolveInnerBean(argName, bdHolder.getBeanName(), bdHolder.getBeanDefinition()); &#125; else if (value instanceof BeanDefinition) &#123; BeanDefinition bd = (BeanDefinition) value; return resolveInnerBean(argName, "(inner bean)", bd); &#125; //对集合数组类型的属性解析 else if (value instanceof ManagedArray) &#123; ManagedArray array = (ManagedArray) value; //获取数组的类型 Class elementType = array.resolvedElementType; if (elementType == null) &#123; //获取数组元素的类型 String elementTypeName = array.getElementTypeName(); if (StringUtils.hasText(elementTypeName)) &#123; try &#123; //使用反射机制创建指定类型的对象 elementType = ClassUtils.forName(elementTypeName, this.beanFactory.getBeanClassLoader()); array.resolvedElementType = elementType; &#125; catch (Throwable ex) &#123; throw new BeanCreationException( this.beanDefinition.getResourceDescription(), this.beanName, "Error resolving array type for " + argName, ex); &#125; &#125; //没有获取到数组的类型，也没有获取到数组元素的类型，则直接设置数 //组的类型为Object else &#123; elementType = Object.class; &#125; &#125; //创建指定类型的数组 return resolveManagedArray(argName, (List&lt;?&gt;) value, elementType); &#125; //解析list类型的属性值 else if (value instanceof ManagedList) &#123; return resolveManagedList(argName, (List&lt;?&gt;) value); &#125; //解析set类型的属性值 else if (value instanceof ManagedSet) &#123; return resolveManagedSet(argName, (Set&lt;?&gt;) value); &#125; //解析map类型的属性值 else if (value instanceof ManagedMap) &#123; return resolveManagedMap(argName, (Map&lt;?, ?&gt;) value); &#125; //解析props类型的属性值，props其实就是key和value均为字符串的map else if (value instanceof ManagedProperties) &#123; Properties original = (Properties) value; //创建一个拷贝，用于作为解析后的返回值 Properties copy = new Properties(); for (Map.Entry propEntry : original.entrySet()) &#123; Object propKey = propEntry.getKey(); Object propValue = propEntry.getValue(); if (propKey instanceof TypedStringValue) &#123; propKey = evaluate((TypedStringValue) propKey); &#125; if (propValue instanceof TypedStringValue) &#123; propValue = evaluate((TypedStringValue) propValue); &#125; copy.put(propKey, propValue); &#125; return copy; &#125; //解析字符串类型的属性值 else if (value instanceof TypedStringValue) &#123; TypedStringValue typedStringValue = (TypedStringValue) value; Object valueObject = evaluate(typedStringValue); try &#123; //获取属性的目标类型 Class&lt;?&gt; resolvedTargetType = resolveTargetType(typedStringValue); if (resolvedTargetType != null) &#123; //对目标类型的属性进行解析，递归调用 return this.typeConverter.convertIfNecessary(valueObject, resolvedTargetType); &#125; //没有获取到属性的目标对象，则按Object类型返回 else &#123; return valueObject; &#125; &#125; catch (Throwable ex) &#123; throw new BeanCreationException( this.beanDefinition.getResourceDescription(), this.beanName, "Error converting typed String value for " + argName, ex); &#125; &#125; else &#123; return evaluate(value); &#125; &#125; //解析引用类型的属性值 private Object resolveReference(Object argName, RuntimeBeanReference ref) &#123; try &#123; //获取引用的Bean名称 String refName = ref.getBeanName(); refName = String.valueOf(evaluate(refName)); //如果引用的对象在父类容器中，则从父类容器中获取指定的引用对象 if (ref.isToParent()) &#123; if (this.beanFactory.getParentBeanFactory() == null) &#123; throw new BeanCreationException( this.beanDefinition.getResourceDescription(), this.beanName, "Can't resolve reference to bean '" + refName + "' in parent factory: no parent factory available"); &#125; return this.beanFactory.getParentBeanFactory().getBean(refName); &#125; //从当前的容器中获取指定的引用Bean对象，如果指定的Bean没有被实例化 //则会递归触发引用Bean的初始化和依赖注入 else &#123; Object bean = this.beanFactory.getBean(refName); //将当前实例化对象的依赖引用对象 this.beanFactory.registerDependentBean(refName, this.beanName); return bean; &#125; &#125; catch (BeansException ex) &#123; throw new BeanCreationException( this.beanDefinition.getResourceDescription(), this.beanName, "Cannot resolve reference to bean '" + ref.getBeanName() + "' while setting " + argName, ex); &#125; &#125; //解析array类型的属性 private Object resolveManagedArray(Object argName, List&lt;?&gt; ml, Class elementType) &#123; //创建一个指定类型的数组，用于存放和返回解析后的数组 Object resolved = Array.newInstance(elementType, ml.size()); for (int i = 0; i &lt; ml.size(); i++) &#123; //递归解析array的每一个元素，并将解析后的值设置到resolved数组中，索引为i Array.set(resolved, i, resolveValueIfNecessary(new KeyedArgName(argName, i), ml.get(i))); &#125; return resolved; &#125; //解析list类型的属性 private List resolveManagedList(Object argName, List&lt;?&gt; ml) &#123; List&lt;Object&gt; resolved = new ArrayList&lt;Object&gt;(ml.size()); for (int i = 0; i &lt; ml.size(); i++) &#123; //递归解析list的每一个元素 resolved.add( resolveValueIfNecessary(new KeyedArgName(argName, i), ml.get(i))); &#125; return resolved; &#125; //解析set类型的属性 private Set resolveManagedSet(Object argName, Set&lt;?&gt; ms) &#123; Set&lt;Object&gt; resolved = new LinkedHashSet&lt;Object&gt;(ms.size()); int i = 0; //递归解析set的每一个元素 for (Object m : ms) &#123; resolved.add(resolveValueIfNecessary(new KeyedArgName(argName, i), m)); i++; &#125; return resolved; &#125; //解析map类型的属性 private Map resolveManagedMap(Object argName, Map&lt;?, ?&gt; mm) &#123; Map&lt;Object, Object&gt; resolved = new LinkedHashMap&lt;Object, Object&gt;(mm.size()); //递归解析map中每一个元素的key和value for (Map.Entry entry : mm.entrySet()) &#123; Object resolvedKey = resolveValueIfNecessary(argName, entry.getKey()); Object resolvedValue = resolveValueIfNecessary( new KeyedArgName(argName, entry.getKey()), entry.getValue()); resolved.put(resolvedKey, resolvedValue); &#125; return resolved; &#125; 通过上面的代码分析，我们明白了Spring是如何将引用类型，内部类以及集合类型等属性进行解析的，属性值解析完成后就可以进行依赖注入了，依赖注入的过程就是Bean对象实例设置到它所依赖的Bean对象属性上去，在第7步中我们已经说过，依赖注入是通过bw.setPropertyValues方法实现的，该方法也使用了委托模式，在BeanWrapper接口中至少定义了方法声明，依赖注入的具体实现交由其实现类BeanWrapperImpl来完成，下面我们就分析依BeanWrapperImpl中赖注入相关的源码。 BeanWrapperImpl对Bean属性的依赖注入BeanWrapperImpl类主要是对容器中完成初始化的Bean实例对象进行属性的依赖注入，即把Bean对象设置到它所依赖的另一个Bean的属性中去，依赖注入的相关源码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259//实现属性依赖注入功能 private void setPropertyValue(PropertyTokenHolder tokens, PropertyValue pv) throws BeansException &#123; //PropertyTokenHolder主要保存属性的名称、路径，以及集合的size等信息 String propertyName = tokens.canonicalName; String actualName = tokens.actualName; //keys是用来保存集合类型属性的size if (tokens.keys != null) &#123; //将属性信息拷贝 PropertyTokenHolder getterTokens = new PropertyTokenHolder(); getterTokens.canonicalName = tokens.canonicalName; getterTokens.actualName = tokens.actualName; getterTokens.keys = new String[tokens.keys.length - 1]; System.arraycopy(tokens.keys, 0, getterTokens.keys, 0, tokens.keys.length - 1); Object propValue; try &#123; //获取属性值，该方法内部使用JDK的内省( Introspector)机制，调用属性//的getter(readerMethod)方法，获取属性的值 propValue = getPropertyValue(getterTokens); &#125; catch (NotReadablePropertyException ex) &#123; throw new NotWritablePropertyException(getRootClass(), this.nestedPath + propertyName, "Cannot access indexed value in property referenced " + "in indexed property path '" + propertyName + "'", ex); &#125; //获取集合类型属性的长度 String key = tokens.keys[tokens.keys.length - 1]; if (propValue == null) &#123; throw new NullValueInNestedPathException(getRootClass(), this.nestedPath + propertyName, "Cannot access indexed value in property referenced " + "in indexed property path '" + propertyName + "': returned null"); &#125; //注入array类型的属性值 else if (propValue.getClass().isArray()) &#123; //获取属性的描述符 PropertyDescriptor pd = getCachedIntrospectionResults().getPropertyDescriptor(actualName); //获取数组的类型 Class requiredType = propValue.getClass().getComponentType(); //获取数组的长度 int arrayIndex = Integer.parseInt(key); Object oldValue = null; try &#123; //获取数组以前初始化的值 if (isExtractOldValueForEditor()) &#123; oldValue = Array.get(propValue, arrayIndex); &#125; //将属性的值赋值给数组中的元素 Object convertedValue = convertIfNecessary(propertyName, oldValue, pv.getValue(), requiredType, new PropertyTypeDescriptor(pd, new MethodParameter(pd.getReadMethod(), -1), requiredType)); Array.set(propValue, arrayIndex, convertedValue); &#125; catch (IndexOutOfBoundsException ex) &#123; throw new InvalidPropertyException(getRootClass(), this.nestedPath + propertyName, "Invalid array index in property path '" + propertyName + "'", ex); &#125; &#125; //注入list类型的属性值 else if (propValue instanceof List) &#123; PropertyDescriptor pd = getCachedIntrospectionResults().getPropertyDescriptor(actualName); //获取list集合的类型 Class requiredType = GenericCollectionTypeResolver.getCollectionReturnType( pd.getReadMethod(), tokens.keys.length); List list = (List) propValue; //获取list集合的size int index = Integer.parseInt(key); Object oldValue = null; if (isExtractOldValueForEditor() &amp;&amp; index &lt; list.size()) &#123; oldValue = list.get(index); &#125; //获取list解析后的属性值 Object convertedValue = convertIfNecessary(propertyName, oldValue, pv.getValue(), requiredType, new PropertyTypeDescriptor(pd, new MethodParameter(pd.getReadMethod(), -1), requiredType)); if (index &lt; list.size()) &#123; //为list属性赋值 list.set(index, convertedValue); &#125; //如果list的长度大于属性值的长度，则多余的元素赋值为null else if (index &gt;= list.size()) &#123; for (int i = list.size(); i &lt; index; i++) &#123; try &#123; list.add(null); &#125; catch (NullPointerException ex) &#123; throw new InvalidPropertyException(getRootClass(), this.nestedPath + propertyName, "Cannot set element with index " + index + " in List of size " + list.size() + ", accessed using property path '" + propertyName + "': List does not support filling up gaps with null elements"); &#125; &#125; list.add(convertedValue); &#125; &#125; //注入map类型的属性值 else if (propValue instanceof Map) &#123; PropertyDescriptor pd = getCachedIntrospectionResults().getPropertyDescriptor(actualName); //获取map集合key的类型 Class mapKeyType = GenericCollectionTypeResolver.getMapKeyReturnType( pd.getReadMethod(), tokens.keys.length); //获取map集合value的类型 Class mapValueType = GenericCollectionTypeResolver.getMapValueReturnType( pd.getReadMethod(), tokens.keys.length); Map map = (Map) propValue; //解析map类型属性key值 Object convertedMapKey = convertIfNecessary(null, null, key, mapKeyType, new PropertyTypeDescriptor(pd, new MethodParameter(pd.getReadMethod(), -1), mapKeyType)); Object oldValue = null; if (isExtractOldValueForEditor()) &#123; oldValue = map.get(convertedMapKey); &#125; //解析map类型属性value值 Object convertedMapValue = convertIfNecessary( propertyName, oldValue, pv.getValue(), mapValueType, new TypeDescriptor(new MethodParameter(pd.getReadMethod(), -1, tokens.keys.length + 1))); //将解析后的key和value值赋值给map集合属性 map.put(convertedMapKey, convertedMapValue); &#125; else &#123; throw new InvalidPropertyException(getRootClass(), this.nestedPath + propertyName, "Property referenced in indexed property path '" + propertyName + "' is neither an array nor a List nor a Map; returned value was [" + pv.getValue() + "]"); &#125; &#125; //对非集合类型的属性注入 else &#123; PropertyDescriptor pd = pv.resolvedDescriptor; if (pd == null || !pd.getWriteMethod().getDeclaringClass().isInstance(this.object)) &#123; pd = getCachedIntrospectionResults().getPropertyDescriptor(actualName); //无法获取到属性名或者属性没有提供setter(写方法)方法 if (pd == null || pd.getWriteMethod() == null) &#123; //如果属性值是可选的，即不是必须的，则忽略该属性值 if (pv.isOptional()) &#123; logger.debug("Ignoring optional value for property '" + actualName + "' - property not found on bean class [" + getRootClass().getName() + "]"); return; &#125; //如果属性值是必须的，则抛出无法给属性赋值，因为每天提供setter方法异常 else &#123; PropertyMatches matches = PropertyMatches.forProperty(propertyName, getRootClass()); throw new NotWritablePropertyException( getRootClass(), this.nestedPath + propertyName, matches.buildErrorMessage(), matches.getPossibleMatches()); &#125; &#125; pv.getOriginalPropertyValue().resolvedDescriptor = pd; &#125; Object oldValue = null; try &#123; Object originalValue = pv.getValue(); Object valueToApply = originalValue; if (!Boolean.FALSE.equals(pv.conversionNecessary)) &#123; if (pv.isConverted()) &#123; valueToApply = pv.getConvertedValue(); &#125; else &#123; if (isExtractOldValueForEditor() &amp;&amp; pd.getReadMethod() != null) &#123; //获取属性的getter方法(读方法)，JDK内省机制 final Method readMethod = pd.getReadMethod(); //如果属性的getter方法不是public访问控制权限的，即访问控制权限比较严格， //则使用JDK的反射机制强行访问非public的方法(暴力读取属性值) if (!Modifier.isPublic(readMethod.getDeclaringClass().getModifiers()) &amp;&amp; !readMethod.isAccessible()) &#123; if (System.getSecurityManager()!= null) &#123; //匿名内部类，根据权限修改属性的读取控制限制 AccessController.doPrivileged(new PrivilegedAction&lt;Object&gt;() &#123; public Object run() &#123; readMethod.setAccessible(true); return null; &#125; &#125;); &#125; else &#123; readMethod.setAccessible(true); &#125; &#125; try &#123; //属性没有提供getter方法时，调用潜在的读取属性值//的方法，获取属性值 if (System.getSecurityManager() != null) &#123; oldValue = AccessController.doPrivileged(new PrivilegedExceptionAction&lt;Object&gt;() &#123; public Object run() throws Exception &#123; return readMethod.invoke(object); &#125; &#125;, acc); &#125; else &#123; oldValue = readMethod.invoke(object); &#125; &#125; catch (Exception ex) &#123; if (ex instanceof PrivilegedActionException) &#123; ex = ((PrivilegedActionException) ex).getException(); &#125; if (logger.isDebugEnabled()) &#123; logger.debug("Could not read previous value of property '" + this.nestedPath + propertyName + "'", ex); &#125; &#125; &#125; //设置属性的注入值 valueToApply = convertForProperty(propertyName, oldValue, originalValue, pd); &#125; pv.getOriginalPropertyValue().conversionNecessary = (valueToApply != originalValue); &#125; //根据JDK的内省机制，获取属性的setter(写方法)方法 final Method writeMethod = (pd instanceof GenericTypeAwarePropertyDescriptor ? ((GenericTypeAwarePropertyDescriptor) pd).getWriteMethodForActualAccess() : pd.getWriteMethod()); //如果属性的setter方法是非public，即访问控制权限比较严格，则使用JDK的反射机制， //强行设置setter方法可访问(暴力为属性赋值) if (!Modifier.isPublic(writeMethod.getDeclaringClass().getModifiers()) &amp;&amp; !writeMethod.isAccessible()) &#123; //如果使用了JDK的安全机制，则需要权限验证 if (System.getSecurityManager()!= null) &#123; AccessController.doPrivileged(new PrivilegedAction&lt;Object&gt;() &#123; public Object run() &#123; writeMethod.setAccessible(true); return null; &#125; &#125;); &#125; else &#123; writeMethod.setAccessible(true); &#125; &#125; final Object value = valueToApply; if (System.getSecurityManager() != null) &#123; try &#123; //将属性值设置到属性上去 AccessController.doPrivileged(new PrivilegedExceptionAction&lt;Object&gt;() &#123; public Object run() throws Exception &#123; writeMethod.invoke(object, value); return null; &#125; &#125;, acc); &#125; catch (PrivilegedActionException ex) &#123; throw ex.getException(); &#125; &#125; else &#123; writeMethod.invoke(this.object, value); &#125; &#125; catch (TypeMismatchException ex) &#123; throw ex; &#125; catch (InvocationTargetException ex) &#123; PropertyChangeEvent propertyChangeEvent = new PropertyChangeEvent(this.rootObject, this.nestedPath + propertyName, oldValue, pv.getValue()); if (ex.getTargetException() instanceof ClassCastException) &#123; throw new TypeMismatchException(propertyChangeEvent, pd.getPropertyType(), ex.getTargetException()); &#125; else &#123; throw new MethodInvocationException(propertyChangeEvent, ex.getTargetException()); &#125; &#125; catch (Exception ex) &#123; PropertyChangeEvent pce = new PropertyChangeEvent(this.rootObject, this.nestedPath + propertyName, oldValue, pv.getValue()); throw new MethodInvocationException(pce, ex); &#125; &#125; &#125; 通过对上面注入依赖代码的分析，我们已经明白了Spring IoC容器是如何将属性的值注入到Bean实例对象中去的： 对于集合类型的属性，将其属性值解析为目标类型的集合后直接赋值给属性。 对于非集合类型的属性，大量使用了JDK的反射和内省机制，通过属性的getter方法(reader method)获取指定属性注入以前的值，同时调用属性的setter方法(writer method)为属性设置注入后的值。看到这里相信很多人都明白了Spring的setter注入原理。 至此Spring IoC容器对Bean定义资源文件的定位，载入、解析和依赖注入已经全部分析完毕，现在Spring IoC容器中管理了一系列靠依赖关系联系起来的Bean，程序不需要应用自己手动创建所需的对象，Spring IoC容器会在我们使用的时候自动为我们创建，并且为我们注入好相关的依赖，这就是Spring核心功能的控制反转和依赖注入的相关功能。]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[源码解读Spring IOC原理（二）]]></title>
    <url>%2Fjava%2Fspring%2Fjava%2Fspring%2F2%2F</url>
    <content type="text"><![CDATA[源码解读Spring IOC原理（二）IOC容器的初始化？IoC容器的初始化包括BeanDefinition的Resource定位、载入和注册这三个基本的过程。我们以ApplicationContext为例讲解，ApplicationContext系列容器也许是我们最熟悉的，因为web项目中使用的XmlWebApplicationContext就属于这个继承体系，还有ClasspathXmlApplicationContext等，其继承体系如下图所示ApplicationContext允许上下文嵌套，通过保持父上下文可以维持一个上下文体系。对于bean的查找可以在这个上下文体系中发生，首先检查当前上下文，其次是父上下文，逐级向上，这样为不同的Spring应用提供了一个共享的bean定义环境。 下面我们分别简单地演示一下两种ioc容器的创建过程 XmlBeanFactory(屌丝IOC)的整个流程通过XmlBeanFactory的源码，我们可以发现:1234567891011121314 public class XmlBeanFactory extends DefaultListableBeanFactory&#123; private final XmlBeanDefinitionReader reader; public XmlBeanFactory(Resource resource)throws BeansException&#123; this(resource, null); &#125; public XmlBeanFactory(Resource resource, BeanFactory parentBeanFactory) throws BeansException&#123; super(parentBeanFactory); this.reader = new XmlBeanDefinitionReader(this); this.reader.loadBeanDefinitions(resource); &#125;&#125; 1234567891011//根据Xml配置文件创建Resource资源对象，该对象中包含了BeanDefinition的信息ClassPathResource resource =new ClassPathResource("application-context.xml");//创建DefaultListableBeanFactoryDefaultListableBeanFactory factory =new DefaultListableBeanFactory();//创建XmlBeanDefinitionReader读取器，用于载入BeanDefinition。之所以需要BeanFactory作为参数，是因为会将读取的信息回调配置给factoryXmlBeanDefinitionReader reader =new XmlBeanDefinitionReader(factory);//XmlBeanDefinitionReader执行载入BeanDefinition的方法，最后会完成Bean的载入和注册。完成后Bean就成功的放置到IOC容器当中，以后我们就可以从中取得Bean来使用reader.loadBeanDefinitions(resource); 通过前面的源码，this.reader = new XmlBeanDefinitionReader(this); 中其中this 传的是factory对象 FileSystemXmlApplicationContext 的IOC容器流程解剖ApplicationContext = new FileSystemXmlApplicationContext(xmlPath);，调用构造函数：123public FileSystemXmlApplicationContext(String... configLocations) throws BeansException &#123; this(configLocations, true, null);&#125; 实际调用 1234567public FileSystemXmlApplicationContext(String[] configLocations, boolean refresh, ApplicationContext parent) throws BeansException &#123; super(parent); setConfigLocations(configLocations); if (refresh) &#123; refresh(); &#125; &#125; 设置资源加载器和资源定位通过分析FileSystemXmlApplicationContext的源代码可以知道，在创建FileSystemXmlApplicationContext容器时，构造方法做以下两项重要工作： 调用父类容器的构造方法(super(parent)方法)为容器设置好Bean资源加载器。 再调用父类AbstractRefreshableConfigApplicationContext的setConfigLocations(configLocations)方法设置Bean定义资源文件的定位路径。 通过追踪FileSystemXmlApplicationContext的继承体系，发现其父类的父类AbstractApplicationContext中初始化IoC容器所做的主要源码如下：123456789101112131415161718192021 public abstract class AbstractApplicationContext extends DefaultResourceLoader implements ConfigurableApplicationContext, DisposableBean &#123; //静态初始化块，在整个容器创建过程中只执行一次 static &#123; //为了避免应用程序在Weblogic8.1关闭时出现类加载异常加载问题，加载IoC容 //器关闭事件(ContextClosedEvent)类 ContextClosedEvent.class.getName(); &#125; //FileSystemXmlApplicationContext调用父类构造方法调用的就是该方法 public AbstractApplicationContext(ApplicationContext parent) &#123; this.parent = parent; this.resourcePatternResolver = getResourcePatternResolver(); &#125; //获取一个Spring Source的加载器用于读入Spring Bean定义资源文件 protected ResourcePatternResolver getResourcePatternResolver() &#123; // AbstractApplicationContext继承DefaultResourceLoader，也是一个S //Spring资源加载器，其getResource(String location)方法用于载入资源 return new PathMatchingResourcePatternResolver(this); &#125; …… &#125; AbstractApplicationContext构造方法中调用PathMatchingResourcePatternResolver的构造方法创建Spring资源加载器： 12345public PathMatchingResourcePatternResolver(ResourceLoader resourceLoader) &#123; Assert.notNull(resourceLoader, "ResourceLoader must not be null"); //设置Spring的资源加载器 this.resourceLoader = resourceLoader; &#125; 在设置容器的资源加载器之后，接下来FileSystemXmlApplicationContet执行setConfigLocations方法通过调用其父类AbstractRefreshableConfigApplicationContext的方法进行对Bean定义资源文件的定位，该方法的源码如下：123456789101112131415161718192021//处理单个资源文件路径为一个字符串的情况 public void setConfigLocation(String location) &#123; //String CONFIG_LOCATION_DELIMITERS = ",; /t/n"; //即多个资源文件路径之间用” ,; /t/n”分隔，解析成数组形式 setConfigLocations(StringUtils.tokenizeToStringArray(location, CONFIG_LOCATION_DELIMITERS)); &#125; //解析Bean定义资源文件的路径，处理多个资源文件字符串数组 public void setConfigLocations(String[] locations) &#123; if (locations != null) &#123; Assert.noNullElements(locations, "Config locations must not be null"); this.configLocations = new String[locations.length]; for (int i = 0; i &lt; locations.length; i++) &#123; // resolvePath为同一个类中将字符串解析为路径的方法 this.configLocations[i] = resolvePath(locations[i]).trim(); &#125; &#125; else &#123; this.configLocations = null; &#125; &#125; 通过这两个方法的源码我们可以看出，我们既可以使用一个字符串来配置多个Spring Bean定义资源文件，也可以使用字符串数组，即下面两种方式都是可以的： ClasspathResource res = new ClasspathResource(“a.xml,b.xml,……”);多个资源文件路径之间可以是用” ,; /t/n”等分隔。 ClasspathResource res = new ClasspathResource(newString[]{“a.xml”,”b.xml”,……});至此，Spring IoC容器在初始化时将配置的Bean定义资源文件定位为Spring封装的Resource。 AbstractApplicationContext的refresh函数载入Bean定义过程Spring IoC容器对Bean定义资源的载入是从refresh()函数开始的，refresh()是一个模板方法，refresh()方法的作用是：在创建IoC容器前，如果已经有容器存在，则需要把已有的容器销毁和关闭，以保证在refresh之后使用的是新建立起来的IoC容器。refresh的作用类似于对IoC容器的重启，在新建立好的容器中对容器进行初始化，对Bean定义资源进行载入 FileSystemXmlApplicationContext通过调用其父类AbstractApplicationContext的refresh()函数启动整个IoC容器对Bean定义的载入过程：123456789101112131415161718192021222324252627282930313233343536373839public void refresh() throws BeansException, IllegalStateException &#123; synchronized (this.startupShutdownMonitor) &#123; //调用容器准备刷新的方法，获取容器的当时时间，同时给容器设置同步标识 prepareRefresh(); //告诉子类启动refreshBeanFactory()方法，Bean定义资源文件的载入从 //子类的refreshBeanFactory()方法启动 ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); //为BeanFactory配置容器特性，例如类加载器、事件处理器等 prepareBeanFactory(beanFactory); try &#123; //为容器的某些子类指定特殊的BeanPost事件处理器 postProcessBeanFactory(beanFactory); //调用所有注册的BeanFactoryPostProcessor的Bean invokeBeanFactoryPostProcessors(beanFactory); //为BeanFactory注册BeanPost事件处理器. //BeanPostProcessor是Bean后置处理器，用于监听容器触发的事件 registerBeanPostProcessors(beanFactory); //初始化信息源，和国际化相关. initMessageSource(); //初始化容器事件传播器. initApplicationEventMulticaster(); //调用子类的某些特殊Bean初始化方法 onRefresh(); //为事件传播器注册事件监听器. registerListeners(); //初始化所有剩余的单态Bean. finishBeanFactoryInitialization(beanFactory); //初始化容器的生命周期事件处理器，并发布容器的生命周期事件 finishRefresh(); &#125; catch (BeansException ex) &#123; //销毁以创建的单态Bean destroyBeans(); //取消refresh操作，重置容器的同步标识. cancelRefresh(ex); throw ex; &#125; &#125; &#125; refresh()方法主要为IoC容器Bean的生命周期管理提供条件，Spring IoC容器载入Bean定义资源文件从其子类容器的refreshBeanFactory()方法启动，所以整个refresh()中“ConfigurableListableBeanFactory beanFactory =obtainFreshBeanFactory();”这句以后代码的都是注册容器的信息源和生命周期事件，载入过程就是从这句代码启动。 refresh()方法的作用是：在创建IoC容器前，如果已经有容器存在，则需要把已有的容器销毁和关闭，以保证在refresh之后使用的是新建立起来的IoC容器。refresh的作用类似于对IoC容器的重启，在新建立好的容器中对容器进行初始化，对Bean定义资源进行载入 AbstractApplicationContext的obtainFreshBeanFactory()方法调用子类容器的refreshBeanFactory()方法，启动容器载入Bean定义资源文件的过程，代码如下： 123456789protected ConfigurableListableBeanFactory obtainFreshBeanFactory() &#123; //这里使用了委派设计模式，父类定义了抽象的refreshBeanFactory()方法，具体实现调用子类容器的refreshBeanFactory()方法 refreshBeanFactory(); ConfigurableListableBeanFactory beanFactory = getBeanFactory(); if (logger.isDebugEnabled()) &#123; logger.debug("Bean factory for " + getDisplayName() + ": " + beanFactory); &#125; return beanFactory; &#125; AbstractApplicationContext子类的refreshBeanFactory()方法：AbstractApplicationContext类中只抽象定义了refreshBeanFactory()方法，容器真正调用的是其子类AbstractRefreshableApplicationContext实现的 refreshBeanFactory()方法，方法的源码如下：123456789101112131415161718192021protected final void refreshBeanFactory() throws BeansException &#123; if (hasBeanFactory()) &#123;//如果已经有容器，销毁容器中的bean，关闭容器 destroyBeans(); closeBeanFactory(); &#125; try &#123; //创建IoC容器 DefaultListableBeanFactory beanFactory = createBeanFactory(); beanFactory.setSerializationId(getId()); //对IoC容器进行定制化，如设置启动参数，开启注解的自动装配等 customizeBeanFactory(beanFactory); //调用载入Bean定义的方法，主要这里又使用了一个委派模式，在当前类中只定义了抽象的loadBeanDefinitions方法，具体的实现调用子类容器 loadBeanDefinitions(beanFactory); synchronized (this.beanFactoryMonitor) &#123; this.beanFactory = beanFactory; &#125; &#125; catch (IOException ex) &#123; throw new ApplicationContextException("I/O error parsing bean definition source for " + getDisplayName(), ex); &#125; &#125; 在这个方法中，先判断BeanFactory是否存在，如果存在则先销毁beans并关闭beanFactory，接着创建DefaultListableBeanFactory，并调用loadBeanDefinitions(beanFactory)装载bean定义 AbstractRefreshableApplicationContext子类的loadBeanDefinitions方法AbstractRefreshableApplicationContext中只定义了抽象的loadBeanDefinitions方法，容器真正调用的是其子类AbstractXmlApplicationContext对该方法的实现，AbstractXmlApplicationContext的主要源码如下：loadBeanDefinitions方法同样是抽象方法，是由其子类实现的，也即在AbstractXmlApplicationContext中。 1234567891011121314151617181920212223242526272829303132333435363738394041public abstract class AbstractXmlApplicationContext extends AbstractRefreshableConfigApplicationContext &#123; …… //实现父类抽象的载入Bean定义方法 @Override protected void loadBeanDefinitions(DefaultListableBeanFactory beanFactory) throws BeansException, IOException &#123; //创建XmlBeanDefinitionReader，即创建Bean读取器，并通过回调设置到容器中去，容 器使用该读取器读取Bean定义资源 XmlBeanDefinitionReader beanDefinitionReader = new XmlBeanDefinitionReader(beanFactory); //为Bean读取器设置Spring资源加载器，AbstractXmlApplicationContext的 //祖先父类AbstractApplicationContext继承DefaultResourceLoader，因此，容器本身也是一个资源加载器 beanDefinitionReader.setResourceLoader(this); //为Bean读取器设置SAX xml解析器 beanDefinitionReader.setEntityResolver(new ResourceEntityResolver(this)); //当Bean读取器读取Bean定义的Xml资源文件时，启用Xml的校验机制 initBeanDefinitionReader(beanDefinitionReader); //Bean读取器真正实现加载的方法 loadBeanDefinitions(beanDefinitionReader); &#125; //Xml Bean读取器加载Bean定义资源 protected void loadBeanDefinitions(XmlBeanDefinitionReader reader) throws BeansException, IOException &#123; //获取Bean定义资源的定位 Resource[] configResources = getConfigResources(); if (configResources != null) &#123; //Xml Bean读取器调用其父类AbstractBeanDefinitionReader读取定位 //的Bean定义资源 reader.loadBeanDefinitions(configResources); &#125; //如果子类中获取的Bean定义资源定位为空，则获取FileSystemXmlApplicationContext构造方法中setConfigLocations方法设置的资源 String[] configLocations = getConfigLocations(); if (configLocations != null) &#123; //Xml Bean读取器调用其父类AbstractBeanDefinitionReader读取定位 //的Bean定义资源 reader.loadBeanDefinitions(configLocations); &#125; &#125; //这里又使用了一个委托模式，调用子类的获取Bean定义资源定位的方法 //该方法在ClassPathXmlApplicationContext中进行实现，对于我们 //举例分析源码的FileSystemXmlApplicationContext没有使用该方法 protected Resource[] getConfigResources() &#123; return null; &#125; …… &#125; Xml Bean读取器(XmlBeanDefinitionReader)调用其父类AbstractBeanDefinitionReader的 reader.loadBeanDefinitions方法读取Bean定义资源。 由于我们使用FileSystemXmlApplicationContext作为例子分析，因此getConfigResources的返回值为null，因此程序执行reader.loadBeanDefinitions(configLocations)分支。 AbstractBeanDefinitionReader读取Bean定义资源AbstractBeanDefinitionReader的loadBeanDefinitions方法源码如下：可以到org.springframework.beans.factory.support看一下BeanDefinitionReader的结构在其抽象父类AbstractBeanDefinitionReader中定义了载入过程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657//重载方法，调用下面的loadBeanDefinitions(String, Set&lt;Resource&gt;);方法 public int loadBeanDefinitions(String location) throws BeanDefinitionStoreException &#123; return loadBeanDefinitions(location, null); &#125; public int loadBeanDefinitions(String location, Set&lt;Resource&gt; actualResources) throws BeanDefinitionStoreException &#123; //获取在IoC容器初始化过程中设置的资源加载器 ResourceLoader resourceLoader = getResourceLoader(); if (resourceLoader == null) &#123; throw new BeanDefinitionStoreException( "Cannot import bean definitions from location [" + location + "]: no ResourceLoader available"); &#125; if (resourceLoader instanceof ResourcePatternResolver) &#123; try &#123; //将指定位置的Bean定义资源文件解析为Spring IoC容器封装的资源 //加载多个指定位置的Bean定义资源文件 Resource[] resources = ((ResourcePatternResolver) resourceLoader).getResources(location); //委派调用其子类XmlBeanDefinitionReader的方法，实现加载功能 int loadCount = loadBeanDefinitions(resources); if (actualResources != null) &#123; for (Resource resource : resources) &#123; actualResources.add(resource); &#125; &#125; if (logger.isDebugEnabled()) &#123; logger.debug("Loaded " + loadCount + " bean definitions from location pattern [" + location + "]"); &#125; return loadCount; &#125; catch (IOException ex) &#123; throw new BeanDefinitionStoreException( "Could not resolve bean definition resource pattern [" + location + "]", ex); &#125; &#125; else &#123; //将指定位置的Bean定义资源文件解析为Spring IoC容器封装的资源 //加载单个指定位置的Bean定义资源文件 Resource resource = resourceLoader.getResource(location); //委派调用其子类XmlBeanDefinitionReader的方法，实现加载功能 int loadCount = loadBeanDefinitions(resource); if (actualResources != null) &#123; actualResources.add(resource); &#125; if (logger.isDebugEnabled()) &#123; logger.debug("Loaded " + loadCount + " bean definitions from location [" + location + "]"); &#125; return loadCount; &#125; &#125; //重载方法，调用loadBeanDefinitions(String); public int loadBeanDefinitions(String... locations) throws BeanDefinitionStoreException &#123; Assert.notNull(locations, "Location array must not be null"); int counter = 0; for (String location : locations) &#123; counter += loadBeanDefinitions(location); &#125; return counter; &#125; loadBeanDefinitions(Resource…resources)方法和上面分析的3个方法类似，同样也是调用XmlBeanDefinitionReader的loadBeanDefinitions方法。 从对AbstractBeanDefinitionReader的loadBeanDefinitions方法源码分析可以看出该方法做了以下两件事： 调用资源加载器的获取资源方法resourceLoader.getResource(location)，获取到要加载的资源。 真正执行加载功能是其子类XmlBeanDefinitionReader的loadBeanDefinitions方法。 资源加载器获取要读入的资源XmlBeanDefinitionReader通过调用其父类DefaultResourceLoader的getResource方法获取要加载的资源，其源码如下12345678910111213141516171819//获取Resource的具体实现方法 public Resource getResource(String location) &#123; Assert.notNull(location, "Location must not be null"); //如果是类路径的方式，那需要使用ClassPathResource 来得到bean 文件的资源对象 if (location.startsWith(CLASSPATH_URL_PREFIX)) &#123; return new ClassPathResource(location.substring(CLASSPATH_URL_PREFIX.length()), getClassLoader()); &#125; try &#123; // 如果是URL 方式，使用UrlResource 作为bean 文件的资源对象 URL url = new URL(location); return new UrlResource(url); &#125; catch (MalformedURLException ex) &#123; &#125; //如果既不是classpath标识，又不是URL标识的Resource定位，则调用 //容器本身的getResourceByPath方法获取Resource return getResourceByPath(location); &#125; FileSystemXmlApplicationContext容器提供了getResourceByPath方法的实现，就是为了处理既不是classpath标识，又不是URL标识的Resource定位这种情况。1234567protected Resource getResourceByPath(String path) &#123; if (path != null &amp;&amp; path.startsWith("/")) &#123; path = path.substring(1); &#125; //这里使用文件系统资源对象来定义bean 文件 return new FileSystemResource(path); &#125; 这样代码就回到了 FileSystemXmlApplicationContext 中来，他提供了FileSystemResource 来完成从文件系统得到配置文件的资源定义。 这样，就可以从文件系统路径上对IOC 配置文件进行加载 - 当然我们可以按照这个逻辑从任何地方加载，在Spring 中我们看到它提供 的各种资源抽象，比如ClassPathResource, URLResource,FileSystemResource 等来供我们使用。上面我们看到的是定位Resource 的一个过程，而这只是加载过程的一部分. XmlBeanDefinitionReader加载Bean定义资源Bean定义的Resource得到了继续回到XmlBeanDefinitionReader的loadBeanDefinitions(Resource …)方法看到代表bean文件的资源定义以后的载入过程。12345678910111213141516171819202122232425262728293031323334353637383940//XmlBeanDefinitionReader加载资源的入口方法 public int loadBeanDefinitions(Resource resource) throws BeanDefinitionStoreException &#123; //将读入的XML资源进行特殊编码处理 return loadBeanDefinitions(new EncodedResource(resource)); &#125; //这里是载入XML形式Bean定义资源文件方法 public int loadBeanDefinitions(EncodedResource encodedResource) throws BeanDefinitionStoreException &#123; ....... try &#123; //将资源文件转为InputStream的IO流 InputStream inputStream = encodedResource.getResource().getInputStream(); try &#123; //从InputStream中得到XML的解析源 InputSource inputSource = new InputSource(inputStream); if (encodedResource.getEncoding() != null) &#123; inputSource.setEncoding(encodedResource.getEncoding()); &#125; //这里是具体的读取过程 return doLoadBeanDefinitions(inputSource, encodedResource.getResource()); &#125; finally &#123; //关闭从Resource中得到的IO流 inputStream.close(); &#125; &#125; ......... 26&#125; //从特定XML文件中实际载入Bean定义资源的方法 protected int doLoadBeanDefinitions(InputSource inputSource, Resource resource) throws BeanDefinitionStoreException &#123; try &#123; int validationMode = getValidationModeForResource(resource); //将XML文件转换为DOM对象，解析过程由documentLoader实现 Document doc = this.documentLoader.loadDocument( inputSource, this.entityResolver, this.errorHandler, validationMode, this.namespaceAware); //这里是启动对Bean定义解析的详细过程，该解析过程会用到Spring的Bean配置规则 return registerBeanDefinitions(doc, resource); &#125; ....... &#125; 通过源码分析，载入Bean定义资源文件的最后一步是将Bean定义资源转换为Document对象，该过程由documentLoader实现 DocumentLoader将Bean定义资源转换为Document对象DocumentLoader将Bean定义资源转换为Document对象，ocumentLoader将Bean定义资源转换成Document对象的源码如下：1234567891011121314151617181920212223242526272829303132333435363738//使用标准的JAXP将载入的Bean定义资源转换成document对象 public Document loadDocument(InputSource inputSource, EntityResolver entityResolver, ErrorHandler errorHandler, int validationMode, boolean namespaceAware) throws Exception &#123; //创建文件解析器工厂 DocumentBuilderFactory factory = createDocumentBuilderFactory(validationMode, namespaceAware); if (logger.isDebugEnabled()) &#123; logger.debug("Using JAXP provider [" + factory.getClass().getName() + "]"); &#125; //创建文档解析器 DocumentBuilder builder = createDocumentBuilder(factory, entityResolver, errorHandler); //解析Spring的Bean定义资源 return builder.parse(inputSource); &#125; protected DocumentBuilderFactory createDocumentBuilderFactory(int validationMode, boolean namespaceAware) throws ParserConfigurationException &#123; //创建文档解析工厂 DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance(); factory.setNamespaceAware(namespaceAware); //设置解析XML的校验 if (validationMode != XmlValidationModeDetector.VALIDATION_NONE) &#123; factory.setValidating(true); if (validationMode == XmlValidationModeDetector.VALIDATION_XSD) &#123; factory.setNamespaceAware(true); try &#123; factory.setAttribute(SCHEMA_LANGUAGE_ATTRIBUTE, XSD_SCHEMA_LANGUAGE); &#125; catch (IllegalArgumentException ex) &#123; ParserConfigurationException pcex = new ParserConfigurationException( "Unable to validate using XSD: Your JAXP provider [" + factory + "] does not support XML Schema. Are you running on Java 1.4 with Apache Crimson? " + "Upgrade to Apache Xerces (or Java 1.5) for full XSD support."); pcex.initCause(ex); throw pcex; &#125; &#125; &#125; return factory; &#125; 该解析过程调用JavaEE标准的JAXP标准进行处理。 至此Spring IoC容器根据定位的Bean定义资源文件，将其加载读入并转换成为Document对象过程完成。 接下来我们要继续分析Spring IoC容器将载入的Bean定义资源文件转换为Document对象之后，是如何将其解析为Spring IoC管理的Bean对象并将其注册到容器中的。 XmlBeanDefinitionReader解析载入的Bean定义资源文件XmlBeanDefinitionReader类中的doLoadBeanDefinitions方法是从特定XML文件中实际载入Bean定义资源的方法，该方法在载入Bean定义资源之后将其转换为Document对象，接下来调用registerBeanDefinitions启动Spring IoC容器对Bean定义的解析过程，registerBeanDefinitions方法源码如下： 123456789101112131415//按照Spring的Bean语义要求将Bean定义资源解析并转换为容器内部数据结构 public int registerBeanDefinitions(Document doc, Resource resource) throws BeanDefinitionStoreException &#123; //得到BeanDefinitionDocumentReader来对xml格式的BeanDefinition解析 BeanDefinitionDocumentReader documentReader = createBeanDefinitionDocumentReader(); //获得容器中注册的Bean数量 int countBefore = getRegistry().getBeanDefinitionCount(); //解析过程入口，这里使用了委派模式，BeanDefinitionDocumentReader只是个接口，//具体的解析实现过程有实现类DefaultBeanDefinitionDocumentReader完成 documentReader.registerBeanDefinitions(doc, createReaderContext(resource)); //统计解析的Bean数量 return getRegistry().getBeanDefinitionCount() - countBefore; &#125; //创建BeanDefinitionDocumentReader对象，解析Document对象 protected BeanDefinitionDocumentReader createBeanDefinitionDocumentReader() &#123; return BeanDefinitionDocumentReader.class.cast(BeanUtils.instantiateClass(this.documentReaderClass)); &#125; Bean定义资源的载入解析分为以下两个过程： 通过调用XML解析器将Bean定义资源文件转换得到Document对象，但是这些Document对象并没有按照Spring的Bean规则进行解析。这一步是载入的过程 在完成通用的XML解析之后，按照Spring的Bean规则对Document对象进行解析。 按照Spring的Bean规则对Document对象解析的过程是在接口BeanDefinitionDocumentReader的实现类DefaultBeanDefinitionDocumentReader中实现的。 DefaultBeanDefinitionDocumentReader对Bean定义的Document对象解析BeanDefinitionDocumentReader接口通过registerBeanDefinitions方法调用其实现类DefaultBeanDefinitionDocumentReader对Document对象进行解析，解析的代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188//根据Spring DTD对Bean的定义规则解析Bean定义Document对象 public void registerBeanDefinitions(Document doc, XmlReaderContext readerContext) &#123; //获得XML描述符 this.readerContext = readerContext; logger.debug("Loading bean definitions"); //获得Document的根元素 Element root = doc.getDocumentElement(); //具体的解析过程由BeanDefinitionParserDelegate实现， //BeanDefinitionParserDelegate中定义了Spring Bean定义XML文件的各种元素 BeanDefinitionParserDelegate delegate = createHelper(readerContext, root); //在解析Bean定义之前，进行自定义的解析，增强解析过程的可扩展性 preProcessXml(root); //从Document的根元素开始进行Bean定义的Document对象 parseBeanDefinitions(root, delegate); //在解析Bean定义之后，进行自定义的解析，增加解析过程的可扩展性 postProcessXml(root); &#125; //创建BeanDefinitionParserDelegate，用于完成真正的解析过程 protected BeanDefinitionParserDelegate createHelper(XmlReaderContext readerContext, Element root) &#123; BeanDefinitionParserDelegate delegate = new BeanDefinitionParserDelegate(readerContext); //BeanDefinitionParserDelegate初始化Document根元素 delegate.initDefaults(root); return delegate; &#125; //使用Spring的Bean规则从Document的根元素开始进行Bean定义的Document对象 protected void parseBeanDefinitions(Element root, BeanDefinitionParserDelegate delegate) &#123; //Bean定义的Document对象使用了Spring默认的XML命名空间 if (delegate.isDefaultNamespace(root)) &#123; //获取Bean定义的Document对象根元素的所有子节点 NodeList nl = root.getChildNodes(); for (int i = 0; i &lt; nl.getLength(); i++) &#123; Node node = nl.item(i); //获得Document节点是XML元素节点 if (node instanceof Element) &#123; Element ele = (Element) node; //Bean定义的Document的元素节点使用的是Spring默认的XML命名空间 if (delegate.isDefaultNamespace(ele)) &#123; //使用Spring的Bean规则解析元素节点 parseDefaultElement(ele, delegate); &#125; else &#123; //没有使用Spring默认的XML命名空间，则使用用户自定义的解//析规则解析元素节点 delegate.parseCustomElement(ele); &#125; &#125; &#125; &#125; else &#123; //Document的根节点没有使用Spring默认的命名空间，则使用用户自定义的 //解析规则解析Document根节点 delegate.parseCustomElement(root); &#125; &#125; //使用Spring的Bean规则解析Document元素节点 private void parseDefaultElement(Element ele, BeanDefinitionParserDelegate delegate) &#123; //如果元素节点是&lt;Import&gt;导入元素，进行导入解析 if (delegate.nodeNameEquals(ele, IMPORT_ELEMENT)) &#123; importBeanDefinitionResource(ele); &#125; //如果元素节点是&lt;Alias&gt;别名元素，进行别名解析 else if (delegate.nodeNameEquals(ele, ALIAS_ELEMENT)) &#123; processAliasRegistration(ele); &#125; //元素节点既不是导入元素，也不是别名元素，即普通的&lt;Bean&gt;元素， //按照Spring的Bean规则解析元素 else if (delegate.nodeNameEquals(ele, BEAN_ELEMENT)) &#123; processBeanDefinition(ele, delegate); &#125; &#125; //解析&lt;Import&gt;导入元素，从给定的导入路径加载Bean定义资源到Spring IoC容器中 protected void importBeanDefinitionResource(Element ele) &#123; //获取给定的导入元素的location属性 String location = ele.getAttribute(RESOURCE_ATTRIBUTE); //如果导入元素的location属性值为空，则没有导入任何资源，直接返回 if (!StringUtils.hasText(location)) &#123; getReaderContext().error("Resource location must not be empty", ele); return; &#125; //使用系统变量值解析location属性值 location = SystemPropertyUtils.resolvePlaceholders(location); Set&lt;Resource&gt; actualResources = new LinkedHashSet&lt;Resource&gt;(4); //标识给定的导入元素的location是否是绝对路径 boolean absoluteLocation = false; try &#123; absoluteLocation = ResourcePatternUtils.isUrl(location) || ResourceUtils.toURI(location).isAbsolute(); &#125; catch (URISyntaxException ex) &#123; //给定的导入元素的location不是绝对路径 &#125; //给定的导入元素的location是绝对路径 if (absoluteLocation) &#123; try &#123; //使用资源读入器加载给定路径的Bean定义资源 int importCount = getReaderContext().getReader().loadBeanDefinitions(location, actualResources); if (logger.isDebugEnabled()) &#123; logger.debug("Imported " + importCount + " bean definitions from URL location [" + location + "]"); &#125; &#125; catch (BeanDefinitionStoreException ex) &#123; getReaderContext().error( "Failed to import bean definitions from URL location [" + location + "]", ele, ex); &#125; &#125; else &#123; //给定的导入元素的location是相对路径 try &#123; int importCount; //将给定导入元素的location封装为相对路径资源 Resource relativeResource = getReaderContext().getResource().createRelative(location); //封装的相对路径资源存在 if (relativeResource.exists()) &#123; //使用资源读入器加载Bean定义资源 importCount = getReaderContext().getReader().loadBeanDefinitions(relativeResource); actualResources.add(relativeResource); &#125; //封装的相对路径资源不存在 else &#123; //获取Spring IoC容器资源读入器的基本路径 String baseLocation = getReaderContext().getResource().getURL().toString(); //根据Spring IoC容器资源读入器的基本路径加载给定导入 //路径的资源 importCount = getReaderContext().getReader().loadBeanDefinitions( StringUtils.applyRelativePath(baseLocation, location), actualResources); &#125; if (logger.isDebugEnabled()) &#123; logger.debug("Imported " + importCount + " bean definitions from relative location [" + location + "]"); &#125; &#125; catch (IOException ex) &#123; getReaderContext().error("Failed to resolve current resource location", ele, ex); &#125; catch (BeanDefinitionStoreException ex) &#123; getReaderContext().error("Failed to import bean definitions from relative location [" + location + "]", ele, ex); &#125; &#125; Resource[] actResArray = actualResources.toArray(new Resource[actualResources.size()]); //在解析完&lt;Import&gt;元素之后，发送容器导入其他资源处理完成事件 getReaderContext().fireImportProcessed(location, actResArray, extractSource(ele)); &#125; //解析&lt;Alias&gt;别名元素，为Bean向Spring IoC容器注册别名 protected void processAliasRegistration(Element ele) &#123; //获取&lt;Alias&gt;别名元素中name的属性值 String name = ele.getAttribute(NAME_ATTRIBUTE); //获取&lt;Alias&gt;别名元素中alias的属性值 String alias = ele.getAttribute(ALIAS_ATTRIBUTE); boolean valid = true; //&lt;alias&gt;别名元素的name属性值为空 if (!StringUtils.hasText(name)) &#123; getReaderContext().error("Name must not be empty", ele); valid = false; &#125; //&lt;alias&gt;别名元素的alias属性值为空 if (!StringUtils.hasText(alias)) &#123; getReaderContext().error("Alias must not be empty", ele); valid = false; &#125; if (valid) &#123; try &#123; //向容器的资源读入器注册别名 getReaderContext().getRegistry().registerAlias(name, alias); &#125; catch (Exception ex) &#123; getReaderContext().error("Failed to register alias '" + alias + "' for bean with name '" + name + "'", ele, ex); &#125; //在解析完&lt;Alias&gt;元素之后，发送容器别名处理完成事件 getReaderContext().fireAliasRegistered(name, alias, extractSource(ele)); &#125; &#125; //解析Bean定义资源Document对象的普通元素 protected void processBeanDefinition(Element ele, BeanDefinitionParserDelegate delegate) &#123; // BeanDefinitionHolder是对BeanDefinition的封装，即Bean定义的封装类 //对Document对象中&lt;Bean&gt;元素的解析由BeanDefinitionParserDelegate实现 BeanDefinitionHolder bdHolder = delegate.parseBeanDefinitionElement(ele); if (bdHolder != null) &#123; bdHolder = delegate.decorateBeanDefinitionIfRequired(ele, bdHolder); try &#123; //向Spring IoC容器注册解析得到的Bean定义，这是Bean定义向IoC容器注册的入口 BeanDefinitionReaderUtils.registerBeanDefinition(bdHolder, getReaderContext().getRegistry()); &#125; catch (BeanDefinitionStoreException ex) &#123; getReaderContext().error("Failed to register bean definition with name '" + bdHolder.getBeanName() + "'", ele, ex); &#125; //在完成向Spring IoC容器注册解析得到的Bean定义之后，发送注册事件 getReaderContext().fireComponentRegistered(new BeanComponentDefinition(bdHolder)); &#125; &#125; 通过上述Spring IoC容器对载入的Bean定义Document解析可以看出，我们使用Spring时，在Spring配置文件中可以使用元素来导入IoC容器所需要的其他资源，Spring IoC容器在解析时会首先将指定导入的资源加载进容器中。使用别名时，Spring IoC容器首先将别名元素所定义的别名注册到容器中。 对于既不是元素，又不是元素的元素，即Spring配置文件中普通的元素的解析由BeanDefinitionParserDelegate类的parseBeanDefinitionElement方法来实现。 BeanDefinitionParserDelegate解析Bean定义资源文件中的元素Bean定义资源文件中的和元素解析在DefaultBeanDefinitionDocumentReader中已经完成，对Bean定义资源文件中使用最多的元素交由BeanDefinitionParserDelegate来解析，其解析实现的源码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125//解析&lt;Bean&gt;元素的入口 public BeanDefinitionHolder parseBeanDefinitionElement(Element ele) &#123; return parseBeanDefinitionElement(ele, null); &#125; //解析Bean定义资源文件中的&lt;Bean&gt;元素，这个方法中主要处理&lt;Bean&gt;元素的id，name //和别名属性 public BeanDefinitionHolder parseBeanDefinitionElement(Element ele, BeanDefinition containingBean) &#123; //获取&lt;Bean&gt;元素中的id属性值 String id = ele.getAttribute(ID_ATTRIBUTE); //获取&lt;Bean&gt;元素中的name属性值 String nameAttr = ele.getAttribute(NAME_ATTRIBUTE); ////获取&lt;Bean&gt;元素中的alias属性值 List&lt;String&gt; aliases = new ArrayList&lt;String&gt;(); //将&lt;Bean&gt;元素中的所有name属性值存放到别名中 if (StringUtils.hasLength(nameAttr)) &#123; String[] nameArr = StringUtils.tokenizeToStringArray(nameAttr, BEAN_NAME_DELIMITERS); aliases.addAll(Arrays.asList(nameArr)); &#125; String beanName = id; //如果&lt;Bean&gt;元素中没有配置id属性时，将别名中的第一个值赋值给beanName if (!StringUtils.hasText(beanName) &amp;&amp; !aliases.isEmpty()) &#123; beanName = aliases.remove(0); if (logger.isDebugEnabled()) &#123; logger.debug("No XML 'id' specified - using '" + beanName + "' as bean name and " + aliases + " as aliases"); &#125; &#125; //检查&lt;Bean&gt;元素所配置的id或者name的唯一性，containingBean标识&lt;Bean&gt; //元素中是否包含子&lt;Bean&gt;元素 if (containingBean == null) &#123; //检查&lt;Bean&gt;元素所配置的id、name或者别名是否重复 checkNameUniqueness(beanName, aliases, ele); &#125; //详细对&lt;Bean&gt;元素中配置的Bean定义进行解析的地方 AbstractBeanDefinition beanDefinition = parseBeanDefinitionElement(ele, beanName, containingBean); if (beanDefinition != null) &#123; if (!StringUtils.hasText(beanName)) &#123; try &#123; if (containingBean != null) &#123; //如果&lt;Bean&gt;元素中没有配置id、别名或者name，且没有包含子//&lt;Bean&gt;元素，为解析的Bean生成一个唯一beanName并注册 beanName = BeanDefinitionReaderUtils.generateBeanName( beanDefinition, this.readerContext.getRegistry(), true); &#125; else &#123; //如果&lt;Bean&gt;元素中没有配置id、别名或者name，且包含了子//&lt;Bean&gt;元素，为解析的Bean使用别名向IoC容器注册 beanName = this.readerContext.generateBeanName(beanDefinition); //为解析的Bean使用别名注册时，为了向后兼容 //Spring1.2/2.0，给别名添加类名后缀 String beanClassName = beanDefinition.getBeanClassName(); if (beanClassName != null &amp;&amp; beanName.startsWith(beanClassName) &amp;&amp; beanName.length() &gt; beanClassName.length() &amp;&amp; !this.readerContext.getRegistry().isBeanNameInUse(beanClassName)) &#123; aliases.add(beanClassName); &#125; &#125; if (logger.isDebugEnabled()) &#123; logger.debug("Neither XML 'id' nor 'name' specified - " + "using generated bean name [" + beanName + "]"); &#125; &#125; catch (Exception ex) &#123; error(ex.getMessage(), ele); return null; &#125; &#125; String[] aliasesArray = StringUtils.toStringArray(aliases); return new BeanDefinitionHolder(beanDefinition, beanName, aliasesArray); &#125; //当解析出错时，返回null return null; &#125; //详细对&lt;Bean&gt;元素中配置的Bean定义其他属性进行解析，由于上面的方法中已经对//Bean的id、name和别名等属性进行了处理，该方法中主要处理除这三个以外的其他属性数据 public AbstractBeanDefinition parseBeanDefinitionElement( Element ele, String beanName, BeanDefinition containingBean) &#123; //记录解析的&lt;Bean&gt; this.parseState.push(new BeanEntry(beanName)); //这里只读取&lt;Bean&gt;元素中配置的class名字，然后载入到BeanDefinition中去 //只是记录配置的class名字，不做实例化，对象的实例化在依赖注入时完成 String className = null; if (ele.hasAttribute(CLASS_ATTRIBUTE)) &#123; className = ele.getAttribute(CLASS_ATTRIBUTE).trim(); &#125; try &#123; String parent = null; //如果&lt;Bean&gt;元素中配置了parent属性，则获取parent属性的值 if (ele.hasAttribute(PARENT_ATTRIBUTE)) &#123; parent = ele.getAttribute(PARENT_ATTRIBUTE); &#125; //根据&lt;Bean&gt;元素配置的class名称和parent属性值创建BeanDefinition //为载入Bean定义信息做准备 AbstractBeanDefinition bd = createBeanDefinition(className, parent); //对当前的&lt;Bean&gt;元素中配置的一些属性进行解析和设置，如配置的单态(singleton)属性等 parseBeanDefinitionAttributes(ele, beanName, containingBean, bd); //为&lt;Bean&gt;元素解析的Bean设置description信息 bd.setDescription(DomUtils.getChildElementValueByTagName(ele, DESCRIPTION_ELEMENT)); //对&lt;Bean&gt;元素的meta(元信息)属性解析 parseMetaElements(ele, bd); //对&lt;Bean&gt;元素的lookup-method属性解析 parseLookupOverrideSubElements(ele, bd.getMethodOverrides()); //对&lt;Bean&gt;元素的replaced-method属性解析 parseReplacedMethodSubElements(ele, bd.getMethodOverrides()); //解析&lt;Bean&gt;元素的构造方法设置 parseConstructorArgElements(ele, bd); //解析&lt;Bean&gt;元素的&lt;property&gt;设置 parsePropertyElements(ele, bd); //解析&lt;Bean&gt;元素的qualifier属性 parseQualifierElements(ele, bd); //为当前解析的Bean设置所需的资源和依赖对象 bd.setResource(this.readerContext.getResource()); bd.setSource(extractSource(ele)); return bd; &#125; catch (ClassNotFoundException ex) &#123; error("Bean class [" + className + "] not found", ele, ex); &#125; catch (NoClassDefFoundError err) &#123; error("Class that bean class [" + className + "] depends on not found", ele, err); &#125; catch (Throwable ex) &#123; error("Unexpected failure during bean definition parsing", ele, ex); &#125; finally &#123; this.parseState.pop(); &#125; //解析&lt;Bean&gt;元素出错时，返回null return null; &#125; 只要使用过Spring，对Spring配置文件比较熟悉的人，通过对上述源码的分析，就会明白我们在Spring配置文件中元素的中配置的属性就是通过该方法解析和设置到Bean中去的。 注意：在解析元素过程中没有创建和实例化Bean对象，只是创建了Bean对象的定义类BeanDefinition，将元素中的配置信息设置到BeanDefinition中作为记录，当依赖注入时才使用这些记录信息创建和实例化具体的Bean对象。 上面方法中一些对一些配置如元信息(meta)、qualifier等的解析，我们在Spring中配置时使用的也不多，我们在使用Spring的元素时，配置最多的是属性，因此我们下面继续分析源码，了解Bean的属性在解析时是如何设置的。 BeanDefinitionParserDelegate解析元素BeanDefinitionParserDelegate在解析调用parsePropertyElements方法解析元素中的属性子元素，解析源码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102//解析&lt;Bean&gt;元素中的&lt;property&gt;子元素 public void parsePropertyElements(Element beanEle, BeanDefinition bd) &#123; //获取&lt;Bean&gt;元素中所有的子元素 NodeList nl = beanEle.getChildNodes(); for (int i = 0; i &lt; nl.getLength(); i++) &#123; Node node = nl.item(i); //如果子元素是&lt;property&gt;子元素，则调用解析&lt;property&gt;子元素方法解析 if (isCandidateElement(node) &amp;&amp; nodeNameEquals(node, PROPERTY_ELEMENT)) &#123; parsePropertyElement((Element) node, bd); &#125; &#125; &#125; //解析&lt;property&gt;元素 public void parsePropertyElement(Element ele, BeanDefinition bd) &#123; //获取&lt;property&gt;元素的名字 String propertyName = ele.getAttribute(NAME_ATTRIBUTE); if (!StringUtils.hasLength(propertyName)) &#123; error("Tag 'property' must have a 'name' attribute", ele); return; &#125; this.parseState.push(new PropertyEntry(propertyName)); try &#123; //如果一个Bean中已经有同名的property存在，则不进行解析，直接返回。 //即如果在同一个Bean中配置同名的property，则只有第一个起作用 if (bd.getPropertyValues().contains(propertyName)) &#123; error("Multiple 'property' definitions for property '" + propertyName + "'", ele); return; &#125; //解析获取property的值 Object val = parsePropertyValue(ele, bd, propertyName); //根据property的名字和值创建property实例 PropertyValue pv = new PropertyValue(propertyName, val); //解析&lt;property&gt;元素中的属性 parseMetaElements(ele, pv); pv.setSource(extractSource(ele)); bd.getPropertyValues().addPropertyValue(pv); &#125; finally &#123; this.parseState.pop(); &#125; &#125; //解析获取property值 public Object parsePropertyValue(Element ele, BeanDefinition bd, String propertyName) &#123; String elementName = (propertyName != null) ? "&lt;property&gt; element for property '" + propertyName + "'" : "&lt;constructor-arg&gt; element"; //获取&lt;property&gt;的所有子元素，只能是其中一种类型:ref,value,list等 NodeList nl = ele.getChildNodes(); Element subElement = null; for (int i = 0; i &lt; nl.getLength(); i++) &#123; Node node = nl.item(i); //子元素不是description和meta属性 if (node instanceof Element &amp;&amp; !nodeNameEquals(node, DESCRIPTION_ELEMENT) &amp;&amp; !nodeNameEquals(node, META_ELEMENT)) &#123; if (subElement != null) &#123; error(elementName + " must not contain more than one sub-element", ele); &#125; else &#123;//当前&lt;property&gt;元素包含有子元素 subElement = (Element) node; &#125; &#125; &#125; //判断property的属性值是ref还是value，不允许既是ref又是value boolean hasRefAttribute = ele.hasAttribute(REF_ATTRIBUTE); boolean hasValueAttribute = ele.hasAttribute(VALUE_ATTRIBUTE); if ((hasRefAttribute &amp;&amp; hasValueAttribute) || ((hasRefAttribute || hasValueAttribute) &amp;&amp; subElement != null)) &#123; error(elementName + " is only allowed to contain either 'ref' attribute OR 'value' attribute OR sub-element", ele); &#125; //如果属性是ref，创建一个ref的数据对象RuntimeBeanReference，这个对象 //封装了ref信息 if (hasRefAttribute) &#123; String refName = ele.getAttribute(REF_ATTRIBUTE); if (!StringUtils.hasText(refName)) &#123; error(elementName + " contains empty 'ref' attribute", ele); &#125; //一个指向运行时所依赖对象的引用 RuntimeBeanReference ref = new RuntimeBeanReference(refName); //设置这个ref的数据对象是被当前的property对象所引用 ref.setSource(extractSource(ele)); return ref; &#125; //如果属性是value，创建一个value的数据对象TypedStringValue，这个对象 //封装了value信息 else if (hasValueAttribute) &#123; //一个持有String类型值的对象 TypedStringValue valueHolder = new TypedStringValue(ele.getAttribute(VALUE_ATTRIBUTE)); //设置这个value数据对象是被当前的property对象所引用 valueHolder.setSource(extractSource(ele)); return valueHolder; &#125; //如果当前&lt;property&gt;元素还有子元素 else if (subElement != null) &#123; //解析&lt;property&gt;的子元素 return parsePropertySubElement(subElement, bd); &#125; else &#123; //propery属性中既不是ref，也不是value属性，解析出错返回null error(elementName + " must specify a ref or value", ele); return null; &#125; &#125; 通过对上述源码的分析，我们可以了解在Spring配置文件中，元素中元素的相关配置是如何处理的： ref被封装为指向依赖对象一个引用 value配置都会封装成一个字符串类型的对象 ref和value都通过“解析的数据类型属性值.setSource(extractSource(ele));”方法将属性值/引用与所引用的属性关联起来 在方法的最后对于元素的子元素通过parsePropertySubElement 方法解析，我们继续分析该方法的源码，了解其解析过程。 解析元素的子元素在BeanDefinitionParserDelegate类中的parsePropertySubElement方法对中的子元素解析，源码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485//解析&lt;property&gt;元素中ref,value或者集合等子元素 public Object parsePropertySubElement(Element ele, BeanDefinition bd, String defaultValueType) &#123; //如果&lt;property&gt;没有使用Spring默认的命名空间，则使用用户自定义的规则解析//内嵌元素 if (!isDefaultNamespace(ele)) &#123; return parseNestedCustomElement(ele, bd); &#125; //如果子元素是bean，则使用解析&lt;Bean&gt;元素的方法解析 else if (nodeNameEquals(ele, BEAN_ELEMENT)) &#123; BeanDefinitionHolder nestedBd = parseBeanDefinitionElement(ele, bd); if (nestedBd != null) &#123; nestedBd = decorateBeanDefinitionIfRequired(ele, nestedBd, bd); &#125; return nestedBd; &#125; //如果子元素是ref，ref中只能有以下3个属性：bean、local、parent else if (nodeNameEquals(ele, REF_ELEMENT)) &#123; //获取&lt;property&gt;元素中的bean属性值，引用其他解析的Bean的名称 //可以不再同一个Spring配置文件中，具体请参考Spring对ref的配置规则 String refName = ele.getAttribute(BEAN_REF_ATTRIBUTE); boolean toParent = false; if (!StringUtils.hasLength(refName)) &#123; //获取&lt;property&gt;元素中的local属性值，引用同一个Xml文件中配置 //的Bean的id，local和ref不同，local只能引用同一个配置文件中的Bean refName = ele.getAttribute(LOCAL_REF_ATTRIBUTE); if (!StringUtils.hasLength(refName)) &#123; //获取&lt;property&gt;元素中parent属性值，引用父级容器中的Bean refName = ele.getAttribute(PARENT_REF_ATTRIBUTE); toParent = true; if (!StringUtils.hasLength(refName)) &#123; error("'bean', 'local' or 'parent' is required for &lt;ref&gt; element", ele); return null; &#125; &#125; &#125; //没有配置ref的目标属性值 if (!StringUtils.hasText(refName)) &#123; error("&lt;ref&gt; element contains empty target attribute", ele); return null; &#125; //创建ref类型数据，指向被引用的对象 RuntimeBeanReference ref = new RuntimeBeanReference(refName, toParent); //设置引用类型值是被当前子元素所引用 ref.setSource(extractSource(ele)); return ref; &#125; //如果子元素是&lt;idref&gt;，使用解析ref元素的方法解析 else if (nodeNameEquals(ele, IDREF_ELEMENT)) &#123; return parseIdRefElement(ele); &#125; //如果子元素是&lt;value&gt;，使用解析value元素的方法解析 else if (nodeNameEquals(ele, VALUE_ELEMENT)) &#123; return parseValueElement(ele, defaultValueType); &#125; //如果子元素是null，为&lt;property&gt;设置一个封装null值的字符串数据 else if (nodeNameEquals(ele, NULL_ELEMENT)) &#123; TypedStringValue nullHolder = new TypedStringValue(null); nullHolder.setSource(extractSource(ele)); return nullHolder; &#125; //如果子元素是&lt;array&gt;，使用解析array集合子元素的方法解析 else if (nodeNameEquals(ele, ARRAY_ELEMENT)) &#123; return parseArrayElement(ele, bd); &#125; //如果子元素是&lt;list&gt;，使用解析list集合子元素的方法解析 else if (nodeNameEquals(ele, LIST_ELEMENT)) &#123; return parseListElement(ele, bd); &#125; //如果子元素是&lt;set&gt;，使用解析set集合子元素的方法解析 else if (nodeNameEquals(ele, SET_ELEMENT)) &#123; return parseSetElement(ele, bd); &#125; //如果子元素是&lt;map&gt;，使用解析map集合子元素的方法解析 else if (nodeNameEquals(ele, MAP_ELEMENT)) &#123; return parseMapElement(ele, bd); &#125; //如果子元素是&lt;props&gt;，使用解析props集合子元素的方法解析 else if (nodeNameEquals(ele, PROPS_ELEMENT)) &#123; return parsePropsElement(ele); &#125; //既不是ref，又不是value，也不是集合，则子元素配置错误，返回null else &#123; error("Unknown property sub-element: [" + ele.getNodeName() + "]", ele); return null; &#125; &#125; 通过上述源码分析，我们明白了在Spring配置文件中，对元素中配置的Array、List、Set、Map、Prop等各种集合子元素的都通过上述方法解析，生成对应的数据对象，比如ManagedList、ManagedArray、ManagedSet等，这些Managed类是Spring对象BeanDefiniton的数据封装，对集合数据类型的具体解析有各自的解析方法实现，解析方法的命名非常规范，一目了然，我们对集合元素的解析方法进行源码分析，了解其实现过程。 解析子元素在BeanDefinitionParserDelegate类中的parseListElement方法就是具体实现解析元素中的集合子元素，源码如下：1234567891011121314151617181920212223242526272829//解析&lt;list&gt;集合子元素 public List parseListElement(Element collectionEle, BeanDefinition bd) &#123; //获取&lt;list&gt;元素中的value-type属性，即获取集合元素的数据类型 String defaultElementType = collectionEle.getAttribute(VALUE_TYPE_ATTRIBUTE); //获取&lt;list&gt;集合元素中的所有子节点 NodeList nl = collectionEle.getChildNodes(); //Spring中将List封装为ManagedList ManagedList&lt;Object&gt; target = new ManagedList&lt;Object&gt;(nl.getLength()); target.setSource(extractSource(collectionEle)); //设置集合目标数据类型 target.setElementTypeName(defaultElementType); target.setMergeEnabled(parseMergeAttribute(collectionEle)); //具体的&lt;list&gt;元素解析 parseCollectionElements(nl, target, bd, defaultElementType); return target; &#125; //具体解析&lt;list&gt;集合元素，&lt;array&gt;、&lt;list&gt;和&lt;set&gt;都使用该方法解析 protected void parseCollectionElements( NodeList elementNodes, Collection&lt;Object&gt; target, BeanDefinition bd, String defaultElementType) &#123; //遍历集合所有节点 for (int i = 0; i &lt; elementNodes.getLength(); i++) &#123; Node node = elementNodes.item(i); //节点不是description节点 if (node instanceof Element &amp;&amp; !nodeNameEquals(node, DESCRIPTION_ELEMENT)) &#123; //将解析的元素加入集合中，递归调用下一个子元素 target.add(parsePropertySubElement((Element) node, bd, defaultElementType)); &#125; &#125; &#125; 经过对Spring Bean定义资源文件转换的Document对象中的元素层层解析，Spring IoC现在已经将XML形式定义的Bean定义资源文件转换为Spring IoC所识别的数据结构——BeanDefinition，它是Bean定义资源文件中配置的POJO对象在Spring IoC容器中的映射，我们可以通过AbstractBeanDefinition为入口，荣IoC容器进行索引、查询和操作。 通过Spring IoC容器对Bean定义资源的解析后，IoC容器大致完成了管理Bean对象的准备工作，即初始化过程，但是最为重要的依赖注入还没有发生，现在在IoC容器中BeanDefinition存储的只是一些静态信息，接下来需要向容器注册Bean定义信息才能全部完成IoC容器的初始化过程 解析过后的BeanDefinition在IoC容器中的注册让我们继续跟踪程序的执行顺序，接下来会到我们第3步中分析DefaultBeanDefinitionDocumentReader对Bean定义转换的Document对象解析的流程中，在其parseDefaultElement方法中完成对Document对象的解析后得到封装BeanDefinition的BeanDefinitionHold对象，然后调用BeanDefinitionReaderUtils的registerBeanDefinition方法向IoC容器注册解析的Bean，BeanDefinitionReaderUtils的注册的源码如下： 123456789101112131415//将解析的BeanDefinitionHold注册到容器中 public static void registerBeanDefinition(BeanDefinitionHolder definitionHolder, BeanDefinitionRegistry registry) throws BeanDefinitionStoreException &#123; //获取解析的BeanDefinition的名称 String beanName = definitionHolder.getBeanName(); //向IoC容器注册BeanDefinition registry.registerBeanDefinition(beanName, definitionHolder.getBeanDefinition()); //如果解析的BeanDefinition有别名，向容器为其注册别名 String[] aliases = definitionHolder.getAliases(); if (aliases != null) &#123; for (String aliase : aliases) &#123; registry.registerAlias(beanName, aliase); &#125; &#125; &#125; 当调用BeanDefinitionReaderUtils向IoC容器注册解析的BeanDefinition时，真正完成注册功能的是DefaultListableBeanFactory。 DefaultListableBeanFactory向IoC容器注册解析后的BeanDefinitionDefaultListableBeanFactory中使用一个HashMap的集合对象存放IoC容器中注册解析的BeanDefinition，向IoC容器注册的主要源码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445//存储注册的俄BeanDefinition private final Map&lt;String, BeanDefinition&gt; beanDefinitionMap = new ConcurrentHashMap&lt;String, BeanDefinition&gt;(); //向IoC容器注册解析的BeanDefiniton public void registerBeanDefinition(String beanName, BeanDefinition beanDefinition) throws BeanDefinitionStoreException &#123; Assert.hasText(beanName, "Bean name must not be empty"); Assert.notNull(beanDefinition, "BeanDefinition must not be null"); //校验解析的BeanDefiniton if (beanDefinition instanceof AbstractBeanDefinition) &#123; try &#123; ((AbstractBeanDefinition) beanDefinition).validate(); &#125; catch (BeanDefinitionValidationException ex) &#123; throw new BeanDefinitionStoreException(beanDefinition.getResourceDescription(), beanName, "Validation of bean definition failed", ex); &#125; &#125; //注册的过程中需要线程同步，以保证数据的一致性 synchronized (this.beanDefinitionMap) &#123; Object oldBeanDefinition = this.beanDefinitionMap.get(beanName); //检查是否有同名的BeanDefinition已经在IoC容器中注册，如果已经注册， //并且不允许覆盖已注册的Bean，则抛出注册失败异常 if (oldBeanDefinition != null) &#123; if (!this.allowBeanDefinitionOverriding) &#123; throw new BeanDefinitionStoreException(beanDefinition.getResourceDescription(), beanName, "Cannot register bean definition [" + beanDefinition + "] for bean '" + beanName + "': There is already [" + oldBeanDefinition + "] bound."); &#125; else &#123;//如果允许覆盖，则同名的Bean，后注册的覆盖先注册的 if (this.logger.isInfoEnabled()) &#123; this.logger.info("Overriding bean definition for bean '" + beanName + "': replacing [" + oldBeanDefinition + "] with [" + beanDefinition + "]"); &#125; &#125; &#125; //IoC容器中没有已经注册同名的Bean，按正常注册流程注册 else &#123; this.beanDefinitionNames.add(beanName); this.frozenBeanDefinitionNames = null; &#125; this.beanDefinitionMap.put(beanName, beanDefinition); //重置所有已经注册过的BeanDefinition的缓存 resetBeanDefinition(beanName); &#125; &#125; 至此，Bean定义资源文件中配置的Bean被解析过后，已经注册到IoC容器中，被容器管理起来，真正完成了IoC容器初始化所做的全部工作。现 在IoC容器中已经建立了整个Bean的配置信息，这些BeanDefinition信息已经可以使用，并且可以被检索，IoC容器的作用就是对这些注册的Bean定义信息进行处理和维护。这些的注册的Bean定义信息是IoC容器控制反转的基础，正是有了这些注册的数据，容器才可以进行依赖注入。 总结现在通过上面的代码，总结一下IOC容器初始化的基本步骤： 初始化的入口在容器实现中的 refresh()调用来完成 对 bean 定义载入 IOC 容器使用的方法是 loadBeanDefinition,其中的大致过程如下：通过 ResourceLoader 来完成资源文件位置的定位，DefaultResourceLoader 是默认的实现，同时上下文本身就给出了 ResourceLoader 的实现，可以从类路径，文件系统, URL 等方式来定为资源位置。如果是 XmlBeanFactory作为 IOC 容器，那么需要为它指定 bean 定义的资源，也就是说 bean 定义文件时通过抽象成 Resource 来被 IOC 容器处理的，容器通过 BeanDefinitionReader来完成定义信息的解析和 Bean 信息的注册,往往使用的是XmlBeanDefinitionReader 来解析 bean 的 xml 定义文件 - 实际的处理过程是委托给 BeanDefinitionParserDelegate 来完成的，从而得到 bean 的定义信息，这些信息在 Spring 中使用 BeanDefinition 对象来表示 - 这个名字可以让我们想到loadBeanDefinition,RegisterBeanDefinition 这些相关的方法 - 他们都是为处理 BeanDefinitin 服务的， 容器解析得到 BeanDefinitionIoC 以后，需要把它在 IOC 容器中注册，这由 IOC 实现 BeanDefinitionRegistry 接口来实现。注册过程就是在 IOC 容器内部维护的一个HashMap 来保存得到的 BeanDefinition 的过程。这个 HashMap 是 IoC 容器持有 bean 信息的场所，以后对 bean 的操作都是围绕这个HashMap 来实现的. 然后我们就可以通过 BeanFactory 和 ApplicationContext 来享受到 Spring IOC 的服务了,在使用 IOC 容器的时候，我们注意到除了少量粘合代码，绝大多数以正确 IoC 风格编写的应用程序代码完全不用关心如何到达工厂，因为容器将把这些对象与容器管理的其他对象钩在一起。基本的策略是把工厂放到已知的地方，最好是放在对预期使用的上下文有意义的地方，以及代码将实际需要访问工厂的地方。 Spring 本身提供了对声明式载入 web 应用程序用法的应用程序上下文,并将其存储在ServletContext 中的框架实现。 在使用 Spring IOC 容器的时候我们还需要区别两个概念: Beanfactory 和 Factory bean，其中 BeanFactory 指的是 IOC 容器的编程抽象，比如 ApplicationContext， XmlBeanFactory 等，这些都是 IOC 容器的具体表现，需要使用什么样的容器由客户决定,但 Spring 为我们提供了丰富的选择。 FactoryBean 只是一个可以在 IOC而容器中被管理的一个 bean,是对各种处理过程和资源使用的抽象,Factory bean 在需要时产生另一个对象，而不返回 FactoryBean本身,我们可以把它看成是一个抽象工厂，对它的调用返回的是工厂生产的产品。所有的 Factory bean 都实现特殊的org.springframework.beans.factory.FactoryBean 接口，当使用容器中 factory bean 的时候，该容器不会返回 factory bean 本身,而是返回其生成的对象。Spring 包括了大部分的通用资源和服务访问抽象的 Factory bean 的实现，其中包括:对 JNDI 查询的处理，对代理对象的处理，对事务性代理的处理，对 RMI 代理的处理等，这些我们都可以看成是具体的工厂,看成是SPRING 为我们建立好的工厂。也就是说 Spring 通过使用抽象工厂模式为我们准备了一系列工厂来生产一些特定的对象,免除我们手工重复的工作，我们要使用时只需要在 IOC 容器里配置好就能很方便的使用了]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[源码解读Spring IOC原理（一）]]></title>
    <url>%2Fjava%2Fspring%2Fjava%2Fspring%2F1%2F</url>
    <content type="text"><![CDATA[源码解读Spring IOC原理（一）什么是IOC/DI？IoC 容器：最主要是完成了完成对象的创建和依赖的管理注入等等。 先从我们自己设计这样一个视角来考虑： 所谓控制反转，就是把原先我们代码里面需要实现的对象创建、依赖的代码，反转给容器来帮忙实现。那么必然的我们需要创建一个容器，同时需要一种描述来让容器知道需要创建的对象与对象的关系。这个描述最具体表现就是我们可配置的文件。XmlXmlXml对象和对象关系怎么表示？ 可以用 xml ， properties 文件等语义化配置文件表示。 描述对象关系的文件存放在哪里？ 可能是 classpath ， filesystem ，或者是 URL 网络资源， servletContext 等。 回到正题，有了配置文件，还需要对配置文件解析。 不同的配置文件对对象的描述不一样，如标准的，自定义声明式的，如何统一？ 在内部需要有一个统一的关于对象的定义，所有外部的描述都必须转化成统一的描述定义。 如何对不同的配置文件进行解析？需要对不同的配置文件语法，采用不同的解析器 Spring IOC体系结构？BeanFactorySpring Bean的创建是典型的工厂模式，这一系列的Bean工厂，也即IOC容器为开发者管理对象间的依赖关系提供了很多便利和基础服务，在Spring中有许多的IOC容器的实现供用户选择和使用，其相互关系其中BeanFactory作为最顶层的一个接口类，它定义了IOC容器的基本功能规范，BeanFactory 有三个子类：ListableBeanFactory、HierarchicalBeanFactory 和AutowireCapableBeanFactory。但是从上图中我们可以发现最终的默认实现类是 DefaultListableBeanFactory，他实现了所有的接口。那为何要定义这么多层次的接口呢？查阅这些接口的源码和说明发现，每个接口都有他使用的场合，它主要是为了区分在 Spring 内部在操作过程中对象的传递和转化过程中，对对象的数据访问所做的限制。例如 ListableBeanFactory 接口表示这些 Bean 是可列表的，而 HierarchicalBeanFactory 表示的是这些 Bean 是有继承关系的，也就是每个Bean 有可能有父 Bean。AutowireCapableBeanFactory 接口定义 Bean 的自动装配规则。这四个接口共同定义了 Bean 的集合、Bean 之间的关系、以及 Bean 行为. 最基本的IOC容器接口BeanFactory 12345678910111213141516171819202122232425public interface BeanFactory &#123; //对FactoryBean的转义定义，因为如果使用bean的名字检索FactoryBean得到的对象是工厂生成的对象， //如果需要得到工厂本身，需要转义 String FACTORY_BEAN_PREFIX = "&amp;"; //根据bean的名字，获取在IOC容器中得到bean实例 Object getBean(String name) throws BeansException; //根据bean的名字和Class类型来得到bean实例，增加了类型安全验证机制。 Object getBean(String name, Class requiredType) throws BeansException; //提供对bean的检索，看看是否在IOC容器有这个名字的bean boolean containsBean(String name); //根据bean名字得到bean实例，并同时判断这个bean是不是单例 boolean isSingleton(String name) throws NoSuchBeanDefinitionException; //得到bean实例的Class类型 Class getType(String name) throws NoSuchBeanDefinitionException; //得到bean的别名，如果根据别名检索，那么其原名也会被检索出来 String[] getAliases(String name); &#125; 在BeanFactory里只对IOC容器的基本行为作了定义，根本不关心你的bean是如何定义怎样加载的。正如我们只关心工厂里得到什么的产品对象，至于工厂是怎么生产这些对象的，这个基本的接口不关心。 而要知道工厂是如何产生对象的，我们需要看具体的IOC容器实现，spring提供了许多IOC容器的实现。比如XmlBeanFactory，ClasspathXmlApplicationContext等。其中XmlBeanFactory就是针对最基本的ioc容器的实现，这个IOC容器可以读取XML文件定义的BeanDefinition（XML文件中对bean的描述）,如果说XmlBeanFactory是容器中的屌丝，ApplicationContext应该算容器中的高帅富.ApplicationContext是Spring提供的一个高级的IoC容器，它除了能够提供IoC容器的基本功能外，还为用户提供了以下的附加服务。 从ApplicationContext接口的实现，我们看出其特点： 支持信息源，可以实现国际化。（实现MessageSource接口） 访问资源。(实现ResourcePatternResolver接口，这个后面要讲) 支持应用事件。(实现ApplicationEventPublisher接口) BeanDefinitionSpringIOC容器管理了我们定义的各种Bean对象及其相互的关系，Bean对象在Spring实现中是以BeanDefinition来描述的，其继承体系如下 Bean 的解析过程非常复杂，功能被分的很细，因为这里需要被扩展的地方很多，必须保证有足够的灵活性，以应对可能的变化。Bean 的解析主要就是对Spring 配置文件的解析。这个解析过程主要通过下图中的类完成：]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA动态代理机制]]></title>
    <url>%2Fjava%2Fjava%2Fproxy-1%2F</url>
    <content type="text"><![CDATA[JAVA动态代理机制以及使用场景什么是代理？大道理上讲代理是一种软件设计模式，目的地希望能做到代码重用。具体上讲，代理这种设计模式是通过不直接访问被代理对象的方式，而访问被代理对象的方法。这个就好比 商户—-&gt;明星经纪人(代理)—-&gt;明星这种模式。我们可以不通过直接与明星对话的情况下，而通过明星经纪人(代理)与其产生间接对话。 什么情况下使用代理？ 设计模式中有一个设计原则是开闭原则，是说对修改关闭对扩展开放，我们在工作中有时会接手很多前人的代码，里面代码逻辑让人摸不着头脑(sometimes the code is really like shit)，这时就很难去下手修改代码，那么这时我们就可以通过代理对类进行增强。 我们在使用RPC框架的时候，框架本身并不能提前知道各个业务方要调用哪些接口的哪些方法 。那么这个时候，就可用通过动态代理的方式来建立一个中间人给客户端使用，也方便框架进行搭建逻辑，某种程度上也是客户端代码和框架松耦合的一种表现。 Spring的AOP机制就是采用动态代理的机制来实现切面编程。 静态代理和动态代理我们根据加载被代理类的时机不同，将代理分为静态代理和动态代理。如果我们在代码编译时就确定了被代理的类是哪一个，那么就可以直接使用静态代理；如果不能确定，那么可以使用类的动态加载机制，在代码运行期间加载被代理的类这就是动态代理，比如RPC框架和Spring AOP机制。 静态代理我们先创建一个接口，遗憾的是java api代理机制求被代理类必须要实现某个接口，对于静态代理方式代理类也要实现和被代理类相同的接口；对于动态代理代理类则不需要显示的实现被代理类所实现的接口。 接口文件：IUser.java123public interface IUser &#123; public String getUserName();&#125; 明星：User.java1234567public class User implements IUser&#123; @Override public void getUserName() &#123; return "我叫Fighting"; &#125;&#125; 经纪人：UserProxyFactory.java1234567891011121314151617public class UserProxyFactory implements IUser &#123; private IUser user; public UserProxyFactory(IUser user)&#123; this.user = user; &#125; @Override public void getUserName() &#123; // TODO Auto-generated method stub System.out.println("代理开始"); this.user.sayHello(); System.out.println("代理结束"); &#125;&#125; 商户：app.java 1234567891011public class app &#123; public static void main(String[] args) &#123; // TODO Auto-generated method stub //user为被代理的对象，某些情况下 我们不希望修改已有的代码，我们采用代理来间接访问 User user = new User(); //创建代理类对象 UserProxyFactory proxy = new UserProxyFactory(user); //调用代理类对象的方法 System.out.println(proxy.getUserName()); &#125;&#125; 静态代理看起来是比较简单的，没有什么问题只不过是在代理类中引入了被代理类的对象而已。那么接下来我们看看动态代理。 动态代理IUser.java不变，User.java不变，UserProxyFactory.java发生改变 123456789101112131415161718192021222324public class UserProxyFactory &#123; private Object target; public UserProxyFactory(Object target) &#123; this.target = target; &#125; public Object getProxyInstance()&#123; return Proxy.newProxyInstance( target.getClass().getClassLoader(), target.getClass().getInterfaces(), new InvocationHandler() &#123; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println("代理开始"); Object retVal = method.invoke(target, args); System.out.println("代理结束"); return retVal; &#125; &#125; ); &#125;&#125; 其中最重要的一点，类似于Spring AOP切面，在代理开始,代理结束地方加上逻辑，就成了切面方式了！这也是为什么要用代理的一个重要原因之一！你不用修改任何已经编写好的代码，只要使用代理就可以灵活的加入任何东西，将来不喜欢了，不用也不会影响原来的代码。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>proxy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle安装（四）]]></title>
    <url>%2Foracle%2Foracle%2F4%2F</url>
    <content type="text"><![CDATA[Linux下Oracle安装（四）启动监听和数据库123# /oracle/app/oracle/product/11.2.0/dbhome_1/bin/lsnrctl start# /oracle/app/oracle/product/11.2.0/dbhome_1/bin/sqlplus / as sysdbaSQL&gt; startup 设置dba密码1SQL&gt; alter user sys identified by 123456; 使用Oracle客户端，创建表空间1234567create tablespace ccmp_app -表空间名称logging datafile '/oracle/ccmp-databases/ccmp_app' -表空间存储位置size 10m autoextend on next 10m maxsize 20480m extent management local; 使用Oracle客户端，创建临时表空间123456create temporary tablespace ccmp_app_temp -表空间名称tempfile '/oracle/ccmp-databases/ccmp_app_temp' -表空间存储位置size 10m autoextend on next 10m maxsize 20480m extent management local; 创建用户和指定表空间123SQL&gt; create user ccmp_app identified by 123456;SQL&gt; default tablespace ccmp_app;SQL&gt; temporary tablespace ccmp_app_temp; 赋予用户权限1SQL&gt; grant connect,resource,dba to ccmp_app;]]></content>
      <categories>
        <category>oracle</category>
      </categories>
      <tags>
        <tag>oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle安装（三）]]></title>
    <url>%2Foracle%2Foracle%2F3%2F</url>
    <content type="text"><![CDATA[Linux下Oracle安装（三）配置监听文件和本地服务1# /oracle/app/oracle/product/11.2.0/dbhome_1/bin/netca 创建监听服务，作为Oracle的服务端 使用tcp协议 Oracle的默认的端口是1521 是否要创建另外一个监听文件，这里就选NO 配置本地网络服务名，作为Oracle的客户端 这里填服务器的ip 这里选yes测试一下，之后就和之前一样结束即可，到此Oracle的安装部署就已经完成了]]></content>
      <categories>
        <category>oracle</category>
      </categories>
      <tags>
        <tag>oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle安装（二）]]></title>
    <url>%2Foracle%2Foracle%2F2%2F</url>
    <content type="text"><![CDATA[Linux下Oracle安装（二）运行Oracle安装文件 123# su oracle# source ~/.bash_profile# /opt/oracle/database/runInstaller -jreLoc /opt/jdk1.7.0_80/ 运行Oracle自带脚本12# /oracle/app/oraInventory/orainstRoot.sh# /oracle/app/oracle/product/11.2.0/dbhome_1/root.sh 安装数据库1# /oracle/app/oracle/product/11.2.0/dbhome_1/bin/dbca 这里有3个模板，在这里就使用第一个，一般用途或事务处理 在这里需要填上之前在环境变量文件中写的sid Oracle的EM资料档案库，可以做到很多的功能实现，可以做到很多的功能，但是如果是不需要的话就把钩去掉 在这里设置用户密码，可以让用户的密码单独不一样，在这里就设置成一样的 这里使用文件系统 在这里设置闪回区大小，建议20~30GB，这个要根据磁盘的时间情况设置，闪回区如果满了Oracle数据库会停止，所以这个需要注意 说明示例，这里不需要 配置Oracle的默认的一些系统配置，这里需要注意的是字符集，这个需要根据需求修改 这样数据库的创建dbca的部分就安装完成了]]></content>
      <categories>
        <category>oracle</category>
      </categories>
      <tags>
        <tag>oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle安装（一）]]></title>
    <url>%2Foracle%2Foracle%2F1%2F</url>
    <content type="text"><![CDATA[Linux下Oracle安装（一）在Linux中安装Oracle主要有以下几步 安装Oracle依赖包 1# yum -y install binutils compat-libstdc++-33 elfutils-libelf elfutils-libelf-devel glibc glibc-common glibc-devel gcc gcc-c++ libaio-devel libaio libgcc libstdc++ libstdc++-devel make sysstat unixODBC unixODBC-devel pdksh ksh libaio.i686 glibc.i686 compat-libstdc++-33.i686 libaio-devel.i686 libgcc.i686 libstdc++.i686 unixODBC.i686 unixODBC-devel.i686 创建Oracle对应目录并设置权限 12# mkdir -p /oracle/app/oracle &amp;&amp; chown -R oracle:oinstall /oracle/app/oracle# mkdir -p /oracle/app/oraInventory &amp;&amp; chown -R oracle:oinstall /oracle/app/oraInventory 安装图形界面 123456# yum install xhost# yum grouplist# yum groupinstall "X Window System"# yum groupinstall Desktop# yum install xterm# yum install xclock 设置图形界面显示位置 export DISPLAY=客户端IP:0.0 创建Oracle系统用户 1234# groupadd oinstall # groupadd dba # useradd -g oinstall -G dba oracle # passwd oracle 修改系统内核（可选） vi /etc/sysctl.conf 新增内容如下： kernel.shmall = 2097152 kernel.shmmax = 536870912 kernel.shmmni = 4096 kernel.sem = 250 32000 100 128 fs.aio-max-nr = 1048576 fs.file-max = 6815744 net.ipv4.ip_local_port_range = 9000 65500 net.core.rmem_default = 262144 net.core.rmem_max = 4194304 net.core.wmem_default = 262144 net.core.wmem_max = 1048586 配置文件限制（可选） vi /etc/security/limits.conf 内容如下： oracle soft nproc 2047 oracle hard nproc 16384 oracle soft nofile 1024 oracle hard nofile 65536 配置Oracle环境变量 12# su oracle# vi .bash_profile bash_profile内容修改如下： export ORACLE_BASE=/oracle/app/oracle #ORACLE基础目录 export ORACLE_HOME=/oracle/app/oracle/product/11.2.0/dbhome_1 #ORACLE安装目录 export ORACLE_SID=CCMP #SID export PATH=$ORACLE_HOME/bin:$PATH export ORACLE_TERM=xterm export TNS_ADMIN=$ORACLE_HOME/network/admin export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$ORACLE_HOME/oracm/lib:$ORACLE_HOME/lib export CLASSPATH=$CLASSPATH:$ORACLE_HOME/rdbms/jlib:$ORACLE_HOME/jlib:$ORACLE_HOME/network/lib export LANG=en_US.gbk export NLS_LANG=american_america.ZHS16GBK export EDITOR=vi PATH=$PATH:$HOME/.local/bin:$HOME/bin export PATH 使用Xshell/MobaXterm来显示界面]]></content>
      <categories>
        <category>oracle</category>
      </categories>
      <tags>
        <tag>oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建跳板机]]></title>
    <url>%2Flinux%2Flinux%2Fssh-drawboard-1%2F</url>
    <content type="text"><![CDATA[前言近遇到这样一个问题，我在阿里云室架设了一台服务器，发现外部网元对服务器攻击非常平凡，都知道服务器开放出来的端口越多，对服务器的风险越大。如何来规避这些风险呢，下面具体说明在阿里云上搭建一个跳板机来访问内网服务器，提升服务器的安全。 描述一下目前的机器状况，梳理梳理： 机器 IP 用户名 备注 A 123.78.150.91 root 外网服务器，相当于桥梁的作用，跳板机 B 172.18.62.123 root 目标服务器，处于内网 C 123.123.123.123 win 自己的电脑 解决方法通俗地说：就是在机器C上使用xshell/mobaXterm软件连接A机器，自动转向到B机器上。 实现前的准备每台都要安装ssh的客户端，在这里我使用的是centos7，都自带ssh。如果是使用其他版本Linux，请手动Google一下咯。 介绍一下使用到的ssh参数反向代理ssh -fCNR 正向代理ssh -fCNL -f 后台执行ssh指令-C 允许压缩数据-N 不执行远程指令-R 将远程主机(服务器)的某个端口转发到本地端指定机器的指定端口-L 将本地机(客户机)的某个端口转发到远端指定机器的指定端口-p 指定远程主机的端口 首先在A上面操作建立A机器到B机器的反向代理，具体指令为ssh -fCNR [B机器IP或省略]:[B机器端口]:[A机器的IP]:[A机器端口] [登陆B机器的用户名@服务器IP] 在这里我使用了B机器的2222端口，以及A机器的22端口，按照上面的指令就是这样子的操作 1ssh -ngfNTR 2222:172.18.62.123:22 root@123.78.150.91 -o ServerAliveInterval=300 检验是否已经启动了可以使用ps aux | grep ssh指令来查看 阿里云打开端口A服务器要打开2222B服务器要打开22，仅限A服务器访问 接着在B上面操作建立B机器的ssh-key 1ssh-keygen 然后一直按回车就可以了，将~/ssh/id_rsa.pub拷贝给A服务器~/ssh/authorized_keys 在B上用autossh建立稳定隧道centos7上没有默认安装autossh的，所以使用一下命令安装 1yum install autossh 在B服务器上使用autossh建立稳定隧道 1autossh -p 22 -M 6777 -fNR 2222:127.0.0.1:22 root@123.78.150.91 命令说明-M 6777 这个表示是监控端口，检测到这个端口不通会重新连接2222:127.0.0.1:22 2222表示需要在远程主机上开启的端口，22表示本地的ssh端口root@123.78.150.91 -p 22 root为vps上面的用户，22为vps ssh端口 现在就可以在A服务器使用SSH端口为2222访问B服务器了]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ssh</tag>
        <tag>drawboard</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ实战（三）]]></title>
    <url>%2Fmq%2Fmq%2Frocketmq%2F3%2F</url>
    <content type="text"><![CDATA[关于多Master多Slave的说明 由于在之前的博客中已经搭建了双Master，其实多Master多Slave大同小异，因此这里并不会一步步的演示搭建多Master多Slave，而是从思路上，分析下重点应该注意的配置项。 这四台机器，对外是一个统一的整体，是一个rocketmq cluster，因此需要brokerClusterName保持统一 123机器是121的从，124机器是122的从，如何在配置中体现？ 主和从的brokerName需要保持一致，另外brokerId标示了谁是主，谁是从（brokerId=0的就是主，大于0的就是从） 注意namesrvAddr的地址是4台NameServer 配置项中brokerRole需要指明 ASYNC_MASTER（异步复制Master） or SYNC_MASTER（同步双写Master） or SLAVE（从） 和以前的多Master启动方式一致，先启动4台Namesrv，然后用指定配置文件的方式启动Master/Slave即可 多Master多Slave的好处在于，即便集群中某个broker挂了，也可以继续消费，保证了实时性的高可用，但是并不是说某个master挂了，slave就可以升级master，开源版本的rocketmq是不可以的。也就是说，在这种情况下，slave只能提供读的功能，将失去消息负载的能力。 Queue in Topic对于RocketMQ而言，Topic只是一个逻辑上的概念，真正的消息存储其实是在Topic中的Queue中。想一想，为什么RocketMQ要这要设计呢？其实是为了消息的顺序消费，后文中将为大家介绍。 初步认识RocketMQ的核心模块 rocketmq-broker：接受生产者发来的消息并存储（通过调用rocketmq-store），消费者从这里取得消息。 rocketmq-client：提供发送、接受消息的客户端API。 rocketmq-namesrv：NameServer，类似于Zookeeper，这里保存着消息的TopicName，队列等运行时的元信息。（有点NameNode的味道） rocketmq-common：通用的一些类，方法，数据结构等 rocketmq-remoting：基于Netty4的client/server + fastjson序列化 + 自定义二进制协议 rocketmq-store：消息、索引存储等 rocketmq-filtersrv：消息过滤器Server，需要注意的是，要实现这种过滤，需要上传代码到MQ！【一般而言，我们利用Tag足以满足大部分的过滤需求，如果更灵活更复杂的过滤需求，可以考虑filtersrv组件】 rocketmq-tools：命令行工具 Order MessageRocketMQ提供了3种模式的Producer： NormalProducer（普通） OrderProducer（顺序） TransactionProducer（事务） 在前面的博客当中，涉及的都是NormalProducer，调用传统的send方法，消息是无序的。接下来，我们来看看顺序消费。模拟这样一个场景，如果一个用户完成一个订单需要3条消息，比如订单的创建、订单的支付、订单的发货，很显然，同一个用户的订单消息必须要顺序消费，但是不同用户之间的订单可以并行消费。 生产者端代码示例： 注意，一个Message除了Topic/Tag外，还有Key的概念。上图的send方法不同于以往，有一个MessageQueueSelector，将用于指定特定的消息发往特定的队列当中！ 注意，在以前普通消费消息时设置的回调是MessageListenerConcurrently，而顺序消费的回调设置是MessageListenerOrderly。 当我们启动2个Consumer进行消费时，可以观察到： 可以观察得到，虽然从全局上来看，消息的消费不是有序的，但是每一个订单下的3条消息是顺序消费的！ 其实，如果需要保证消息的顺序消费，那么很简单，首先需要做到一组需要有序消费的消息发往同一个broker的同一个队列上！其次消费者端采用有序Listener即可。 这里，RocketMQ底层是如何做到消息顺序消费的，看一看源码你就能大概了解到，至少来说，在多线程消费场景下，一个线程只去消费一个队列上的消息，那么自然就保证了消息消费的顺序性，同时也保证了多个线程之间的并发性。也就是说其实broker并不能完全保证消息的顺序消费，它仅仅能保证的消息的顺序发送而已！ 关于多线程消费这块，RocketMQ早就替我们想好了，这样设置即可： 想一想，在ActiveMQ中，我们如果想实现并发消费的话，恐怕还得搞个线程池提交任务吧，RocketMQ让我们的工作变得简单！ Transaction Message在说事务消息之前，我们先来说说分布式事务的那些事！ 什么是分布式事务，我的理解是一半事务。怎么说，比如有2个异构系统，A异构系统要做T1，B异构系统要做T2，要么都成功，要么都失败。要知道异构系统，很显然，不在一个数据库实例上，它们往往分布在不同物理节点上，本地事务已经失效。 2阶段提交协议，Two-Phase Commit，是处理分布式事务的一种常见手段。2PC，存在2个重要角色：事务协调器（TC），事务执行者。 2PC，可以看到节点之间的通信次数太多了，时间很长！时间变长了，从而导致，事务锁定的资源时间也变长了，造成资源等待时间变长！在高并发场景下，存在严重的性能问题 下面，我们来看看MQ在高并发场景下，是如何解决分布式事务的。 考虑生活中的场景： 我们去北京庆丰包子铺吃炒肝，先去营业员那里付款（Action1），拿到小票（Ticket），然后去取餐窗口排队拿炒肝（Action2）。思考2个问题：第一，为什么不在付款的同时，给顾客炒肝？如果这样的话，会增加处理时间，使得后面的顾客等待时间变长，相当于降低了接待顾客的能力（降低了系统的QPS）。第二，付了款，拿到的是Ticket，顾客为什么会接受？从心理上说，顾客相信Ticket会兑现炒肝。事实上也是如此，就算在最后炒肝没了，或者断电断水（系统出现异常），顾客依然可以通过Ticket进行退款操作，这样都不会有什么损失！（虽然这么说，但是实际上包子铺最大化了它的利益，如果炒肝真的没了，浪费了顾客的时间，不过顾客顶多发发牢骚，最后接受） 生活已经告诉我们处理分布式事务，保证数据最终一致性的思路！这个Ticket（凭证）其实就是消息！ 业务操作和消息的生成耦合在一起，保证了只要A银行的账户发生扣款，那么一定会生成一条转账消息。只要A银行系统的事务成功提交，我们可以通过实时消息服务，将转账消息通知B银行系统，如果B银行系统回复成功，那么A银行系统可以在table中设置这条转账消息的状态。 这样耦合的方式，从架构上来看，就有点不太优雅，而且存在一些问题。比如说，消息的存储实质上是在A银行系统中的，如果A银行系统出了问题，将导致无法转账。如果解耦，将消息独立出来呢？ 如上图所示，消息数据独立存储，业务和消息解耦，实质上消息的发送有2次，一条是转账消息，另一条是确认消息。 到这里，我们先来看看基于RocketMQ的代码： 生产者这里用到是：TransactionMQProducer。 这里涉及到2个角色：本地事务执行器（代码中的TransactionExecuterImpl）、服务器回查客户端Listener（代码中的TransactionCheckListener）。 如果事务消息发送到MQ上后，会回调 本地事务执行器；但是此时事务消息是prepare状态，对消费者还不可见，需要 本地事务执行器 返回RMQ一个确认消息。 事务消息是否对消费者可见，完全由事务返回给RMQ的状态码决定（状态码的本质也是一条消息）。 生产者发送了2条消息给RMQ，有一条本地事务执行成功，有一条本地事务执行失败。 2条业务消息 + 2条确认消息 因此是4条； 注意，到消费者只消费了一条数据，就是只有告诉RMQ本地事务执行成功的那条消息才会被消费！因此是1条！ 但是，注意到本地事务执行失败的消息，RMQ并没有check listener？这是为什么呢？因为RMQ在3.0.8的时候还是支持check listener回查机制的，但是到了3.2.6的时候将事务回查机制“阉割”了！ 那么3.0.8的时候，RMQ是怎么做事务回查的呢？看一看源码，你会知道，其实事务消息开始是prepare状态，然后RMQ会将其持久化到MySQL当中，然后如果收到确认消息，就删除掉这条prepare消息，如果迟迟收不到确认消息，那么RMQ会定时的扫描prepare消息，发送给produce group进行回查确认！ 到这里，问题来了，要知道3.2.6版本，没有回查机制了，会存在问题么？ 当然会存在问题！假设，我们发送一条转账事务消息给RMQ，成功后回调本地事务，DB减操作成功，刚准备给RMQ一个确认消息，此时突然断电，或者网络抖动，使得这条确认消息没有发送出去。此时RMQ中的那条转账事务消息，始终处于prepare状态，消费者读取不到，但是却已经完成一方的账户资金变动！！！ 既然，RMQ3.2.6版本不为我们进行回查，那么只能由我们自己完成了。]]></content>
      <categories>
        <category>mq</category>
      </categories>
      <tags>
        <tag>mq</tag>
        <tag>rocketmq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ实战（二）]]></title>
    <url>%2Fmq%2Fmq%2Frocketmq%2F2%2F</url>
    <content type="text"><![CDATA[Quick Start写一个简单的生产者、消费者，带大家快速体验RocketMQ~ Maven配置： 生产者： 消费者： 无论生产者、消费者都必须给出GroupName，而且具有唯一性！生产到哪个Topic的哪个Tag下，消费者也是从Topic的哪个Tag进行消费，可见这个Tag有点类似于JMS Selector机制，即实现消息的过滤。生产者、消费者需要设置NameServer地址。这里，采用的是Consumer Push的方式，即设置Listener机制回调，相当于开启了一个线程。以后为大家介绍Consumer Pull的方式。 我们看一下运行结果： 仔细看看生产者结果输出，你会发现，有的消息发往broker-a，有的在broker-b上，自动实现了消息的负载均衡！ 这里消费消息是没有什么顺序的，以后我们在来谈消息的顺序性。 在多Master模式中，如果某个Master进程挂了，显然这台broker将不可用，上面的消息也将无法消费，要知道开源版本的RocketMQ是没有提供切换程序，来自动恢复故障的，因此在实际开发中，我们一般提供一个监听程序，用于监控Master的状态。 在ActiveMQ中，生产消息的时候会提供是否持久化的选择，但是对于RocketMQ而言，消息是一定会被持久化的！ 上面的消费者采用的是Push Consumer的方式，那么监听的Listener中的消息List到底是多少条呢？虽然提供了API，如consumer.setConsumeMessageBatchMaxSize(10)，实际上即使设置了批量的条数，但是注意了，是最大是10，并不意味着每次batch的都是10，只有在消息有挤压的情况下才有可能。而且Push Consumer的最佳实践方式就是一条条的消费，如果需要batch，可以使用Pull Consumer。 务必保证先启动消费者进行Topic订阅，然后在启动生产者进行生产（否则极有可能导致消息的重复消费，重复消费，重复消费！重要的事情说三遍！关于消息的重复问题后续给大家介绍~）。而且在实际开发中，有时候不会批量的处理消息，而是原子性的，单线程的去一条一条的处理消息，这样就是实时的在处理消息。（批量的处理海量的消息，可以考虑Kafka） 初步了解消息失败重试机制消息失败，无非涉及到2端：从生产者端发往MQ的失败；消费者端从MQ消费消息的失败； 生产者端的失败重试 生产者端的消息失败：比如网络抖动导致生产者发送消息到MQ失败。 上图代码示例的处理手段是：如果该条消息在1S内没有发送成功，那么重试3次。 消费者端的失败重试 消费者端的失败，分为2种情况，一个是timeout，一个是exceptiontimeout，比如由于网络原因导致消息压根就没有从MQ到消费者上，在RocketMQ内部会不断的尝试发送这条消息，直至发送成功为止！（比如集群中一个broker失败，就尝试另一个broker）exception，消息正常的到了消费者，结果消费者发生异常，处理失败了。这里涉及到一些问题，需要我们思考下，比如，消费者消费消息的状态有哪些定义？如果失败，MQ将采取什么策略进行重试？假设一次性批量PUSH了10条，其中某条数据消费异常，那么消息重试是10条呢，还是1条呢？而且在重试的过程中，需要保证不重复消费吗？ 消息消费的状态，有2种，一个是成功（CONSUME_SUCCESS），一个是失败&amp;稍后重试（RECONSUME_LATER） 在启动broker的过程中，可以观察下日志，你会发现RECONSUME_LATER的策略。 如果消费失败，那么1S后再次消费，如果失败，那么5S后，再次消费，……直至2H后如果消费还失败，那么该条消息就会终止发送给消费者了！ RocketMQ为我们提供了这么多次数的失败重试，但是在实际中也许我们并不需要这么多重试，比如重试3次，还没有成功，我们希望把这条消息存储起来并采用另一种方式处理，而且希望RocketMQ不要在重试呢，因为重试解决不了问题了！这该如何做呢？ 我们先来看一下一条消息MessageExt对象的输出： MessageExt [queueId=0, storeSize=137, queueOffset=0, sysFlag=0, bornTimestamp=1492213846916, bornHost=/192.168.99.219:50478, storeTimestamp=1492213846981, storeHost=/192.168.99.121:10911, msgId=C0A8637900002A9F0000000000000000, commitLogOffset=0, bodyCRC=613185359, reconsumeTimes=0, preparedTransactionOffset=0, toString()=Message [topic=TopicTest2, flag=0, properties={TAGS=TagA, WAIT=true, MAX_OFFSET=3, MIN_OFFSET=0}, body=16]] 注意到reconsumeTimes属性，这个属性就代表消息重试的次数！来看一段代码： 注意了，对于消费消息而言，存在2种指定的状态（成功 OR 失败重试），如果一条消息在消费端处理没有返回这2个状态，那么相当于这条消息没有达到消费者，势必会再次发送给消费者！也即是消息的处理必须有返回值，否则就进行重发。 天然的消息负载均衡及高效的水平扩展机制 对于RocketMQ而言，通过ConsumeGroup的机制，实现了天然的消息负载均衡！通俗点来说，RocketMQ中的消息通过ConsumeGroup实现了将消息分发到C1/C2/C3/……的机制，这意味着我们将非常方便的通过加机器来实现水平扩展！ 我们考虑一下这种情况：比如C2发生了重启，一条消息发往C3进行消费，但是这条消息的处理需要0.1S，而此时C2刚好完成重启，那么C2是否可能会收到这条消息呢？答案是肯定的，也就是consume broker的重启，或者水平扩容，或者不遵守先订阅后生产消息，都可能导致消息的重复消费！关于去重的话题会在后续中予以介绍！ 至于消息分发到C1/C2/C3，其实也是可以设置策略的。 集群消费 AND 广播消费 RocketMQ的消费方式有2种，在默认情况下，就是集群消费，也就是上面提及的消息的负载均衡消费。另一种消费模式，是广播消费。广播消费，类似于ActiveMQ中的发布订阅模式，消息会发给Consume Group中的每一个消费者进行消费。]]></content>
      <categories>
        <category>mq</category>
      </categories>
      <tags>
        <tag>mq</tag>
        <tag>rocketmq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ实战（一）]]></title>
    <url>%2Fmq%2Fmq%2Frocketmq%2F1%2F</url>
    <content type="text"><![CDATA[阿里巴巴有2大核心的分布式技术，一个是OceanBase，另一个就是RocketMQ。在实际项目中已经领教过RocketMQ的强大，本人计划写一个RocketMQ实战系列，将涵盖RocketMQ的简介，环境搭建，初步使用、API详解、架构分析、管理员集群操作等知识。 What is RocketMQ?RocketMQ作为一款分布式的消息中间件（阿里的说法是不遵循任何规范的，所以不能完全用JMS的那一套东西来看它），经历了Metaq1.x、Metaq2.x的发展和淘宝双十一的洗礼，在功能和性能上远超ActiveMQ。 要知道RocketMQ原生就是支持分布式的，而ActiveMQ原生存在单点性。 RocketMQ可以保证严格的消息顺序，而ActiveMQ无法保证！ RocketMQ提供亿级消息的堆积能力，这不是重点，重点是堆积了亿级的消息后，依然保持写入低延迟！ 丰富的消息拉取模式（Push or Pull）Push好理解，比如在消费者端设置Listener回调；而Pull，控制权在于应用，即应用需要主动的调用拉消息方法从Broker获取消息，这里面存在一个消费位置记录的问题（如果不记录，会导致消息重复消费）。 在Metaq1.x/2.x的版本中，分布式协调采用的是Zookeeper，而RocketMQ自己实现了一个NameServer，更加轻量级，性能更好！ 消息失败重试机制、高效的订阅者水平扩展能力、强大的API、事务机制等等（后续详细介绍） 初步理解Producer/Consumer GroupActiveMQ中并没有Group这个概念，而在RocketMQ中理解Group的机制很重要。 想过没有，通过Group机制，让RocketMQ天然的支持消息负载均衡！ 比如某个Topic有9条消息，其中一个Consumer Group有3个实例（3个进程 OR 3台机器），那么每个实例将均摊3条消息！（注意RocketMQ只有一种模式，即发布订阅模式。） RocketMQ的Broker集群部署模式还挺多的，比如单Master模式、多Master模式、多Master多Slave模式（异步复制）、多Master多Slave模式（同步双写）等。明确个概念，RocketMQ Slave不可以写，可以读，类似于MySQL的主从机制。 单Master模式：无需多言，一旦单个broker重启或宕机，一切都结束了！很显然，线上不可以使用。 多Master模式：全是Master，没有Slave。当然，一个broker宕机了，应用是无影响的，缺点在于宕机的Master上未被消费的消息在Master没有恢复之前不可以订阅。 多Master多Slave模式（异步复制）：多对Master-Slave，高可用！采用异步复制的方式，主备之间短暂延迟，MS级别。Master宕机，消费者可以从Slave上进行消费，不受影响，但是Master的宕机，会导致丢失掉极少量的消息。 多Master多Slave模式（同步双写）：和上面的区别点在于采用的是同步方式，也就是在Master/Slave都写成功的前提下，向应用返回成功，可见不论是数据，还是服务都没有单点，都非常可靠！缺点在于同步的性能比异步稍低。 这里我将采用2个Master的方式进行搭建演示，会了双Master，其他的将很简单。（多Master在实际中也是非常常用的，如果并发非常大，考虑多Master多Slave模式） 在192.168.99.121/122机器上各一个NameServer、Master进程。 实例以192.168.99.121为例： 修改/etc/hosts文件 确保相互之间可以ping通 解压并创建存储路径123tar -xvf alibaba-rocketmq-3.2.6.tar.gzmkdir -p alibaba-rocketmq/store/&#123;commitlog,consumequeue,index&#125; 配置文件 上面已经将实际中常用的配置项给出来了！ 修改日志配置文件 注意到logback.*.xml配置文件中： 可以使用sed进行替换： 1sed -i 's#$&#123;user.home&#125;#/software/alibaba-rocketmq#g' *.xml 修改启动脚本中的JVM参数 注意，在这里我将JVM的堆的初始化和最大大小统一设置为1G，并将新生代大小设置为512M。主要是考虑到我的虚拟机内存，实际上在线上是可以走默认的4G堆内存的。 第六步，启动NameServer1nohup sh mqnamesrv &amp; 启动broker-X 注意观察日志： RocketMQ Console把rocketmq-console.war部署到Tomcat下即可。 在解压WAR包后的CLASS下更改config.properties 这个管控台实际上还是比较简陋的，我们使用比较多的是mqadmin操作命令，后续会介绍。]]></content>
      <categories>
        <category>mq</category>
      </categories>
      <tags>
        <tag>mq</tag>
        <tag>rocketmq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Feign真正正确的使用方法]]></title>
    <url>%2Fjava%2Fjava%2Ffeign%2F1%2F</url>
    <content type="text"><![CDATA[Feign是spring cloud中服务消费端的调用框架,通常与ribbon,hystrix等组合使用。 但是在某些项目中，由于遗留原因，整个系统并不是spring cloud项目，甚至不是spring项目，而使用者关注的重点仅仅是简化http调用代码的编写。 如果采用httpclient或者okhttp这样相对较重的框架，对初学者来说编码量与学习曲线都会是一个挑战，而使用spring中RestTemplate，又没有配置化的解决方案，由此想到是否可以脱离spring cloud，独立使用Feign。 maven依赖12345&lt;dependency&gt; &lt;groupId&gt;com.netflix.feign&lt;/groupId&gt; &lt;artifactId&gt;feign-core&lt;/artifactId&gt; &lt;version&gt;8.18.0&lt;/version&gt;&lt;/dependency&gt; 自定义接口12345678import feign.Param;import feign.RequestLine;public interface RemoteService &#123; @RequestLine("GET /users/list?name=&#123;name&#125;") String getOwner(@Param(value = "name") String name);&#125; 通过@RequestLine指定HTTP协议及URL地址 配置类1234RemoteService service = Feign.builder() .options(new Options(1000, 3500)) .retryer(new Retryer.Default(5000, 5000, 3)) .target(RemoteService.class, "http://127.0.0.1:8085"); options方法指定连接超时时长及响应超时时长，retryer方法指定重试策略,target方法绑定接口与服务端地址。返回类型为绑定的接口类型。 调用1String result = service.getOwner("scott"); 与调用本地方法相同的方式调用feign包装的接口，直接获取远程服务提供的返回值。 附：服务生产者12345678910111213141516import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;import org.springframework.web.bind.annotation.RequestParam;import org.springframework.web.bind.annotation.ResponseBody;@Controller@RequestMapping(value="users")public class UserController &#123; @RequestMapping(value="/list",method=&#123;RequestMethod.GET,RequestMethod.POST,RequestMethod.PUT&#125;) @ResponseBody public String list(@RequestParam String name) throws InterruptedException&#123; return name.toUpperCase(); &#125;&#125; 更进一步在项目中，服务消费端与生产端之间交换的数据往往是一或多个对象，feign同样提供基于json的对象转换工具，方便我们直接以对象形式交互。 业务接口123456public interface RemoteService &#123; @Headers(&#123;"Content-Type: application/json","Accept: application/json"&#125;) @RequestLine("POST /users/list") User getOwner(User user);&#125; 加入@Headers注解，指定Content-Type为json 配置123456RemoteService service = Feign.builder() .encoder(new JacksonEncoder()) .decoder(new JacksonDecoder()) .options(new Options(1000, 3500)) .retryer(new Retryer.Default(5000, 5000, 3)) .target(RemoteService.class, "http://127.0.0.1:8085"); encoder指定对象编码方式，decoder指定对象解码方式。这里用的是基于Jackson的编、解码方式，需要在pom.xml中添加Jackson的依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.netflix.feign&lt;/groupId&gt; &lt;artifactId&gt;feign-jackson&lt;/artifactId&gt; &lt;version&gt;8.18.0&lt;/version&gt;&lt;/dependency&gt; 调用1User result = service.getOwner(u); 附：服务生产者12345678910111213@Controller@RequestMapping(value="users")public class UserController &#123; @RequestMapping(value="/list",method=&#123;RequestMethod.GET,RequestMethod.POST,RequestMethod.PUT&#125;) @ResponseBody public User list(@RequestBody User user) throws InterruptedException&#123; System.out.println(user.getUsername()); user.setId(100L); user.setUsername(user.getUsername().toUpperCase()); return user; &#125;&#125; 唯一的变化就是使用了@RequestBody来接收json格式的数据。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>feign</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入浅出UML类图（五）]]></title>
    <url>%2Fuml%2Fuml%2F5%2F</url>
    <content type="text"><![CDATA[实例分析3——售票机控制程序某运输公司决定为新的售票机开发车票销售的控制软件。图I给出了售票机的面板示意图以及相关的控制部件。 售票机相关部件的作用如下所述： 目的地键盘用来输入行程目的地的代码（例如，200表示总站）。 乘客可以通过车票键盘选择车票种类（单程票、多次往返票和座席种类）。 继续/取消键盘上的取消按钮用于取消购票过程，继续按钮允许乘客连续购买多张票。 显示屏显示所有的系统输出和用户提示信息。 插卡口接受MCard（现金卡），硬币口和纸币槽接受现金。 打印机用于输出车票。 所有部件均可实现自检并恢复到初始状态。 现采用面向对象方法开发该系统，使用UML进行建模，绘制该系统的初始类图。 参考解决方案参考类图如下： 类说明： 类 名 说 明 Component 抽象部件类，所有部件类的父类 Keyboard 抽象键盘类 ActionKeyboard 继续/取消键盘类 TicketKindKeyboard 车票种类键盘类 DestinationKeyboard 目的地键盘类 Screen 显示屏类 CardDriver 卡驱动器类 CashSlot 现金（硬币/纸币）槽类 Printer 打印机类 TicketSoldSystem 售票系统类 方法说明： 方法名 说 明 Component 的init()方法 初始化部件 Component 的doSeltTest()方法 自检 Keyboard的getSelectedKey()方法 获取按键值 ActionKeyboard的getAction()方法 继续/取消键盘事件处理 TicketKindKeyboard的getTicketKind()方法 车票种类键盘事件处理 DestinationKeyboard的getDestinationCode()方法 目的地键盘事件处理 Screen的showText()方法 显示信息 CardDriver的getCredit()方法 获取金额 CardDriver的debitFare()方法 更新卡余额 CardDriver的ejectMCard()方法 退卡 CashSlot的getCredit()方法 获取金额 Printer的printTicket()方法 打印车票 Printer的ejectTicket()方法 出票 TicketSoldSystem的verifyCredit()方法 验证金额 TicketSoldSystem的calculateFare()方法 计算费用]]></content>
      <categories>
        <category>uml</category>
      </categories>
      <tags>
        <tag>uml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入浅出UML类图（四）]]></title>
    <url>%2Fuml%2Fuml%2F4%2F</url>
    <content type="text"><![CDATA[实例分析1——登录模块某基于C/S的即时聊天系统登录模块功能描述如下： 用户通过登录界面(LoginForm)输入账号和密码，系统将输入的账号和密码与存储在数据库(User)表中的用户信息进行比较，验证用户输入是否正确，如果输入正确则进入主界面(MainForm)，否则提示“输入错误”。 根据以上描述绘制初始类图。 参考解决方案：参考类图如下： 考虑到系统扩展性，在本实例中引入了抽象数据访问接口IUserDAO，再将具体数据访问对象注入到业务逻辑对象中，可通过配置文件（如XML文件）等方式来实现，将具体的数据访问类类名存储在配置文件中，如果需要更换新的具体数据访问对象，只需修改配置文件即可，原有程序代码无须做任何修改。 类说明 类 名 说 明 LoginForm 登录窗口，省略界面组件和按钮事件处理方法（边界类） LoginBO 登录业务逻辑类，封装实现登录功能的业务逻辑（控制类） IUserDAO 抽象数据访问类接口，声明对User表的数据操作方法，省略除查询外的其他方法（实体类） UserDAO 具体数据访问类，实现对User表的数据操作方法，省略除查询外的其他方法（实体类） MainForm 主窗口（边界类） 方法说明 方法名 说 明 LoginForm类的LoginForm()方法 LoginForm构造函数，初始化实例成员 LoginForm类的validate()方法 界面类的验证方法，通过调用业务逻辑类LoginBO的validate()方法实现对用户输入信息的验证 LoginBO类的validate()方法 业务逻辑类的验证方法，通过调用数据访问类的findUserByAccAndPwd()方法验证用户输入信息的合法性 LoginBO类的setIUserDAO()方法 Setter方法，在业务逻辑对象中注入数据访问对象（注意：此处针对抽象数据访问类编程） IUserDAO接口的findUserByAccAndPwd()方法 业务方法声明，通过用户账号和密码在数据库中查询用户信息，判断该用户身份的合法性 UserDAO类的findUserByAccAndPwd()方法 业务方法实现，实现在IUserDAO接口中声明的数据访问方法 实例分析2——注册模块某基于Java语言的C/S软件需要提供注册功能，该功能简要描述如下： 用户通过注册界面(RegisterForm)输入个人信息，用户点击“注册”按钮后将输入的信息通过一个封装用户输入数据的对象(UserDTO)传递给操作数据库的数据访问类，为了提高系统的扩展性，针对不同的数据库可能需要提供不同的数据访问类，因此提供了数据访问类接口，如IUserDAO，每一个具体数据访问类都是某一个数据访问类接口的实现类，如OracleUserDAO就是一个专门用于访问Oracle数据库的数据访问类。 根据以上描述绘制类图。为了简化类图，个人信息仅包括账号(userAccount)和密码(userPassword)，且界面类无需涉及界面细节元素。 参考解决方案在以上功能说明中，可以分析出该系统包括三个类和一个接口，这三个类分别是注册界面类RegisterForm、用户数据传输类UserDTO、Oracle用户数据访问类OracleUserDAO，接口是抽象用户数据访问接口IUserDAO。它们之间的关系如下： 在RegisterForm中需要使用UserDTO类传输数据且需要使用数据访问类来操作数据库，因此RegisterForm与UserDTO和IUserDAO之间存在关联关系，在RegisterForm中可以直接实例化UserDTO，因此它们之间可以使用组合关联。 由于数据库类型需要灵活更换，因此在RegisterForm中不能直接实例化IUserDAO的子类，可以针对接口IUserDAO编程，再通过注入的方式传入一个IUserDAO接口的子类对象（在本书后续章节中将学习如何具体实现），因此RegisterForm和IUserDAO之间具有聚合关联关系。 OracleUserDAO是实现了IUserDAO接口的子类，因此它们之间具有类与接口的实现关系。 在声明IUserDAO接口的增加用户信息方法addUser()时，需要将在界面类中实例化的UserDTO对象作为参数传递进来，然后取出封装在UserDTO对象中的数据插入数据库，因此addUser()方法的函数原型可以定义为：public boolean addUser(UserDTO user)，在IUserDAO的方法addUser()中将UserDTO类型的对象作为参数，故IUserDAO与UserDTO存在依赖关系。 通过以上分析，该实例参考类图如图1所示： 注意：在绘制类图或其他UML图形时，可以通过注释(Comment)来对图中的符号或元素进行一些附加说明，如果需要详细说明类图中的某一方法的功能或者实现过程，可以使用如图2所示表示方式：]]></content>
      <categories>
        <category>uml</category>
      </categories>
      <tags>
        <tag>uml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入浅出UML类图（三）]]></title>
    <url>%2Fuml%2Fuml%2F3%2F</url>
    <content type="text"><![CDATA[类与类之间的关系（2）依赖关系依赖(Dependency)关系是一种使用关系，特定事物的改变有可能会影响到使用该事物的其他事物，在需要表示一个事物使用另一个事物时使用依赖关系。大多数情况下，依赖关系体现在某个类的方法使用另一个类的对象作为参数。在UML中，依赖关系用带箭头的虚线表示，由依赖的一方指向被依赖的一方。例如：驾驶员开车，在Driver类的drive()方法中将Car类型的对象car作为一个参数传递，以便在drive()方法中能够调用car的move()方法，且驾驶员的drive()方法依赖车的move()方法，因此类Driver依赖类Car，如图1所示： 在系统实施阶段，依赖关系通常通过三种方式来实现，第一种也是最常用的一种方式是如图1所示的将一个类的对象作为另一个类中方法的参数，第二种方式是在一个类的方法中将另一个类的对象作为其局部变量，第三种方式是在一个类的方法中调用另一个类的静态方法。图1对应的Java代码片段如下： 12345678910111213public class Driver &#123; public void drive(Car car) &#123; car.move(); &#125; …… &#125; public class Car &#123; public void move() &#123; ...... &#125; …… &#125; 泛化关系泛化(Generalization)关系也就是继承关系，用于描述父类与子类之间的关系，父类又称作基类或超类，子类又称作派生类。在UML中，泛化关系用带空心三角形的直线来表示。在代码实现时，我们使用面向对象的继承机制来实现泛化关系，如在Java语言中使用extends关键字、在C++/C#中使用冒号“：”来实现。例如：Student类和Teacher类都是Person类的子类，Student类和Teacher类继承了Person类的属性和方法，Person类的属性包含姓名(name)和年龄(age)，每一个Student和Teacher也都具有这两个属性，另外Student类增加了属性学号(studentNo)，Teacher类增加了属性教师编号(teacherNo)，Person类的方法包括行走move()和说话say()，Student类和Teacher类继承了这两个方法，而且Student类还新增方法study()，Teacher类还新增方法teach()。如图2所示： 图2对应的Java代码片段如下： 12345678910111213141516171819202122232425262728293031//父类 public class Person &#123; protected String name; protected int age; public void move() &#123; …… &#125; public void say() &#123; …… &#125; &#125; //子类 public class Student extends Person &#123; private String studentNo; public void study() &#123; …… &#125; &#125; //子类 public class Teacher extends Person &#123; private String teacherNo; public void teach() &#123; …… &#125; &#125; 接口与实现关系在很多面向对象语言中都引入了接口的概念，如Java、C#等，在接口中，通常没有属性，而且所有的操作都是抽象的，只有操作的声明，没有操作的实现。UML中用与类的表示法类似的方式表示接口，如图3所示： 接口之间也可以有与类之间关系类似的继承关系和依赖关系，但是接口和类之间还存在一种实现(Realization)关系，在这种关系中，类实现了接口，类中的操作实现了接口中所声明的操作。在UML中，类与接口之间的实现关系用带空心三角形的虚线来表示。例如：定义了一个交通工具接口Vehicle，包含一个抽象操作move()，在类Ship和类Car中都实现了该move()操作，不过具体的实现细节将会不一样，如图4所示： 实现关系在编程实现时，不同的面向对象语言也提供了不同的语法，如在Java语言中使用implements关键字，而在C++/C#中使用冒号“：”来实现。图4对应的Java代码片段如下： 123456789101112131415public interface Vehicle &#123; public void move(); &#125; public class Ship implements Vehicle &#123; public void move() &#123; …… &#125; &#125; public class Car implements Vehicle &#123; public void move() &#123; …… &#125; &#125;]]></content>
      <categories>
        <category>uml</category>
      </categories>
      <tags>
        <tag>uml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入浅出UML类图（二）]]></title>
    <url>%2Fuml%2Fuml%2F2%2F</url>
    <content type="text"><![CDATA[类与类之间的关系（1）在软件系统中，类并不是孤立存在的，类与类之间存在各种关系，对于不同类型的关系，UML提供了不同的表示方式。 关联关系关联(Association)关系是类与类之间最常用的一种关系，它是一种结构化关系，用于表示一类对象与另一类对象之间有联系，如汽车和轮胎、师傅和徒弟、班级和学生等等。在UML类图中，用实线连接有关联关系的对象所对应的类，在使用Java、C#和C++等编程语言实现关联关系时，通常将一个类的对象作为另一个类的成员变量。在使用类图表示关联关系时可以在关联线上标注角色名，一般使用一个表示两者之间关系的动词或者名词表示角色名（有时该名词为实例对象名），关系的两端代表两种不同的角色，因此在一个关联关系中可以包含两个角色名，角色名不是必须的，可以根据需要增加，其目的是使类之间的关系更加明确。 如在一个登录界面类LoginForm中包含一个JButton类型的注册按钮loginButton，它们之间可以表示为关联关系，代码实现时可以在LoginForm中定义一个名为loginButton的属性对象，其类型为JButton。如图1所示： 图1对应的Java代码片段如下： 12345678public class LoginForm &#123; private JButton loginButton; //定义为成员变量 …… &#125; public class JButton &#123; …… &#125; 在UML中，关联关系通常又包含如下几种形式： 双向关联默认情况下，关联是双向的。例如：顾客(Customer)购买商品(Product)并拥有商品，反之，卖出的商品总有某个顾客与之相关联。因此，Customer类和Product类之间具有双向关联关系，如图2所示： 图2对应的Java代码片段如下： 123456789public class Customer &#123; private Product[] products; …… &#125; public class Product &#123; private Customer customer; …… &#125; 单向关联类的关联关系也可以是单向的，单向关联用带箭头的实线表示。例如：顾客(Customer)拥有地址(Address)，则Customer类与Address类具有单向关联关系，如图3所示： 图3对应的Java代码片段如下： 12345678public class Customer &#123; private Address address; …… &#125; public class Address &#123; …… &#125; 自关联在系统中可能会存在一些类的属性对象类型为该类本身，这种特殊的关联关系称为自关联。例如：一个节点类(Node)的成员又是节点Node类型的对象，如图4所示： 图4对应的Java代码片段如下： 1234public class Node &#123; private Node subNode; …… &#125; 多重性关联多重性关联关系又称为重数性(Multiplicity)关联关系，表示两个关联对象在数量上的对应关系。在UML中，对象之间的多重性可以直接在关联直线上用一个数字或一个数字范围表示。 对象之间可以存在多种多重性关联关系，常见的多重性表示方式如表1所示： 表示方式 多重性说明 1..1 表示另一个类的一个对象只与该类的一个对象有关系 0..* 表示另一个类的一个对象与该类的零个或多个对象有关系 1..* 表示另一个类的一个对象与该类的一个或多个对象有关系 0..1 表示另一个类的一个对象没有或只与该类的一个对象有关系 m..n 表示另一个类的一个对象与该类最少m，最多n个对象有关系 (m≤n) 例如：一个界面(Form)可以拥有零个或多个按钮(Button)，但是一个按钮只能属于一个界面，因此，一个Form类的对象可以与零个或多个Button类的对象相关联，但一个Button类的对象只能与一个Form类的对象关联，如图5所示： 图5对应的Java代码片段如下： 12345678public class Form &#123; private Button[] buttons; //定义一个集合对象 …… &#125; public class Button &#123; …… &#125; 聚合关系聚合(Aggregation)关系表示整体与部分的关系。在聚合关系中，成员对象是整体对象的一部分，但是成员对象可以脱离整体对象独立存在。在UML中，聚合关系用带空心菱形的直线表示。例如：汽车发动机(Engine)是汽车(Car)的组成部分，但是汽车发动机可以独立存在，因此，汽车和发动机是聚合关系，如图6所示： 在代码实现聚合关系时，成员对象通常作为构造方法、Setter方法或业务方法的参数注入到整体对象中，图6对应的Java代码片段如下： 123456789101112131415161718public class Car &#123; private Engine engine; //构造注入 public Car(Engine engine) &#123; this.engine = engine; &#125; //设值注入 public void setEngine(Engine engine) &#123; this.engine = engine; &#125; …… &#125; public class Engine &#123; …… &#125; 组合关系组合(Composition)关系也表示类之间整体和部分的关系，但是在组合关系中整体对象可以控制成员对象的生命周期，一旦整体对象不存在，成员对象也将不存在，成员对象与整体对象之间具有同生共死的关系。在UML中，组合关系用带实心菱形的直线表示。例如：人的头(Head)与嘴巴(Mouth)，嘴巴是头的组成部分之一，而且如果头没了，嘴巴也就没了，因此头和嘴巴是组合关系，如图7所示： 在代码实现组合关系时，通常在整体类的构造方法中直接实例化成员类，图7对应的Java代码片段如下： 123456789101112public class Head &#123; private Mouth mouth; public Head() &#123; mouth = new Mouth(); //实例化成员类 &#125; …… &#125; public class Mouth &#123; …… &#125;]]></content>
      <categories>
        <category>uml</category>
      </categories>
      <tags>
        <tag>uml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入浅出UML类图（一）]]></title>
    <url>%2Fuml%2Fuml%2F1%2F</url>
    <content type="text"><![CDATA[在UML 2.0的13种图形中，类图是使用频率最高的UML图之一。Martin Fowler在其著作《UML Distilled: A Brief Guide to the Standard Object Modeling Language, Third Edition》（《UML精粹：标准对象建模语言简明指南（第3版）》）中有这么一段：“If someone were to come up to you in a dark alley and say, ‘Psst, wanna see a UML diagram?’ that diagram would probably be a class diagram. The majority of UML diagrams I see are class diagrams.”（“如果有人在黑暗的小巷中向你走来并对你说：‘嘿，想不想看一张UML图？’那么这张图很有可能就是一张类图，我所见过的大部分的UML图都是类图”），由此可见类图的重要性。 类图用于描述系统中所包含的类以及它们之间的相互关系，帮助人们简化对系统的理解，它是系统分析和设计阶段的重要产物，也是系统编码和测试的重要模型依据。 类类(Class)封装了数据和行为，是面向对象的重要组成部分，它是具有相同属性、操作、关系的对象集合的总称。在系统中，每个类都具有一定的职责，职责指的是类要完成什么样的功能，要承担什么样的义务。一个类可以有多种职责，设计得好的类一般只有一种职责。在定义类的时候，将类的职责分解成为类的属性和操作（即方法）。类的属性即类的数据职责，类的操作即类的行为职责。设计类是面向对象设计中最重要的组成部分，也是最复杂和最耗时的部分。 在软件系统运行时，类将被实例化成对象(Object)，对象对应于某个具体的事物，是类的实例(Instance)。 类图(Class Diagram)使用出现在系统中的不同类来描述系统的静态结构，它用来描述不同的类以及它们之间的关系。 在系统分析与设计阶段，类通常可以分为三种，分别是实体类(Entity Class)、控制类(Control Class)和边界类(Boundary Class)，下面对这三种类加以简要说明： 实体类：实体类对应系统需求中的每个实体，它们通常需要保存在永久存储体中，一般使用数据库表或文件来记录，实体类既包括存储和传递数据的类，还包括操作数据的类。实体类来源于需求说明中的名词，如学生、商品等。 控制类：控制类用于体现应用程序的执行逻辑，提供相应的业务操作，将控制类抽象出来可以降低界面和数据库之间的耦合度。控制类一般是由动宾结构的短语（动词+名词）转化来的名词，如增加商品对应有一个商品增加类，注册对应有一个用户注册类等。 边界类：边界类用于对外部用户与系统之间的交互对象进行抽象，主要包括界面类，如对话框、窗口、菜单等。 在面向对象分析和设计的初级阶段，通常首先识别出实体类，绘制初始类图，此时的类图也可称为领域模型，包括实体类及其它们之间的相互关系。 类的UML图示在UML中，类使用包含类名、属性和操作且带有分隔线的长方形来表示，如定义一个Employee类，它包含属性name、age和email，以及操作modifyInfo()，在UML类图中该类如图1所示： 图1对应的Java代码片段如下： 123456789public class Employee &#123; private String name; private int age; private String email; public void modifyInfo() &#123; ...... &#125; &#125; 在UML类图中，类一般由三部分组成： 第一部分是类名：每个类都必须有一个名字，类名是一个字符串。 第二部分是类的属性(Attributes)：属性是指类的性质，即类的成员变量。一个类可以有任意多个属性，也可以没有属性。 UML规定属性的表示方式为：(可见性 名称:类型 [ = 缺省值 ])其中： 可见性”表示该属性对于类外的元素而言是否可见，包括公有(public)、私有(private)和受保护(protected)三种，在类图中分别用符号+、-和#表示。 “名称”表示属性名，用一个字符串表示。 “类型”表示属性的数据类型，可以是基本数据类型，也可以是用户自定义类型。 “缺省值”是一个可选项，即属性的初始值。 第三部分是类的操作(Operations)：操作是类的任意一个实例对象都可以使用的行为，是类的成员方法。 UML规定操作的表示方式为：(可见性 名称(参数列表) [ : 返回类型])其中： “可见性”的定义与属性的可见性定义相同。 “名称”即方法名，用一个字符串表示。 “参数列表”表示方法的参数，其语法与属性的定义相似，参数个数是任意的，多个参数之间用逗号“，”隔开。 “返回类型”是一个可选项，表示方法的返回值类型，依赖于具体的编程语言，可以是基本数据类型，也可以是用户自定义类型，还可以是空类型(void)，如果是构造方法，则无返回类型。 在类图2中，操作method1的可见性为public(+)，带入了一个Object类型的参数par，返回值为空(void)；操作method2的可见性为protected(#)，无参数，返回值为String类型；操作method3的可见性为private(-)，包含两个参数，其中一个参数为int类型，另一个为int[]类型，返回值为int类型。 由于在Java语言中允许出现内部类，因此可能会出现包含四个部分的类图，如图3所示：]]></content>
      <categories>
        <category>uml</category>
      </categories>
      <tags>
        <tag>uml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[说说分布式事务(五)]]></title>
    <url>%2Fjava%2Fjava%2Ftransaction%2F5%2F</url>
    <content type="text"><![CDATA[业务回补场景业务对资金进行操作 简化流程整个资金平台会和支付宝进行交互(冻结金额,出账金额),对这两个动作支付宝都会返回成功或者失败,当然还有异常流接口超时(实际成功/实际失败). 正常流的业务,我们都可以根据实际的返回进行自己业务逻辑的处理,但是异常流对于调用方其实不知道实际结果,这个时候就需要进行业务数据回补,丰富一下调用时序图 对资金进行操作以后,如果最终是成功的话,都会发送相应的成功消息,业务可以根据实际情况接受消息进行处理,对应的流程图为 自身业务,需要监听调用方的业务消息,因为会出现接口返回失败(比如说接口超时),但是实际成功的场景,通过监听成功消息进行流程回溯 问题 涉及异步更新的操作,都会存在短暂的状态不一致的情况,当数据处于中间状态,可能会出现业务重复提交的情况,这个就需要业务上规避类似的问题(比如对于资金会加入审核流程)]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>transaction</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[说说分布式事务(四)]]></title>
    <url>%2Fjava%2Fjava%2Ftransaction%2F4%2F</url>
    <content type="text"><![CDATA[最终一致性(二)基于MQ的分布式事务补偿机制序列图 异常场景处理 预创建订单失败:如果实际预创建订单成功,订单定时补偿机制,定时删除这部分订单,不影响数据一致性,下单失败 预扣减库存失败:如果预扣减库存真实失败,则下单失败(订单由定时补偿机制定时删除,其它应用参照场景4的处理方式,下单失败;如果实际预扣减库存成功,参照场景4的处理方式,下单失败 实际创建订单失败:如果创建订单真实失败(不需要发送下单失败消息,防止实际创建订单成功场景),订单的预处理数据通过订单的定时补偿机制尝试删除(需要考虑事务处理时间,将超过某个时间范围该事务还处于预处理状态的订单删除),下单失败;如果实际创建订单成功,其它应用参照场景4的处理方式,下单成功(提示用户下单失败) 发送订单创建成功消息失败/库存服务由于各种原因没有接到下单成功消息:库存服务定时轮询处理数据(需要考虑事务处理时间,将超过某个时间范围该事务还处于预处理状态的订单筛选出来),询问订单服务改订单Id对应的订单是否创建成功,根据订单创建成功与否选取相应的事务补偿机制 和TCC的比较 TCC是把所有的订单创建步骤平等看待,只要有一个失败,整个下单流程全部失败(比较TCC里面的confirm失败和基于MQ实际创建订单失败的补偿难易程度) TCC是通过发消息给TCC服务器,然后由TCC服务调用应用服务;基于MQ的分布式事务补偿机制,是通过将消息发送到MQ,然后由应用自己去监听MQ的事件]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>transaction</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[说说分布式事务(三)]]></title>
    <url>%2Fjava%2Fjava%2Ftransaction%2F3%2F</url>
    <content type="text"><![CDATA[最终一致性(一)TCC简介TCC是由支付宝架构师提供的一种柔性解决分布式事务解决方案,主要包括三个步骤: TCC流程TCC的关键流程如下图(以下单和扣减库存为例子) Q: 预生成订单失败了,为什么要通过TCC执行预处理数据回滚?A: 可能预生成订单成功,但是接口返回失败(超时失败),所以预处理在某些情况下是有预处理数据,需要清理 TCC异常场景在整个流程,我们主要需要关注的是cancel失败和confirm失败引起的数据不一致现象 注意事项 TCC服务支持接口失败重试,所以对TCC暴露的接口都需要满足幂等性(根据事务Id很好满足) 基于TCC的中心化事务一致性解决方法,各个应用服务器如果需要感知某次事务是否成功的成本很高,所以对于自身而言进行事务补偿成本就会很高.举个例子: 后记 是否一定需要TCC服务器? 不一定,可以让交易链路来充当TCC服务器的角色,但是长期来看,TCC相当于是一个公用的组件,所以其它地方也需要TCC分布式事务,可以公用这一个组件(交易链路可以完成TCC所能完成的一切操作,把TCC单独部署一个服务,仅仅是考虑整个系统的抽象结构和功能复用) 这里说的预处理,指的是什么? 在整个分布式事务中预处理的含义其实很广泛,比如订单,所谓的预处理就是生成订单,但是用户真实是看不到这些订单的,至于具体实现是在一张新表中记录还是在原有的订单表是加上标记位,具体实现方式由自己统筹考虑(当然还需要考虑记录事务Id);像减库存这种预处理,可以直接减少原始库存,再通过另外一张表来记录这次事务Id操作了哪个Sku的库存数量,当然也可以不减少库存只记录操作,但是这种方式在计算实际库存的时候复杂度会提高(需要减掉预处理的那部分)]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>transaction</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[说说分布式事务(二)]]></title>
    <url>%2Fjava%2Fjava%2Ftransaction%2F2%2F</url>
    <content type="text"><![CDATA[3PC 以两阶段提交来说，主持人收到一个提案请求，打电话跟每个组员询问是否通过并统计回复，然后将最后决定打电话通知各组员。要是主持人在跟第一位组员通完电话后失忆，而第一位组员在得知结果并执行后老人痴呆，那么即使重新选出主持人，也没人知道最后的提案决定是什么，也许是通过，也许是驳回，不管大家选择哪一种决定，都有可能与第一位组员已执行过的真实决定不一致，老板就会不开心认为决策小组沟通有问题而解雇。三阶段提交即是引入了另一个步骤，主持人打电话跟组员通知请准备通过提案，以避免没人知道真实决定而造成决定不一致的失业危机。为什么能够解决二阶段提交的问题呢？回到刚刚提到的状况，在主持人通知完第一位组员请准备通过后两人意外失忆，即使没人知道全体在第一阶段的决定为何，全体决策组员仍可以重新协调过程或直接否决，不会有不一致决定而失业。那么当主持人通知完全体组员请准备通过并得到大家的再次确定后进入第三阶段，当主持人通知第一位组员请通过提案后两人意外失忆，这时候其他组员再重新选出主持人后，仍可以知道目前至少是处于准备通过提案阶段，表示第一阶段大家都已经决定要通过了，此时便可以直接通过 以上资料来自wiki百科,说明在2PC过程中,在第二个阶段当协调者通知第一个客户端A,并且第一个客户端刚好执行完毕以后,这两台机器都Down掉了,而恰好这N-1台机器投的都是Yes票(都处于不确定的状态),这个时候整个事务就会被Block,暂时称之为聋哑事件 客户端A投的是Abort票,那么由于协调者和客户端A都Down掉,那么整个事务应该是abort 客户端A投的是commit票,并且协调者决定commit,那么整个事务应该是commit 客户端A投的是commit票,并且协调者由于自身的原因决定abort,那么整个事务应该是abort 在3PC中引入了一个预提交的状态 当在第二阶段出现聋哑事件,那么这N-1台机器可以根据超时机制直接abort掉,因为客户端A如果提交了事务,只是预提交,当该机器重启以后只要询问周边机器事务状态,简单的将事务回滚或者提交事务,就能保持事务的最终一致性 当进行到第三阶段的时候,如果发生聋哑事件,那么其它处于「不确定状态」的客户端会直接执行commit,而不会像2PC一样导致事务block,但是这样会有一个风险(进入到第三个阶段说明客户端在第一阶段投的都是Yes),因为在聋哑事件中,那台Down掉的机器在第二阶段中给协调者发送的不是prepared,这个时候协调者收到消息给客户端发送的是abort命令.所以3PC只是乐观的认为只要你第一阶段大家投的都是Yes,那么最后成功提交的几率很大]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>transaction</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[说说分布式事务(一)]]></title>
    <url>%2Fjava%2Fjava%2Ftransaction%2F1%2F</url>
    <content type="text"><![CDATA[2PC(两阶段事务提交)两阶段事务提交简化图 两阶段事务提交异常点 节点本身故障(比如Down机) 节点之间通信故障 两阶段事务提交错误点分析 说明 图中有问号的条目,是我不确定的地方,但是不影响这个分布式事务的结果 图中的感叹号条目,个人感觉其实也是允许先发消息再记录日志的,但是如果这样子做以后发生Down机,客户端或者TM都需要向其它机器询问结果才能得到结论(而这样子做的话会大大加长分布事务的阻塞时间和事务处理的复杂度,同时这样做会有一个致命的缺陷,抹除了一部分可以自恢复场景。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>transaction</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sonarqube和mvn整合]]></title>
    <url>%2Fjava%2Fjava%2Fsonarqube%2F1%2F</url>
    <content type="text"><![CDATA[SonarQube(Sonar)是一个用于管理代码质量的开源平台。SonarQube目前已支持超过20种主流编程语言，它管理的代码质量主要涉及7个维度:架构与设计、重复、单元测试、复杂度、潜在的bug、代码标准、注释。 本文，笔者将围绕搭建SonarQube这样的代码质量管理平台这个主题展开，结合java代码实例一步步讲述具体的过程，其中涉及Sonar的下载安装、创建对应Mysql数据库以及运行和管理，并对实践过程中出现的一些问题进行了分析和解决。 注：本文中所有的实践都是在Docker虚拟机下进行，但目测同样适用于各个平台。 安装postgres数据库 12345678910postgres: image: registry.cn-shenzhen.aliyuncs.com/zhouqi/postgres:1.0 environment: - POSTGRES_USER=root - POSTGRES_PASSWORD=000000 ports: - "5432:5432" container_name: postgres 安装sonarqube 1234567891011121314sonarqube: image: registry.cn-shenzhen.aliyuncs.com/zhouqi/sonarqube:2.0 depends_on: - postgres environment: - SONARQUBE_JDBC_USERNAME=root - SONARQUBE_JDBC_PASSWORD=000000 - SONARQUBE_JDBC_URL=jdbc:postgresql://192.168.137.60/sonar ports: - "9000:9000" - "9092:9092" container_name: sonarqube 配置maven 12345678910111213141516171819&lt;settings&gt; &lt;pluginGroups&gt; &lt;pluginGroup&gt;org.sonarsource.scanner.maven&lt;/pluginGroup&gt; &lt;/pluginGroups&gt; &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;sonar&lt;/id&gt; &lt;activation&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;/activation&gt; &lt;properties&gt; &lt;!-- Optional URL to server. Default value is http://localhost:9000 --&gt; &lt;sonar.host.url&gt; http://myserver:9000 &lt;/sonar.host.url&gt; &lt;/properties&gt; &lt;/profile&gt; &lt;/profiles&gt;&lt;/settings&gt; idea运行 12mvn installmvn sonar:sonar]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>sonar</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7升级后无法重启或关机解决办法]]></title>
    <url>%2Flinux%2Flinux%2Fcentos-1%2F</url>
    <content type="text"><![CDATA[由于CentOS7新发布的yum源升级包中systemd关机流程判断条件发生了变化，可能会导致centos7升级后，服务器重启时卡死。新发布的systemd进程的判断更加严格，如果某些进程不响应SIGTERM信号，可能会导致重启是挂死。该问题和业务进程对SIGTERM信号的处理有关。 执行yum update systemd（或者yum update）将systemd系列软件包更新到219-19.el7版本之后，reboot会出现如下卡机界面导致系统挂住，无法重启： 现象： 查询版本包： 解决办法： 新建/etc/systemd/system/rc-local.service并写入： 12345678910[Unit]Description=/etc/rc.d/rc.local CompatibilityConditionFileIsExecutable=/etc/rc.d/rc.localAfter=network.target[Service]Type=forkingExecStart=/etc/rc.d/rc.local startTimeoutSec=5RemainAfterExit=yes 备份/etc/systemd/system.conf1cp -a /etc/systemd/system.conf /etc/systemd/system.conf_bak 修改文件1sed -i 's/#DefaultTimeoutStopSec=90s/DefaultTimeoutStopSec=30s/g' /etc/systemd/system.conf 重新加载1systemctl daemon-reload]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>centos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[适配器模式之对象适配器]]></title>
    <url>%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2Fdesign%2Fpattern-1%2F</url>
    <content type="text"><![CDATA[问题导入：比如有A型螺母和B型螺母，那么用户可以再A型螺母上直接使用按着A型螺母生产的A型螺丝，同样也可以在B型螺母上直接使用按着B型螺母标准生产的B型螺丝。但是由于A型螺母和B型螺母的标准不一样，用户在A型螺母上不能直接使用B型的螺丝，反之也一样。该如何达到这个目的呢？ 使用适配器就可以解决这个问题：生产一种“A型螺母适配器”，这种A型螺母适配器的前端符合A型螺母标准要求，可以拧在A型螺母上，后端又焊接了一个B型螺母。这样用户就可以借助A型螺母适配器在A型螺母上使用B型的螺丝了。 适配器模式又称为包装器，是用来将一个类的接口转换成客户希望的另外一个接口。这可以使得原本由于接口不兼容而不能一起工作的那些类可以一起工作。适配器模式的关键是建立一个适配器，这个适配器实现了目标接口并且包含了被适配者的引用。 适配器模式的三种角色： 目标：目标是一个接口，该接口是客户想要使用的接口。 被适配者：被适配者是一个已经存在的接口或抽象类，这个接口接口或者抽象类需要适配。 适配器：适配器是一个类，该类实现了目标接口并且包含有被适配者的引用，即适配器的职责是对适配者接口或抽象类与目标接口进行适配。 以下通过一个简单的问题来描述适配器模式中所涉及的各个角色。 实例用户已经有一个两厢的插座，但是最近用户又有了一个新的三厢插座。用户现有一台洗衣机和一台电视机，洗衣机是三厢插头，而电视机是两厢插头。现在用户想用心的三厢插座来使用洗衣机和电视机，即用心的三厢插座为洗衣机和电视机接通电流。 针对以上问题，使用适配器模式设计若干个类。 目标 本问题是使用三厢插座来为电视机和洗衣机接通电流，所以目标是三厢插座。把三厢插座设置为一个接口： 123456package com.adatpe;//适配目标：三相插座public interface ThreeElectricOutlet &#123; void connectElectricCurrent();&#125; 被适配者 对于本问题，用户是想要用三厢插座为两厢插头的电视机接通电流，所以被适配者应该是两厢插座，也设置为一个接口： 123456package com.adatpe;//被适配者：两相插座public interface TwoElectricOutlet &#123; void connectElectricCurrent();&#125; 适配器 该适配器实现了目标接口三厢插座ThreeElectricOutlet，同时又包含了两厢插座TwoElectricOutlet的引用： 1234567891011121314package com.adatpe;//适配器：实现目标接口public class ThreeElectricAdapter implements ThreeElectricOutlet &#123; //适配器包含被适配者的引用 private TwoElectricOutlet outlet; public ThreeElectricAdapter(TwoElectricOutlet outlet) &#123; this.outlet = outlet; &#125; public void connectElectricCurrent() &#123; outlet.connectElectricCurrent(); &#125;&#125; 下列应用程序中，Application.java使用了适配器模式中所涉及的类，应用程序负责用Wash类创建一个对象来模拟一台洗衣机，使用TV类创建一个对象来模拟一台电视机 使用ThreeElectricOutlet接口变量调用Wash对象的connectElectricCurrent()方法，并借助适配器调用TV对象的connectElectricCurrent()方法，即用三厢插座分别为洗衣机和电视机接通电流。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package com.adatpe; public class Application &#123; public static void main(String[] args) &#123; ThreeElectricOutlet outlet; //目标接口（三相插座） Wash wash = new Wash(); //洗衣机 outlet = wash; //洗衣机插在三相插座上 System.out.println("使用三相插座接通电流"); outlet.connectElectricCurrent(); //接通电流开始洗衣服 TV tv = new TV(); //电视机 ThreeElectricAdapter adapter = new ThreeElectricAdapter(tv); //把电视插在适配器上面 outlet = adapter; //再把适配器插在三厢插座上 System.out.println("使用三厢插座接通电流"); outlet.connectElectricCurrent(); //接通电流，开始播放电视节目 &#125;&#125;//洗衣机使用三相插座class Wash implements ThreeElectricOutlet&#123; private String name; public Wash() &#123; name = "黄河洗衣机"; &#125; public Wash(String name)&#123; this.name = name; &#125; public void connectElectricCurrent() &#123; turnOn(); &#125; public void turnOn()&#123; System.out.println(name+"开始洗衣服了"); &#125;&#125;//电视机使用两厢插座class TV implements TwoElectricOutlet&#123; private String name; public TV() &#123; name = "长江电视机"; &#125; public TV(String name)&#123; this.name = name; &#125; public void connectElectricCurrent() &#123; turnOn(); &#125; public void turnOn()&#123; System.out.println(name+"开始播放电视节目"); &#125;&#125; 运行结果为： 使用三相插座接通电流黄河洗衣机开始洗衣服了使用三厢插座接通电流长江电视机开始播放电视节目 双向适配器在适配器模式中，如果Adapter角色同时实现目标接口和被适配者接口，并包含目标接口和被适配接口的引用，那么该适配器就是一个双向适配器。使用双向适配器，用户既可以用新的接口又可以用已有的接口。在以上例子中，如果用户希望能有三厢插座来接通洗衣机和电视机的电流，有可以用两厢插座来接通洗衣机和电视机的电流，那么就必须使用一个双向适配器。具体代码如下： 12345678910111213141516171819202122232425262728293031323334353637package com.adatpe;public class ThreeAndTwoElectricAdapter implements ThreeElectricOutlet, TwoElectricOutlet &#123; private ThreeElectricOutlet threeElectricOutlet; private TwoElectricOutlet twoElectricOutlet; public ThreeAndTwoElectricAdapter(ThreeElectricOutlet threeOutlet,TwoElectricOutlet twoOutlet) &#123; threeElectricOutlet = threeOutlet; twoElectricOutlet = twoOutlet; &#125; public ThreeAndTwoElectricAdapter(TwoElectricOutlet twoOutlet,ThreeElectricOutlet threeOutlet)&#123; threeElectricOutlet = threeOutlet; twoElectricOutlet = twoOutlet; &#125; public void connectElectricCurrent() &#123; if(this instanceof ThreeElectricOutlet)&#123; twoElectricOutlet.connectElectricCurrent();//twoElectricOutlet是被适配的接口 &#125; if(this instanceof TwoElectricOutlet)&#123; threeElectricOutlet.connectElectricCurrent(); //threeElectricOutlet是被适配的接口 &#125; &#125; public static void main(String[] args) &#123; ThreeElectricOutlet threeOutlet; TwoElectricOutlet twOutlet; Wash wash = new Wash(); TV tv = new TV(); ThreeAndTwoElectricAdapter adapter = new ThreeAndTwoElectricAdapter(wash,tv); threeOutlet = adapter; System.out.println("使用三厢插座接通电源"); threeOutlet.connectElectricCurrent(); twOutlet = adapter; System.out.println("使用两厢插座接通电源"); twOutlet.connectElectricCurrent(); &#125;&#125; 运行结果为： 使用三厢插座接通电源长江电视机开始播放电视节目黄河洗衣机开始洗衣服了使用两厢插座接通电源长江电视机开始播放电视节目黄河洗衣机开始洗衣服了 这样就实现了即可以用三厢插座又可以用两厢插座来为电视机和洗衣机接通电流了。 优点 目标和被适配者是完全解耦的关系。 适配器模式满足“开–闭原则”，当添加一个实现了Adapter接口的新类时，不必修改Adapter，Adapter就能对这个新类的实例进行适配。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
        <tag>适配器模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于MapStruct转换list的问题]]></title>
    <url>%2Fjava%2Fjava%2Fmapstruct%2F2%2F</url>
    <content type="text"><![CDATA[mapstruct在转换list之前必须有一个前置转换，即他们的实体之间的转换 错误的转换方式：1List&lt;EggVo&gt; listpoTovo(List&lt;Egg&gt; po); 正确的转换方式： 1234@Mapping(source = "id", target = "lid")EggVo poTovo(Egg po);List&lt;EggVo&gt; listpoTovo(List&lt;Egg&gt; po); 添加了实体转换之后，就可以正常的进行list转换了，同理把属性映射直接加在list转换上也是不行的，要加在实体转换上，然后list的转换也会继承这和属性的映射。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>mapstruct</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lombok介绍]]></title>
    <url>%2Fjava%2Fjava%2Flombok%2F</url>
    <content type="text"><![CDATA[背景我们在开发过程中，通常都会定义大量的JavaBean，然后通过IDE去生成其属性的构造器、getter、setter、equals、hashcode、toString方法，当要对某个属性进行改变时，比如命名、类型等，都需要重新去生成上面提到的这些方法，那Java中有没有一种方式能够避免这种重复的劳动呢？答案是有，我们来看一下下面这张图，右面是一个简单的JavaBean，只定义了两个属性，在类上加上了@Data，从左面的结构图上可以看到，已经自动生成了上面提到的方法。 Lombok简介ombok是一个可以通过简单的注解形式来帮助我们简化消除一些必须有但显得很臃肿的Java代码的工具，通过使用对应的注解，可以在编译源码的时候生成对应的方法。 官方地址：https://projectlombok.org/ github地址：https://github.com/rzwitserloot/lombok Lombok使用注解介绍 @Getter / @Setter 可以作用在类上和属性上，放在类上，会对所有的非静态(non-static)属性生成Getter/Setter方法，放在属性上，会对该属性生成Getter/Setter方法。并可以指定Getter/Setter方法的访问级别。 @EqualsAndHashCode 默认情况下，会使用所有非瞬态(non-transient)和非静态(non-static)字段来生成equals和hascode方法，也可以指定具体使用哪些属性。 @ToString 生成toString方法，默认情况下，会输出类名、所有属性，属性会按照顺序输出，以逗号分割。 @NoArgsConstructor, @RequiredArgsConstructor and @AllArgsConstructor 无参构造器、部分参数构造器、全参构造器，当我们需要重载多个构造器的时候，Lombok就无能为力了。 @Data @ToString, @EqualsAndHashCode, 所有属性的@Getter, 所有non-final属性的@Setter和@RequiredArgsConstructor的组合，通常情况下，我们使用这个注解就足够了。 Lombok原理了解了简单的使用之后，现在应该比较好奇它是如何实现的。整个使用的过程中，只需要使用注解而已，不需要做其它额外的工作，那玄妙之处应该是在注解的解析上。JDK5引入了注解的同时，也提供了两种解析方式。 运行时解析运行时能够解析的注解，必须将@Retention设置为RUNTIME，这样可以通过反射拿到该注解。java.lang.reflect反射包中提供了一个接口AnnotatedElement，该接口定义了获取注解信息的几个方法，Class、Constructor、Field、Method、Package等都实现了该接口，大部分开发者应该都很熟悉这种解析方式。 1234boolean isAnnotationPresent(Class&lt;? extends Annotation&gt; annotationClass);&lt;T extends Annotation&gt; T getAnnotation(Class&lt;T&gt; annotationClass);Annotation[] getAnnotations();Annotation[] getDeclaredAnnotations(); 编译时解析编译时解析有两种机制，网上很多文章都把它俩搞混了，分别简单描述一下。 Annotation Processing Toolapt自JDK5产生，JDK7已标记为过期，不推荐使用，JDK8中已彻底删除，自JDK6开始，可以使用Pluggable Annotation Processing API来替换它，apt被替换主要有2点原因： api都在com.sun.mirror非标准包下 没有集成到javac中，需要额外运行 Pluggable Annotation Processing APIJSR 269，自JDK6加入，作为apt的替代方案，它解决了apt的两个问题，javac在执行的时候会调用实现了该API的程序，这样我们就可以对编译器做一些增强，这时javac执行的过程如下： Lombok问题 无法支持多种参数构造器的重载 奇淫巧技，使用会有争议]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>lombok</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapStruct介绍]]></title>
    <url>%2Fjava%2Fjava%2Fmapstruct%2F1%2F</url>
    <content type="text"><![CDATA[介绍MapStruct在一个成熟可维护的工程中，细分模块后，domian工程最好不要被其他工程依赖，但是实体类一般存于domain之中，这样其他工程想获取实体类数据时就需要在各自工程写model，自定义model可以根据自身业务需要而并不需要映射整个实体属性。mapstruct这个插件就是用来处理domin实体类与model类的属性映射，定义mapper接口，mapstruct就会自动的帮我们实现这个映射接口，避免了麻烦复杂的映射实现。 Github地址：https://github.com/mapstruct/mapstruct/ 使用例子：https://github.com/mapstruct/mapstruct-examples MapStrcut与其它工具对比以及使用说明：http://www.tuicool.com/articles/uiIRjai 如何使用Mapper基本类：BasicObjectMapper，BasicObjectMapper包含了4个基本方法，单个和集合以及反转的单个和集合。 开发中如需要对象转换操作可直接新建interface并继承BasicObjectMapper，并在新建的接口上加上 @Mapper(componentModel = “spring”)， 如果是属性中包含其它类以及该类已经存在Mapper则注解中加上 users = {类名.class}，具体如何使用以及其他各种用法在此不再赘述（本文的重点是看标题，看标题，看标题），google不行可以找度娘， componentModel = “spring”该配置表示生成的实现类默认加上spring @Component注解，使用时可直接通过@Autowire进行注入。 123456789101112131415161718192021package com.ampmind.framework.map;import java.util.List;import org.mapstruct.InheritConfiguration;import org.mapstruct.InheritInverseConfiguration;import org.mapstruct.Mappings;public interface BasicObjectMapper&lt;SOURCE, TARGET&gt; &#123; @Mappings(&#123;&#125;) @InheritConfiguration TARGET to(SOURCE var1); @InheritConfiguration List&lt;TARGET&gt; to(List&lt;SOURCE&gt; var1); @InheritInverseConfiguration SOURCE from(TARGET var1); @InheritInverseConfiguration List&lt;SOURCE&gt; from(List&lt;TARGET&gt; var1);&#125; 下面是两个不同的例子：先贴一下两个Model类 123456789101112131415161718192021222324252627282930package com.ampmind.service.skumng.domain;public class ProductCategory &#123; /** * 类别编码 */ private String categoryCode; /** * 类别名称 */ private String categoryName; public String getCategoryCode() &#123; return categoryCode; &#125; public void setCategoryCode(String categoryCode) &#123; this.categoryCode = categoryCode; &#125; public String getCategoryName() &#123; return categoryName; &#125; public void setCategoryName(String categoryName) &#123; this.categoryName = categoryName; &#125;&#125; CategoryVo 12345678910111213141516171819202122232425262728293031package com.ampmind.service.api.protocol.vo;public class ProductCategory &#123; private String code; private String name; public Integer getParentId() &#123; return parentId; &#125; public void setParentId(Integer parentId) &#123; this.parentId = parentId; &#125; public String getCode() &#123; return code; &#125; public void setCode(String code) &#123; this.code = code; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; 1.如何与Spring配合CategoryMapper 1234567891011121314151617package com.ampmind.service.skumng.api.mapper;import com.ampmind.framework.api.base.BasicObjectMapper;import com.ampmind.service.api.protocol.vo.CategoryVo;import com.ampmind.service.domain.ProductCategory;import org.mapstruct.Mapper;import org.mapstruct.Mapping;import org.mapstruct.Mappings;@Mapper(componentModel = "spring")public interface CategoryMapper extends BasicObjectMapper&lt;CategoryVo, ProductCategory&gt; &#123; @Mappings(&#123; @Mapping(source = "code", target = "categoryCode"), @Mapping(source = "name", target = "categoryName") &#125;) ProductCategory to(CategoryVo source);&#125; 上面重写了to方法，注意如果属性名一样可以不用重写。保持接口空的就行，有不一样的需要重写to方法，并在方法上加上 @Mappings注解和子注解spring注入并使用 12345678910111213141516171819@Componentpublic class Test &#123; @Autowired private CategoryMapper categoryMapper; public void test() &#123; CategoryVo vo = new CategoryVo; vo.setCategoryCode("0000"); vo.setCategoryName("属性名称"); ProductCategory pc = categoryMapper.to(vo);// 通过to方法得到 ProductCategory CategoryVo vo1 = categoryMapper.form(pc);// 通过from方法得到CategoryVo，既反转to方法。 List&lt;ProductCategory&gt; pcList = categoryMapper.to(Arrays.asList(vo, vo1));// 通过to方法从集合得到转换后的集合 List&lt;CategoryVo&gt; voList = categoryMapper.from(pcList); // 反转集合 &#125;&#125; 2.如何直接使用CategoryMapper 1234567891011121314151617181920212223package com.ampmind.service.skumng.api.mapper;import com.ampmind.framework.api.base.BasicObjectMapper;import com.ampmind.service.skumng.api.protocol.vo.CategoryVo;import com.ampmind.service.skumng.domain.ProductCategory;import org.mapstruct.Mapper;import org.mapstruct.Mapping;import org.mapstruct.Mappings;import org.mapstruct.factory.Mappers;/** * */@Mapperpublic interface CategoryMapper extends BasicObjectMapper&lt;CategoryVo, ProductCategory&gt; &#123; CategoryMapper MAPPER = Mappers.getMapper(CategoryMapper.class); @Mappings(&#123; @Mapping(source = "code", target = "categoryCode"), @Mapping(source = "name", target = "categoryName") &#125;) ProductCategory to(CategoryVo source);&#125; 直接可以通过main方法进行测试 123456789101112131415public class Test &#123; public static void main(String[] args) &#123; CategoryVo vo = new CategoryVo; vo.setCategoryCode("0000"); vo.setCategoryName("属性名称"); ProductCategory pc = CategoryMapper.MAPPER.to(vo);// 通过to方法得到 ProductCategory CategoryVo vo1 = CategoryMapper.MAPPER.form(pc);// 通过from方法得到CategoryVo，既反转to方法。 List&lt;ProductCategory&gt; pcList = CategoryMapper.MAPPER.to(Arrays.asList(vo, vo1));// 通过to方法从集合得到转换后的集合 List&lt;CategoryVo&gt; voList = CategoryMapper.MAPPER.from(pcList); // 反转集合 &#125;&#125;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>mapstruct</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaEE PO VO BO DTO POJO DAO 整理总结]]></title>
    <url>%2Fjava%2Fjava%2Fpojo%2F</url>
    <content type="text"><![CDATA[DAO[data access object]数据访问对象 DAO层对开发人员黑盒，由架构师设计封装。在很长一段时间内，我将它理解为对数据库的访问，后面随着项目的积累。发现自己的理解相对狭隘，对数据访问不仅仅指的是对数据库的访问。假如A系统调用B系统的服务获取数据，这时候A系统对B系统访问数据对象的封装也可以称为DAO。 DTO[data transfer object]数据传输对象 假设数据表中存在20个字段，但是在页面展示列表的时候，这20个字段显然都不会用到。我想对其中的5个字段进行展示，而且这5个字段展示的时候，也并不是数据库中他们原有的样子。还需要进行计算、截取、业务代码转名称 …..等等数据传输对象因此而被诞生，一是能提高数据传输的速度，二能隐藏后端表结构。 PO[persistant object]持久层对象 持久对象属性和数据库中的字段是一一对应的，数据库中的一条数据可以理解为一个持久对象。因ORM框架的广泛使用而被引入到 JavaEE 项目设计当中。 BO[bussiness object]业务对象 业务对象顾名思义是在业务处理中抽象出来的对象，里面除了get/set 方法外，也可以有对字段进行业务处理的方法。假设你要对一个班级进行业务处理，其中的学生、教师、甚至是桌椅板凳都是业务对象的组成部分。当然其中的学生、教室….都可以是和数据库对应的PO。 VO[value object]值对象 值对象也可以称做页面对象，如果称做页面对象，那门它所代表的将是整个页面展示层的对象。可以由需要的业务对象进行的换算转换而来。如果称呼他为值对象的话，那门他可以理解为存放业务对象的一个地方。假设锅碗瓢盆分别为对应的业务对象的话，那门整个碗柜就是一个值对象。 POJO[plain ordiary java object] 简单java对象 简单java对象应该是JavaEE世界里面最灵活的对象。在简单系统中，如果从数据库到页面展示都是POJO的话，它可以是DTO。如果从数据库中到业务处理中都是POJO的话，他也可以是BO。同样如果从数据库到整个页面的展示的话，它同样可以是VO。 小结： 各个数据对象之间的转换是相当灵活的，在项目中可以定义上述对象的全部和其中的几种类型，这取决与架构师和需求。在大型项目中，架构师在项目初期的任务除了搭建起整个开发环境以外，定义在系统中流转的数据结构对象同样是重重之重。这项工作需要许多项目的积累和长期对软件开发的思考，多实践，多思考，提供最合适的数据对象解决方法，方能展现架构师的魅力。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>pojo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[npm使用]]></title>
    <url>%2Fnodejs%2Fnpm%2F1%2F</url>
    <content type="text"><![CDATA[NPM是随同NodeJS一起安装的包管理工具，能解决NodeJS代码部署上的很多问题，常见的使用场景有以下几种： 允许用户从NPM服务器下载别人编写的第三方包到本地使用。 允许用户从NPM服务器下载并安装别人编写的命令行程序到本地使用。 允许用户将自己编写的包或命令行程序上传到NPM服务器供别人使用。 由于新版的nodejs已经集成了npm，所以之前npm也一并安装好了。同样可以通过输入 “npm -v” 来测试是否成功安装。命令如下，出现版本提示表示安装成功: 12$ npm -v2.3.0 如果你安装的是旧版本的 npm，可以很容易得通过 npm 命令来升级，命令如下： 123$ sudo npm install npm -g/usr/local/bin/npm -&gt; /usr/local/lib/node_modules/npm/bin/npm-cli.jsnpm@2.14.2 /usr/local/lib/node_modules/npm 如果是 Window 系统使用以下命令即可： 1$ npm install npm -g 使用 npm 命令安装模块npm 安装 Node.js 模块语法格式如下： 1$ npm install &lt;Module Name&gt; 以下实例，我们使用 npm 命令安装常用的 Node.js web框架模块 express: 1$ npm install express 安装好之后，express 包就放在了工程目录下的 node_modules 目录中，因此在代码中只需要通过 require(‘express’) 的方式就好，无需指定第三方包路径。 1var express = require('express'); 全局安装与本地安装npm 的包安装分为本地安装（local）、全局安装（global）两种，从敲的命令行来看，差别只是有没有-g而已，比如 12$ npm install express # 本地安装$ npm install express -g # 全局安装 如果出现以下错误： 1npm err! Error: connect ECONNREFUSED 127.0.0.1:8087 解决办法为： 1$ npm config set proxy null 本地安装 将安装包放在 ./node_modules 下（运行 npm 命令时所在的目录），如果没有 node_modules 目录，会在当前执行 npm 命令的目录下生成 node_modules 目录。 可以通过 require() 来引入本地安装的包。 全局安装 将安装包放在 /usr/local 下或者你 node 的安装目录。 可以直接在命令行里使用。 如果你希望具备两者功能，则需要在两个地方安装它或使用 npm link。接下来我们使用全局方式安装 express 1$ npm install express -g 查看安装信息你可以使用以下命令来查看所有全局安装的模块： 1$ npm list -g 如果要查看某个模块的版本号，可以使用命令如下： 1$ npm list grunt 使用 package.jsonpackage.json 位于模块的目录下，用于定义包的属性。接下来让我们来看下 express 包的 package.json 文件，位于 node_modules/express/package.json 内容： 12345678910111213141516171819202122232425262728293031&#123; "name": "hexo-site", "version": "0.0.0", "private": true, "hexo": &#123; "version": "3.4.4" &#125;, "dependencies": &#123; "hexo": "^3.2.0", "hexo-deployer-git": "^0.3.1", "hexo-generator-archive": "^0.1.4", "hexo-generator-category": "^0.1.3", "hexo-generator-index": "^0.2.0", "hexo-generator-tag": "^0.2.0", "hexo-renderer-ejs": "^0.3.0", "hexo-renderer-marked": "^0.3.0", "hexo-renderer-stylus": "^0.3.1", "hexo-server": "^0.2.0" &#125;, "devDependencies": &#123; "gulp": "^3.9.1", "gulp-htmlclean": "^2.7.16", "gulp-htmlmin": "^4.0.0", "gulp-minify-css": "^1.2.4", "gulp-uglify": "^3.0.0", "hexo-admin": "^2.3.0", "hexo-generator-searchdb": "^1.0.8", "hexo-wordcount": "^3.0.2" &#125;&#125;` Package.json 属性说明 name - 包名。 version - 包的版本号。 description - 包的描述。 homepage - 包的官网 url 。 author - 包的作者姓名。 contributors - 包的其他贡献者姓名。 dependencies - 依赖包列表。如果依赖包没有安装，npm 会自动将依赖包安装在 node_module 目录下。 repository - 包代码存放的地方的类型，可以是 git 或 svn，git 可在 Github 上。 main - main 字段指定了程序的主入口文件，require(‘moduleName’) 就会加载这个文件。这个字段的默认值是模块根目录下面的 index.js。 keywords - 关键字 卸载模块我们可以使用以下命令来卸载 Node.js 模块。 1$ npm uninstall express 卸载后，你可以到 /node_modules/ 目录下查看包是否还存在，或者使用以下命令查看： 1$ npm ls 更新模块我们可以使用以下命令更新模块： 1$ npm update express 搜索模块使用以下来搜索模块： 1$ npm search express 使用淘宝 NPM 镜像大家都知道国内直接使用 npm 的官方镜像是非常慢的，这里推荐使用淘宝 NPM 镜像。 淘宝 NPM 镜像是一个完整 npmjs.org 镜像，你可以用此代替官方版本(只读)，同步频率目前为 10分钟 一次以保证尽量与官方服务同步。 你可以使用淘宝定制的 cnpm (gzip 压缩支持) 命令行工具代替默认的 npm: 1$ npm install -g cnpm --registry=https://registry.npm.taobao.org 安装不创建bin链接1$ npm install --no-bin-links 编译并不创建bin链接1$ npm rebuild node-sass --no-bin-links]]></content>
      <categories>
        <category>nodejs</category>
      </categories>
      <tags>
        <tag>nodejs</tag>
        <tag>npm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VirtualBox建立软链接]]></title>
    <url>%2FVirtualBox%2Fvirtualbox%2F1%2F</url>
    <content type="text"><![CDATA[VirtualBox共享目录，建立软链接 关闭 VirtualBox。 将VirtualBox安装目录的路径加入系统环境变量PATH中。 打开命令行窗口，执行如下命令： 12VBoxManage setextradata &#123;YOURVMNAME&#125;VBoxInternal2/SharedFoldersEnableSymlinksCreate/&#123;YOURSHAREFOLDERNAME&#125; 1 参数说明YOURVMNAME：为虚拟机中ubuntu系统的名YOURSHAREFOLDERNAME：为共享的目录名称 “以管理者身份运行” VirtualBox 即可！]]></content>
      <categories>
        <category>VirtualBox</category>
      </categories>
      <tags>
        <tag>VirtualBox</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[9个常用iptables配置实例]]></title>
    <url>%2Flinux%2Flinux%2Fshell-iptabls-2%2F</url>
    <content type="text"><![CDATA[iptables命令可用于配置Linux的包过滤规则，常用于实现防火墙、NAT。咋一看iptables的配置很复杂，掌握规律后，其实用iptables完成指定任务并不难，下面我们通过具体实例，学习iptables的详细用法。 删除已有规则 在新设定iptables规则时，我们一般先确保旧规则被清除，用以下命令清除旧规则 12$ iptables -F(or iptables --flush) 设置chain策略 对于filter table，默认的chain策略为ACCEPT，我们可以通过以下命令修改chain的策略 123$ iptables -P INPUT DROP$ iptables -P FORWARD DROP$ iptables -P OUTPUT DROP 以上命令配置将接收、转发和发出包均丢弃，施行比较严格的包管理。由于接收和发包均被设置为丢弃，当进一步配置其他规则的时候，需要注意针对INPUT和OUTPUT分别配置。当然，如果信任本机器往外发包，以上第三条规则可不必配置。 屏蔽指定ip 有时候我们发现某个ip不停的往服务器发包，这时我们可以使用以下命令，将指定ip发来的包丢弃 12$ BLOCK_THIS_IP="x.x.x.x"$ iptables -A INPUT -i eth0 -p tcp -s "$BLOCK_THIS_IP" -j DROP 以上命令设置将由x.x.x.x ip发往eth0网口的tcp包丢弃 配置服务项 利用iptables，我们可以对日常用到的服务项进行安全管理，比如设定只能通过指定网段、由指定网口通过SSH连接本机 12$ iptables -A INPUT -i eth0 -p tcp -s 192.168.100.0/24 --dport 22 -m state --state NEW,ESTABLESHED -j ACCEPT$ iptables -A OUTPUT -o eth0 -p tcp --sport 22 -m state --state ESTABLISHED -j ACCEPT 若要支持由本机通过SSH连接其他机器，由于在本机端口建立连接，因而还需要设置以下规则 12$ iptables -A INPUT -i eth0 -p tcp -s 192.168.100.0/24 --dport 22 -m state --state ESTABLESHED -j ACCEPT$ iptables -A OUTPUT -o eth0 -p tcp --sport 22 -m state --state NEW,ESTABLISHED -j ACCEPT 类似的，对于HTTP/HTTPS(80/443)、pop3(110)、rsync(873)、MySQL(3306)等基于tcp连接的服务，也可以参照上述命令配置。 对于基于udp的dns服务，使用以下命令开启端口服务 12$ iptables -A OUTPUT -p udp -o eth0 --dport 53 -j ACCEPT$ iptables -A INPUT -p udp -i eth0 --sport 53 -j ACCEPT 网口转发配置 对于用作防火墙或网关的服务器，一个网口连接到公网，其他网口的包转发到该网口实现内网向公网通信，假设eth0连接内网，eth1连接公网，配置规则如下 1$ iptables -A FORWARD -i eth0 -o eth1 -j ACCEPT 端口转发配置 对于端口，我们也可以运用iptables完成转发配置 1$ iptables -t nat -A PREROUTING -p tcp -d 192.168.102.37 --dport 422 -j DNAT --to 192.168.102.37:22 以上命令将422端口的包转发到22端口，因而通过422端口也可进行SSH连接，当然对于422端口，我们也需要像以上“4.配置服务项”一节一样，配置其支持连接建立的规则。 DoS攻击防范 利用扩展模块limit，我们还可以配置iptables规则，实现DoS攻击防范 1$ iptables -A INPUT -p tcp --dport 80 -m limit --limit 25/minute --limit-burst 100 -j ACCEPT –litmit 25/minute 指示每分钟限制最大连接数为25–litmit-burst 100 指示当总连接数超过100时，启动 litmit/minute 限制 配置web流量均衡 我们可以将一台服务器作为前端服务器，利用iptables进行流量分发，配置方法如下 123$ iptables -A PREROUTING -i eth0 -p tcp --dport 80 -m state --state NEW -m nth --counter 0 --every 3 --packet 0 -j DNAT --to-destination 192.168.1.101:80$ iptables -A PREROUTING -i eth0 -p tcp --dport 80 -m state --state NEW -m nth --counter 0 --every 3 --packet 0 -j DNAT --to-destination 192.168.1.102:80$ iptables -A PREROUTING -i eth0 -p tcp --dport 80 -m state --state NEW -m nth --counter 0 --every 3 --packet 0 -j DNAT --to-destination 192.168.1.103:80 以上配置规则用到nth扩展模块，将80端口的流量均衡到三台服务器 将丢弃包情况记入日志 使用LOG目标和syslog服务，我们可以记录某协议某端口下的收发包情况。拿记录丢包情况举例，可以通过以下方式实现。 首先自定义一个chain 1$ iptables -N LOGGING 其次将所有接收包导入LOGGING chain中 1$ iptables -A INPUT -j LOGGING 然后设置日志前缀、日志级别 1$ iptables -A LOGGING -m limit --limit 2/min -j LOG --log-prefix "IPTables Packet Dropped: " --log-level 7 最后将包倒向DROP，将包丢弃 1$ iptables -A LOGGING -j DROP 另可以配置syslog.conf文件，指定iptables的日志输出]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>shell</tag>
        <tag>iptables</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TC基于CBQ队列的流量管理范例]]></title>
    <url>%2Flinux%2Flinux%2Fshell-tc-1%2F</url>
    <content type="text"><![CDATA[简介参考了TC的很多文档，自己也整理了一篇配置记录。在实际使用过程中效果还不错，在此分享给大家以备参考。环境：局域网规模不是很大40多台机器。 NAT共享上网（内网：eth0 外网：eth2）CBQ是通过硬件的闲置时间来计算队列，硬件不同，效果也不同，对于比较大的网络使用HTB比较好。以下限制上传和下载的方法可以写成脚本，通过mrtg发现流量的异常情况，然后通过ntop查处是谁在干坏事，最后用写好的tc脚本限制他的流量，避免影响其他人的网络使用。 示例针对网络物理设备绑定一个CBQ队列 1$ tc qdisc add dev eth0 root handle 1: cbq bandwidth 10Mbit avpkt 1000 cell 8 mpu 64 将一个cbq队列绑定到网络物理设备eth0上，其编号为1:0；网络物理设备eth0的实际带宽为10Mbit，包的平均大小为1000字节；包间隔发送单元的大小为8字节，最小传输包大小为64字节。 在该队列上建立分类 1$ tc class add dev eth0 parent 1:0 classid 1:1 cbq bandwidth 10Mbit rate 10Mbit maxburst 20 allot 1514 prio 1 avpkt 1000 cell 8 weight 1Mbit 创建根分类1:1；分配带宽为10Mbit，优先级别为1。该队列的最大可用带宽为10Mbit，实际分配的带宽为10Mbit，可接收冲突的发送最长包数目为20字节；最大传输单元加MAC头的大小为1514字节，优先级别为1，包的平均大小为1000字节，包间隔发送单元的大小为8字节，相应于实际带宽的加权速率为1Mbit。 创建子分类 创建分类1:2，其父分类为1:1，分配带宽为64Kbit，优先级别为8。该队列的最大可用带宽为10Mbit，实际分配的带宽为64Kbit，可接收冲突的发送最长包数目为20字节；最大传输单元加MAC头的大小为1514字节，优先级别为8，包的平均大小为1000字节，包间隔发送单元的大小为8字节，相应于实际带宽的加权速率为100Kbit，且不可借用未使用带宽。$ tc class add dev eth0 parent 1:1 classid 1:2 cbq bandwidth 10Mbit rate 64Kbit maxburst 20 allot 1514 prio 8 avpkt 1000 cell 8 weight 100Kbit bounded 创建分类1:3，其父分类为1:1，分配带宽为64Kbit，优先级别为9。该队列的最大可用带宽为10Mbit，实际分配的带宽为64Kbit，可接收冲突的发送最长包数目为20字节；最大传输单元加MAC头的大小为1514字节，优先级别为9，包的平均大小为1000字节，包间隔发送单元的大小为8字节，相应于实际带宽的加权速率为100Kbit，且不可借用未使用带宽。$ tc class add dev eth0 parent 1:1 classid 1:3 cbq bandwidth 10Mbit rate 64Kbit maxburst 20 allot 1514 prio 9 avpkt 1000 cell 8 weight 100Kbit bounded 在子分类地下创建队列，使用sfq随机公平队列 12$ tc qdisc add dev eth0 parent 1:2 sfq quantum 1514b perturb 15$ tc qdisc add dev eth0 parent 1:3 sfq quantum 1514b perturb 15 在分类底下，创建队列，使用sfq随即公平队列 为每一分类建立一个基于路由的过滤 12$ tc filter add dev eth0 parent 1:0 protocol ip prio 1 u32 match ip dst 192.111.1.116 flowid 1:2$ tc filter add dev eth0 parent 1:0 protocol ip prio 1 u32 match ip dst 192.111.1.66 flowid 1:3 限制各ip地址的下载带宽，使用u32过滤器，对目的地址进行分类，对应已经创建的队列需要添加新的被限制ip的下载带宽，需要先要创建新的分类(比如1:4),然后根据新的分类创建新的sfq队列，最后使用u32过滤器对目的地址进行带宽限制。需要对几个ip限制下载带宽，就需要创建几个分类、队列、过滤器 限制上传 将一个cbq队列绑定到网络物理设备eth2上，其编号为2:0；网络物理设备eth2的实际带宽为2Mbit，包的平均大小为1000字节；包间隔发送单元的大小为8字节，最小传输包大小为64字节。 1$ tc qdisc add dev eth2 root handle 2: cbq bandwidth 2Mbit avpkt 1000 cell 8 mpu 64 创建根分类2:1；分配带宽为2Mbit，优先级别为1。该队列的最大可用带宽为2Mbit，实际分配的带宽2Mbit， 可接收冲突的发送最长包数目为20字节；最大传输单元加MAC头的大小为1514字节，优先级别为1，包的平均 大小为1000字节，包间隔发送单元的大小为8字节，相应于实际带宽的加权速率为200Kbit。 1$ tc class add dev eth2 parent 2:0 classid 2:1 cbq bandwidth 2Mbit rate 2Mbit maxburst 20 allot 1514 prio 1 avpkt 1000 cell 8 weight 200Kbit 创建分类2:2，其父分类为2:1，分配带宽为64Kbit，优先级别为8。该队列的最大可用带宽为2Mbit，实际分配的带宽为64Kbit，可接收冲突的发送最长包数目为20字节；最大传输单元加MAC头的大小为1514字节，优先级别为8，包的平均大小为1000字节，包间隔发送单元的大小为8字节，相应于实际带宽的加权速率为100Kbit，且不可借用未使用带宽。 1$ tc class add dev eth2 parent 2:1 classid 2:2 cbq bandwidth 2Mbit rate 64Kbit maxburst 20 allot 1514 prio 8 avpkt 1000 cell 8 weight 200Kbit bounded 在分类底下，创建队列，使用sfq随即公平队列 1$ tc qdisc add dev eth2 parent 2:2 sfq quantum 1514b perturb 15 应用路由分类器到cbq队列的根，过滤协议为ip，优先级为100 1$ tc filter add dev eth2 parent 2:0 protocol ip prio 1 handle 2 fw classid 2:2 给数据包打标签,可以通过RETURN方法避免遍历所有的规则，加快处理速度 12$ iptables –t mangle –A PREROUTING –i eth0 –s 192.111.1.xxx –j MARK --set-mark 2$ iptables –t mangle –A PREROUTING –i eth0 –s 192.111.1.xxx –j RETURN nat模式 1$ iptables -t nat -A POSTROUTING -s 192.111.1.0/24 -o eth2 -j SNAT --to 外网IP 需要添加新的被限制ip的上传带宽，需要先要创建新的分类(比如2:3),然后根据新的分类创建新的sfq队列，最后使用路由过滤器，过滤协议为ip，给原地址是需要限制的ip地址来的数据包打标记。需要对几个ip限制下载带宽，就需要创建几个分类、队列、路由过滤器、iptable的mangle表的PREROUTING链 另外还有其他的过滤器比如 1$ tc filter add dev eth0 parent 1:0 protocol ip prio 100 route to 2 flowid 1:2 ip route add 192.111.1.24 dev eth0 via 192.111.1.4 realm 2 维护 主要包括对队列、分类、过滤器和路由的增添、修改和删除。 增添动作一般依照”队列-&gt;分类-&gt;过滤器-&gt;路由”的顺序进行；修改动作则没有什么要求；删除则依照”路由-&gt;过滤器-&gt;分类-&gt;队列”的顺序进行。 简单显示指定设备的队列状况 1$ tc qdisc ls dev eth0 详细显示指定设备的队列状况 1$ tc –s qdisc ls dev eth0 简单显示指定设备的分类状况 1$ tc class ls dev eth0 详细显示指定设备的分类状况 1$ tc –s class ls dev eth0 显示过滤器的状况 1$ tc –s filter ls dev eth0 队列的维护 一般对于一台流量控制器来说，出厂时针对每个以太网卡均已配置好一个队列了，通常情况下对队列无需进行增添、修改和删除动作了。 分类的维护 增添动作通过tc class add命令实现。 修改动作通过tc class change命令实现，如下所示： 1$ tc class change dev eth0 parent 1:1 classid 1:2 cbq bandwidth 10Mbit rate 64Kbit maxburst 20 allot 1514 prio 8 avpkt 1000 cell 8 weight 100Kbit bounded 对于bounded命令应慎用，一旦添加后就进行修改，只可通过删除后再添加来实现。 过滤器的维护 增添动作通过tc filter add命令实现。 修改动作通过tc filter change命令实现，如下所示： 1$ tc filter change dev eth0 parent 1:0 protocol ip prio 1 u32 match ip dst 192.111.1.116 flowid 1:2 删除动作通过tc filter del命令实现，如下所示： 1$ tc filter del dev eth0 parent 1:0 protocol ip prio 1 u32 match ip dst 192.111.1.116 flowid 1:]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>shell</tag>
        <tag>tc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iptables之nat表应用——IP与端口的映射]]></title>
    <url>%2Flinux%2Flinux%2Fshell-iptables-1%2F</url>
    <content type="text"><![CDATA[需求 将192.168.3.195：80 映射到192.168.3.193：80，即访问192.168.3.195：80，得到192.168.3.193：80的结果，实现linux的路由。 实现 123456789101112131415161718192021#!/bin/bash#打开转发功能echo "1" &gt; /proc/sys/net/ipv4/ip_forward /sbin/iptables -F -t filter#清空iptables/sbin/iptables -F -t nat #去192.168.3.193的一条路/sbin/iptables -t nat -A PREROUTING -d 192.168.3.195 -p tcp --dport 80 -j DNAT --to-destination 192.168.3.193:80 #返回的时候的一条路，ip传输要有去有回才能连通 /sbin/iptables -t nat -A POSTROUTING -s 192.168.3.0/24 -o eth0 -j SNAT --to 192.168.3.195 #用2网段访问的时候的回路/sbin/iptables -t nat -A POSTROUTING -s 192.168.2.0/24 -o eth0 -j SNAT --to 192.168.3.195 #用0网段访问的时候的回路/sbin/iptables -t nat -A POSTROUTING -s 192.168.0.0/24 -o eth0 -j SNAT --to 192.168.3.195 回路的那一条，或者只用下面这一句：iptables -t nat -A POSTROUTING -s 0.0.0.0/0 -o eth0 -j SNAT --to 192.168.3.195或者iptables -t nat -A POSTROUTING -o eth0 -j SNAT --to 192.168.3.195把193上的80端口打开 测试 在192.168.3.195上访问 192.168.3.195：80看到 195的apache主页，因为在本机上访问，没有走PREROUTING这条链。在其他主机上访问192.168.3.195：80 返回193的apache主页，路线为PREROUTING–&gt;FORWARD–&gt;PSOTROUTING.]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>shell</tag>
        <tag>iptables</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Iptables命令]]></title>
    <url>%2Flinux%2Flinux%2Fshell-iptables%2F</url>
    <content type="text"><![CDATA[简介iptables命令是Linux上常用的防火墙软件，是netfilter项目的一部分。可以直接配置，也可以通过许多前端和图形界面配置。 参数说明12345678910111213141516-t&lt;表&gt;：指定要操纵的表；-A：向规则链中添加条目；-D：从规则链中删除条目；-i：向规则链中插入条目；-R：替换规则链中的条目；-L：显示规则链中已有的条目；-F：清楚规则链中已有的条目；-Z：清空规则链中的数据包计算器和字节计数器；-N：创建新的用户自定义规则链；-P：定义规则链中的默认目标；-h：显示帮助信息；-p：指定要匹配的数据包协议类型；-s：指定要匹配的数据包源ip地址；-j&lt;目标&gt;：指定要跳转的目标；-i&lt;网络接口&gt;：指定数据包进入本机的网络接口；-o&lt;网络接口&gt;：指定数据包要离开本机所使用的网络接口。 命令格式iptables -t 表名 &lt;-A/I/D/R&gt; 规则链名 [规则号] &lt;-i/o 网卡名&gt; -p 协议名 &lt;-s 源IP/源子网&gt; --sport 源端口 &lt;-d 目标IP/目标子网&gt; --dport 目标端口 -j 动作 表名包括 1234raw：高级功能，如：网址过滤。mangle：数据包修改（QOS），用于实现服务质量。net：地址转换，用于网关路由器。filter：包过滤，用于防火墙规则。 规则链名包括 12345INPUT链：处理输入数据包。OUTPUT链：处理输出数据包。PORWARD链：处理转发数据包。PREROUTING链：用于目标地址转换（DNAT）。POSTOUTING链：用于源地址转换（SNAT）。 动作包括 1234567accept：接收数据包。DROP：丢弃数据包。REDIRECT：重定向、映射、透明代理。SNAT：源地址转换。DNAT：目标地址转换。MASQUERADE：IP伪装（NAT），用于ADSL。LOG：日志记录。 示例清除已有iptables规则123$ iptables -F$ iptables -X$ iptables -Z 开放指定的端口123456789$ iptables -A INPUT -s 127.0.0.1 -d 127.0.0.1 -j ACCEPT #允许本地回环接口(即运行本机访问本机)$ iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT #允许已建立的或相关连的通行$ iptables -A OUTPUT -j ACCEPT #允许所有本机向外的访问$ iptables -A INPUT -p tcp --dport 22 -j ACCEPT #允许访问22端口$ iptables -A INPUT -p tcp --dport 80 -j ACCEPT #允许访问80端口$ iptables -A INPUT -p tcp --dport 21 -j ACCEPT #允许ftp服务的21端口$ iptables -A INPUT -p tcp --dport 20 -j ACCEPT #允许FTP服务的20端口$ iptables -A INPUT -j reject #禁止其他未允许的规则访问$ iptables -A FORWARD -j REJECT #禁止其他未允许的规则访问 屏蔽IP1234$ iptables -I INPUT -s 123.45.6.7 -j DROP #屏蔽单个IP的命令$ iptables -I INPUT -s 123.0.0.0/8 -j DROP #封整个段即从123.0.0.1到123.255.255.254的命令$ iptables -I INPUT -s 124.45.0.0/16 -j DROP #封IP段即从123.45.0.1到123.45.255.254的命令$ iptables -I INPUT -s 123.45.6.0/24 -j DROP #封IP段即从123.45.6.1到123.45.6.254的命令是 查看已添加的iptables规则123456789101112131415$ iptables -L -n -vChain INPUT (policy DROP 48106 packets, 2690K bytes) pkts bytes target prot opt in out source destination 5075 589K ACCEPT all -- lo * 0.0.0.0/0 0.0.0.0/0 191K 90M ACCEPT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:221499K 133M ACCEPT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:804364K 6351M ACCEPT all -- * * 0.0.0.0/0 0.0.0.0/0 state RELATED,ESTABLISHED 6256 327K ACCEPT icmp -- * * 0.0.0.0/0 0.0.0.0/0 Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 3382K packets, 1819M bytes) pkts bytes target prot opt in out source destination 5075 589K ACCEPT all -- * lo 0.0.0.0/0 0.0.0.0/0 删除已添加的iptables规则 将所有iptables以序号标记显示，执行 1$ iptables -L -n --line-numbers 比如要删除INPUT里序号为8的规则，执行 1$ iptables -D INPUT 8]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>shell</tag>
        <tag>iptables</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell /bin/bash^M: bad interpreter错误解决]]></title>
    <url>%2Flinux%2Flinux%2Ferror-bash-1%2F</url>
    <content type="text"><![CDATA[错误原因之一很有可能是你的脚本文件是DOS格式的, 即每一行的行尾以\r\n来标识, 其ASCII码分别是0x0D, 0x0A.可以有很多种办法看这个文件是DOS格式的还是UNIX格式的, 还是MAC格式的。 vi filename然后用命令:set ff?可以看到dos或unix的字样. 如果的确是dos格式的, 那么你可以用set ff=unix把它强制为unix格式的, 然后存盘退出. 再运行一遍看. 用joe filename如果是DOS格式的, 那么行尾会有很多绿色的^M字样出现. 你也可以用上述办法把它转为UNIX格式的. 用od -t x1 filename如果你看到有0d 0a 这样的字符, 那么它是dos格式的, 如果只有0a而没有0d, 那么它是UNIX格式的, 同样可以用上述方法把它转为UNIX格式的. 转换不同平台的文本文件格式可以用 unix2dos或dos2unix这两个小程序来做. 很简单. 在djgpp中这两个程序的名字叫dtou和utod, u代表unix, d代表dos 也可以用sed 这样的工具来做: 12$ sed 's/^M//' filename &gt; tmp_filename$ mv -f tmp_filename filename 来做说明:^M并不是按键shift + 6产生的^和字母M, 它是一个字符, 其ASCII是0x0D, 生成它的办法是先按CTRL+V, 然后再回车(或CTRL+M)]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tcpdump命令]]></title>
    <url>%2Flinux%2Flinux%2Fshell-tcpdump%2F</url>
    <content type="text"><![CDATA[简介用简单的话来定义tcpdump，就是：dump the traffic on a network，根据使用者的定义对网络上的数据包进行截获的包分析工具。 tcpdump可以将网络中传送的数据包的“头”完全截获下来提供分析。它支持针对网络层、协议、主机、网络或端口的过滤，并提供and、or、not等逻辑语句来帮助你去掉无用的信息。 参数说明 使用格式 $ tcpdump [ -DenNqvX ] [ -c count ] [ -F file ] [ -i interface ] [ -r file ] [ -s snaplen ] [ -wfile ] [ expression ] 抓包选项 12345678910111213-c：指定要抓取的包数量。注意，是最终要获取这么多个包。例如，指定"-c 10"将获取10个包，但可能已经 处理了100个包，只不过只有10个包是满足条件的包。 -i interface：指定tcpdump需要监听的接口。若未指定该选项，将从系统接口列表中搜寻编号最小的已配置好的接口(不包括loopback接口，要抓取loopback接口使用tcpdump -i lo)，一旦找到第一个符合条件的接口，搜寻马上结束。可以使用'any'关键字表示所有网络接口。 -n：对地址以数字方式显式，否则显式为主机名，也就是说-n选项不做主机名解析。-nn：除了-n的作用外，还把端口显示为数值，否则显示端口服务名。-N：不打印出host的域名部分。例如tcpdump将会打印'nic'而不是'nic.ddn.mil'。-P：指定要抓取的包是流入还是流出的包。可以给定的值为"in"、"out"和"inout"，默认为"inout"。-s len：设置tcpdump的数据包抓取长度为len，如果不设置默认将会是65535字节。对于要抓取的数据包较大时，长度设置不够可能会产生包截断，若出现包截断，输出行中会出现"[|proto]"的标志(proto实际会显示为协议名)。但是抓取len越长，包的处理时间越长，并且会减少tcpdump可缓存的数据包的数量，从而会导致数据包的丢失，所以在能抓取我们想要的包的前提下，抓取长度越小越好。 输出选项 1234567-e：输出的每行中都将包括数据链路层头部信息，例如源MAC和目标MAC。-q：快速打印输出。即打印很少的协议相关信息，从而输出行都比较简短。-X：输出包的头部数据，会以16进制和ASCII两种方式同时输出。-XX：输出包的头部数据，会以16进制和ASCII两种方式同时输出，更详细。-v：当分析和打印的时候，产生详细的输出。-vv：产生比-v更详细的输出。-vvv：产生比-vv更详细的输出。 其他功能性选项 12345-D：列出可用于抓包的接口。将会列出接口的数值编号和接口名，它们都可以用于"-i"后。-F：从文件中读取抓包的表达式。若使用该选项，则命令行中给定的其他表达式都将失效。-w：将抓包数据输出到文件中而不是标准输出。可以同时配合"-G time"选项使得输出文件每time秒就自动切换到另一个文件。可通过"-r"选项载入这些文件以进行分析和打印。-r：从给定的数据包文件中读取数据。使用"-"表示从标准输入中读取。 示例 监视指定网络接口的数据包 1$ tcpdump -i eth1 如果不指定网卡，默认tcpdump只会监视第一个网络接口，如eth0。 监视指定主机的数据包，例如所有进入或离开longshuai的数据包 1$ tcpdump host longshuai 打印helios&lt;–&gt;hot或helios&lt;–&gt;ace之间通信的数据包 1$ tcpdump host helios and \( hot or ace \) 打印ace与任何其他主机之间通信的IP数据包,但不包括与helios之间的数据包 1$ tcpdump ip host ace and not helios 截获主机hostname发送的所有数据 1$ tcpdump src host hostname 监视所有发送到主机hostname的数据包 1$ tcpdump dst host hostname 监视指定主机和端口的数据包 1$ tcpdump tcp port 22 and host hostname 对本机的udp 123端口进行监视(123为ntp的服务端口) 1$ tcpdump udp port 123 监视指定网络的数据包，如本机与192.168网段通信的数据包，”-c 10”表示只抓取10个包 1$ tcpdump -c 10 net 192.168 打印所有通过网关snup的ftp数据包(注意,表达式被单引号括起来了,这可以防止shell对其中的括号进行错误解析) 1$ tcpdump 'gateway snup and (port ftp or ftp-data)' 抓取ping包 1$ tcpdump -c 5 -nn -i eth0 icmp 如果明确要抓取主机为192.168.100.70对本机的ping，则使用and操作符。 1$ tcpdump -c 5 -nn -i eth0 icmp and src 192.168.100.62 注意不能直接写icmp src 192.168.100.70，因为icmp协议不支持直接应用host这个type。 抓取到本机22端口包 1$ tcpdump -c 10 -nn -i eth0 tcp dst port 22 解析包数据 1$ tcpdump -c 2 -q -XX -vvv -nn -i eth0 tcp dst port 22]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>shell</tag>
        <tag>tcpdump</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DATE命令]]></title>
    <url>%2Flinux%2Flinux%2Fshell-date%2F</url>
    <content type="text"><![CDATA[简介很多shell脚本里面需要打印不同格式的时间或日期，以及要根据时间和日期执行操作。延时通常用于脚本执行过程中提供一段等待的时间。日期可以以多种格式去打印，也可以使用命令设置固定的格式。在类UNIX系统中，日期被存储为一个整数，其大小为自世界标准时间（UTC）1970年1月1日0时0分0秒起流逝的秒数。 参数说明12345-d&lt;字符串&gt;：显示字符串所指的日期与时间。字符串前后必须加上双引号；-s&lt;字符串&gt;：根据字符串来设置日期与时间。字符串前后必须加上双引号；-u：显示GMT；--help：在线帮助；--version：显示版本信息。 格式列表1234567891011121314151617181920212223242526272829%n : 下一行%t : 跳格%H : 小时(00..23)%I : 小时(01..12)%k : 小时(0..23)%l : 小时(1..12)%M : 分钟(00..59)%p : 显示本地 AM 或 PM%r : 直接显示时间 (12 小时制，格式为 hh:mm:ss [AP]M)%s : 从 1970 年 1 月 1 日 00:00:00 UTC 到目前为止的秒数%S : 秒(00..61)%T : 直接显示时间 (24 小时制)%X : 相当于 %H:%M:%S%Z : 显示时区 %a : 星期几 (Sun..Sat)%A : 星期几 (Sunday..Saturday)%b : 月份 (Jan..Dec)%B : 月份 (January..December)%c : 直接显示日期与时间%d : 日 (01..31)%D : 直接显示日期 (mm/dd/yy)%h : 同 %b%j : 一年中的第几天 (001..366)%m : 月份 (01..12)%U : 一年中的第几周 (00..53) (以 Sunday 为一周的第一天的情形)%w : 一周中的第几天 (0..6)%W : 一年中的第几周 (00..53) (以 Monday 为一周的第一天的情形)%x : 直接显示日期 (mm/dd/yy)%y : 年份的最后两位数字 (00.99)%Y : 完整年份 (0000..9999) 示例设置时间1234567date -s //设置当前时间，只有root权限才能设置，其他只能查看。date -s 20080523 //设置成20080523，这样会把具体时间设置成空00:00:00date -s 01:01:01 //设置具体时间，不会对日期做更改date -s “01:01:01 2008-05-23″ //这样可以设置全部时间date -s “01:01:01 20080523″ //这样可以设置全部时间date -s “2008-05-23 01:01:01″ //这样可以设置全部时间date -s “20080523 01:01:01″ //这样可以设置全部时间 时间加减1234567date +%Y%m%d //显示现在天年月日date +%Y%m%d --date="+1 day" //显示后一天的日期date +%Y%m%d --date="-1 day" //显示前一天的日期date +%Y%m%d --date="-1 month" //显示上一月的日期date +%Y%m%d --date="+1 month" //显示下一月的日期date +%Y%m%d --date="-1 year" //显示前一年的日期date +%Y%m%d --date="+1 year" //显示下一年的日期]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>shell</tag>
        <tag>date</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hwclock命令]]></title>
    <url>%2Flinux%2Flinux%2Fshell-hwclock%2F</url>
    <content type="text"><![CDATA[简介hwclock命令是一个硬件时钟访问工具，它可以显示当前时间、设置硬件时钟的时间和设置硬件时钟为系统时间，也可设置系统时间为硬件时钟的时间。 在Linux中有硬件时钟与系统时钟等两种时钟。硬件时钟是指主机板上的时钟设备，也就是通常可在BIOS画面设定的时钟。系统时钟则是指kernel中的时钟。当Linux启动时，系统时钟会去读取硬件时钟的设定，之后系统时钟即独立运作。所有Linux相关指令与函数都是读取系统时钟的设定。 参数说明12345678910--adjust：hwclock每次更改硬件时钟时，都会记录在/etc/adjtime文件中。使用--adjust参数，可使hwclock根据先前的记录来估算硬件时钟的偏差，并用来校正目前的硬件时钟；--debug：显示hwclock执行时详细的信息；--directisa：hwclock预设从/dev/rtc设备来存取硬件时钟。若无法存取时，可用此参数直接以I/O指令来存取硬件时钟；--hctosys：将系统时钟调整为与目前的硬件时钟一致；--set --date=&lt;日期与时间&gt;：设定硬件时钟；--show：显示硬件时钟的时间与日期；--systohc：将硬件时钟调整为与目前的系统时钟一致；--test：仅测试程序，而不会实际更改硬件时钟；--utc：若要使用格林威治时间，请加入此参数，hwclock会执行转换的工作；--version：显示版本信息。 示例设置硬件时间要依赖于操作系统时间12$ hwclock –systohc$ hwclock --systohc –-utc 查看当前的硬件日期和时间1$ hwclock 设置硬件时间1$ hwclock -w 查看clock文件，确认是否设置了UTC12$ cat /etc/default/rcS UTC=yes]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>shell</tag>
        <tag>hwclock</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vsftpd安装]]></title>
    <url>%2Flinux%2Flinux%2Fsoftware-vsftpd%2F</url>
    <content type="text"><![CDATA[准备工作 linux系统centos7 vsftpd软件 yum安装 执行步骤安装vsftpd服务器 1$ yum install vsftpd 安装一个加密工具 1$ yum install libdb-utils.x86_64 修改配置VSFTP 1$ vi /etc/vsftpd/vsftpd.conf 配置文件参数说明 anonymous_enable=NO #设定不允许匿名访问 local_enable=YES #设定本地用户可以访问。注：如使用虚拟宿主用户，在该项目设定为NO的情况下所有虚拟用户将无法访问。 chroot_list_enable=YES #使用户不能离开主目录 ascii_upload_enable=YES #允许使用ASCII模式上传 ascii_download_enable=YES #设定支持ASCII模式的上传和下载功能。 pam_service_name=vsftpd #PAM认证文件名。PAM将根据/etc/pam.d/vsftpd进行认证 guest_enable=YES #设定启用虚拟用户功能。 guest_username=ftp #指定虚拟用户的宿主用户。-RHEL/CentOS中已经有内置的ftp用户了 user_config_dir=/etc/vsftpd/vuser_conf #设定虚拟用户个人vsftp的RHEL/CentOS FTP服务文件存放路 径。 listen=YES # 只监听ipv4的地址 xferlog_file=/var/log/xferlog # 日志文件的路径 listen_port=1315 #FTP端口 pasv_enable=YES #开启被动模式 pasv_min_port=10060 pasv_max_port=10070 创建chroot list，将ftp用户加入其中 12$ touch /etc/vsftpd/chroot_list$ echo ftp &gt;&gt; /etc/vsftpd/chroot_list 安装Berkeley DB工具 1$ yum install db4 db4-utils 创建用户密码文本 1$ touch /etc/vsftpd/vuser_passwd.txt 生成虚拟用户认证的db文件 12$ db_load -T -t hash -f /etc/vsftpd/vuser_passwd.txt /etc/vsftpd/vuser_passwd.db$ chmod 600 /etc/vsftpd/vuser_passwd.db 编辑认证文件 1$ vi /etc/pam.d/vsftpd 把前面的注释去掉，然后加上以下几条 系统为32位： auth required pam_userdb.so db=/etc/vsftpd/vuser_passwd account required pam_userdb.so db=/etc/vsftpd/vuser_passwd 系统为64位：auth required /lib64/security/pam_userdb.sodb=/etc/vsftpd/vuser_passwd account required/lib64/security/pam_userdb.so db=/etc/vsftpd/vuser_passwd 修改VSFTPD端口 执行vi /etc/services，将其中的 ftp 21/tcp 改为 ftp 1315/tcp , ftp21/udp改为 ftp 1315/udp 重启动vsftp服务 1$ service vsftpd restart]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>software</tag>
        <tag>vsftpd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GREP命令]]></title>
    <url>%2Flinux%2Flinux%2Fshell-grep%2F</url>
    <content type="text"><![CDATA[简介grep（global search regular expression(RE) and print out the line，全面搜索正则表达式并把行打印出来）是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。 参数说明12345678910111213141516171819202122232425-a 不要忽略二进制数据。-A&lt;显示列数&gt; 除了显示符合范本样式的那一行之外，并显示该行之后的内容。-b 在显示符合范本样式的那一行之外，并显示该行之前的内容。-c 计算符合范本样式的列数。-C&lt;显示列数&gt;或-&lt;显示列数&gt; 除了显示符合范本样式的那一列之外，并显示该列之前后的内容。-d&lt;进行动作&gt; 当指定要查找的是目录而非文件时，必须使用这项参数，否则grep命令将回报信息并停止动作。-e&lt;范本样式&gt; 指定字符串作为查找文件内容的范本样式。-E 将范本样式为延伸的普通表示法来使用，意味着使用能使用扩展正则表达式。-f&lt;范本文件&gt; 指定范本文件，其内容有一个或多个范本样式，让grep查找符合范本条件的文件内容，格式为每一列的范本样式。-F 将范本样式视为固定字符串的列表。-G 将范本样式视为普通的表示法来使用。-h 在显示符合范本样式的那一列之前，不标示该列所属的文件名称。-H 在显示符合范本样式的那一列之前，标示该列的文件名称。-i 忽略字符大小写的差别。-l 列出文件内容符合指定的范本样式的文件名称。-L 列出文件内容不符合指定的范本样式的文件名称。-n 在显示符合范本样式的那一列之前，标示出该列的编号。-q 不显示任何信息。-R/-r 此参数的效果和指定“-d recurse”参数相同。-s 不显示错误信息。-v 反转查找。-w 只显示全字符合的列。-x 只显示全列符合的列。-y 此参数效果跟“-i”相同。-o 只输出文件中匹配到的部分。 示例 在文件中搜索一个单词，命令会返回一个包含“match_pattern”的文本行： 12$ grep match_pattern file_name$ grep "match_pattern" file_name 在多个文件中查找 1$ grep "match_pattern" file_1 file_2 file_3 ... 输出除之外的所有行 -v 选项 1$ grep -v "match_pattern" file_name 标记匹配颜色 –color=auto 选项 1$ grep "match_pattern" file_name --color=auto 使用正则表达式 -E 选项 12$ grep -E "[1-9]+"$ egrep "[1-9]+" 只输出文件中匹配到的部分 -o 选项 12$ echo this is a test line. | grep -o -E "[a-z]+\."$ echo this is a test line. | egrep -o "[a-z]+\." 统计文件或者文本中包含匹配字符串的行数 -c 选项 1$ grep -c "text" file_name 输出包含匹配字符串的行数 -n 选项 123456$ grep "text" -n file_name或$ cat file_name | grep "text" -n#多个文件$ grep "text" -n file_1 file_2 打印样式匹配所位于的字符或字节偏移 1234$ echo gun is not unix | grep -b -o "not"7:not#一行中字符串的字符便宜是从该行的第一个字符开始计算，起始值为0。选项 -b -o 一般总是配合使用。 搜索多个文件并查找匹配文本在哪些文件中 1$ grep -l "text" file1 file2 file3... 在多级目录中对文本进行递归搜索 12$ grep "text" . -r -n# .表示当前目录。 忽略匹配样式中的字符大小写 1$ echo "hello world" | grep -i "HELLO" 选项 -e 制动多个匹配样式 12345$ echo this is a text line | grep -e "is" -e "line" -o#也可以使用-f选项来匹配多个样式，在样式文件中逐行写出需要匹配的字符。$ echo aaa bbb ccc ddd eee | grep -f patfile -o 在grep搜索结果中包括或者排除指定文件 12345678#只在目录中所有的.php和.html文件中递归搜索字符"main()"$ grep "main()" . -r --include *.&#123;php,html&#125;#在搜索结果中排除所有README文件$ grep "main()" . -r --exclude "README"#在搜索结果中排除filelist文件列表里的文件$ grep "main()" . -r --exclude-from filelist 使用0值字节后缀的grep与xargs 12345678#测试文件：$ echo "aaa" &gt; file1$ echo "bbb" &gt; file2$ echo "aaa" &gt; file3$ grep "aaa" file* -lZ | xargs -0 rm#执行后会删除file1和file3，grep输出用-Z选项来指定以0值字节作为终结符文件名（\0），xargs -0 读取输入并用0值字节终结符分隔文件名，然后删除匹配文件，-Z通常和-l结合使用。 grep静默输出 123$ grep -q "test" filename#不会输出任何信息，如果命令运行成功返回0，失败则返回非0值。一般用于条件测试。 打印出匹配文本之前或者之后的行 1234567891011#显示匹配某个结果之后的3行，使用 -A 选项：$ seq 10 | grep "5" -A 3#显示匹配某个结果之前的3行，使用 -B 选项：$ seq 10 | grep "5" -B 3#显示匹配某个结果的前三行和后三行，使用 -C 选项：$ seq 10 | grep "5" -C 3#如果匹配结果有多个，会用“--”作为各匹配结果之间的分隔符：$ echo -e "a\nb\nc\na\nb\nc" | grep a -A 1]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>shell</tag>
        <tag>grep</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux设置全路径显示]]></title>
    <url>%2Flinux%2Flinux%2Fconfig-setallpath%2F</url>
    <content type="text"><![CDATA[在Linux中，编辑vi /etc/bashrc文件，搜索PS1=&quot;[\u@\h \W]，将大写的W改为w 修改前： 修改后: 效果显示:]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DF命令]]></title>
    <url>%2Flinux%2Flinux%2Fshell-df%2F</url>
    <content type="text"><![CDATA[简介df命令用于显示磁盘分区上的可使用的磁盘空间。默认显示单位为KB。可以利用该命令来获取硬盘被占用了多少空间，目前还剩下多少空间等信息。 参数说明12345678910111213141516-a或--all：包含全部的文件系统；--block-size=&lt;区块大小&gt;：以指定的区块大小来显示区块数目；-h或--human-readable：以可读性较高的方式来显示信息；-H或--si：与-h参数相同，但在计算时是以1000 Bytes为换算单位而非1024 Bytes；-i或--inodes：显示inode的信息；-k或--kilobytes：指定区块大小为1024字节；-l或--local：仅显示本地端的文件系统；-m或--megabytes：指定区块大小为1048576字节；--no-sync：在取得磁盘使用信息前，不要执行sync指令，此为预设值；-P或--portability：使用POSIX的输出格式；--sync：在取得磁盘使用信息前，先执行sync指令；-t&lt;文件系统类型&gt;或--type=&lt;文件系统类型&gt;：仅显示指定文件系统类型的磁盘信息；-T或--print-type：显示文件系统的类型；-x&lt;文件系统类型&gt;或--exclude-type=&lt;文件系统类型&gt;：不要显示指定文件系统类型的磁盘信息；--help：显示帮助；--version：显示版本信息。 示例查看系统磁盘设备，默认是KB为单位1$ df 结果显示： 字段说明：Filesystem: 文件系统1K-blocks: 1K-块Used: 已用Available: 可用Use%: 已用%Mounted on: 挂载点 使用-h选项以KB以上的单位来显示，可读性高1$ df -h 结果显示：]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>shell</tag>
        <tag>df</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Route命令]]></title>
    <url>%2Flinux%2Flinux%2Fshell-route%2F</url>
    <content type="text"><![CDATA[简介route命令用来显示并设置Linux内核中的网络路由表，route命令设置的路由主要是静态路由。要实现两个不同的子网之间的通信，需要一台连接两个网络的路由器，或者同时位于两个网络的网关来实现。 在Linux系统中设置路由通常是为了解决以下问题：该Linux系统在一个局域网中，局域网中有一个网关，能够让机器访问Internet，那么就需要将这台机器的ip地址设置为Linux机器的默认路由。要注意的是，直接在命令行下执行route命令来添加路由，不会永久保存，当网卡重启或者机器重启之后，该路由就失效了；可以在/etc/rc.local中添加route命令来保证该路由设置永久有效。 参数说明1234567-A：设置地址类型；-C：打印将Linux核心的路由缓存；-v：详细信息模式；-n：不执行DNS反向查找，直接显示数字形式的IP地址；-e：netstat格式显示路由表；-net：到一个网络的路由表；-host：到一个主机的路由表。 示例显示当前路由1$ route -n 结果显示： 字段说明： U Up表示此路由当前为启动状态。H Host，表示此网关为一主机。G Gateway，表示此网关为一路由器。R Reinstate Route，使用动态路由重新初始化的路由。D Dynamically,此路由是动态性地写入。M Modified，此路由是由路由守护程序或导向器动态修改。! 表示此路由当前为关闭状态。 添加网关/设置网关1$ route add -net 224.0.0.0 netmask 240.0.0.0 dev eth0 屏蔽一条路由1$ route add -net 224.0.0.0 netmask 240.0.0.0 reject 删除路由记录12$ route del -net 224.0.0.0 netmask 240.0.0.0$ route del -net 224.0.0.0 netmask 240.0.0.0 reject 删除和添加设置默认网关12$ route del default gw 192.168.120.240$ route add default gw 192.168.120.240]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>shell</tag>
        <tag>route</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tail命令]]></title>
    <url>%2Flinux%2Flinux%2Fshell-tail%2F</url>
    <content type="text"><![CDATA[简介tail命令用于输入文件中的尾部内容。tail命令默认在屏幕上显示指定文件的末尾10行。如果给定的文件不止一个，则在显示的每个文件前面加一个文件名标题。如果没有指定文件或者文件名为“-”，则读取标准输入。 注意：如果表示字节或行数的N值之前有一个”+”号，则从文件开头的第N项开始显示，而不是显示文件的最后N项。N值后面可以有后缀：b表示512，k表示1024，m表示1 048576(1M)。 参数说明1234567891011--retry：即是在tail命令启动时，文件不可访问或者文件稍后变得不可访问，都始终尝试打开文件。使用此选项时需要与选项“——follow=name”连用；-c&lt;N&gt;或——bytes=&lt;N&gt;：输出文件尾部的N（N为整数）个字节内容；-f&lt;name/descriptor&gt;或；--follow&lt;nameldescript&gt;：显示文件最新追加的内容。“name”表示以文件名的方式监视文件的变化。“-f”与“-fdescriptor”等效；-F：与选项“-follow=name”和“--retry"连用时功能相同；-n&lt;N&gt;或——line=&lt;N&gt;：输出文件的尾部N（N位数字）行内容。--pid=&lt;进程号&gt;：与“-f”选项连用，当指定的进程号的进程终止后，自动退出tail命令；-q或——quiet或——silent：当有多个文件参数时，不输出各个文件名；-s&lt;秒数&gt;或——sleep-interal=&lt;秒数&gt;：与“-f”选项连用，指定监视文件变化时间隔的秒数；-v或——verbose：当有多个文件参数时，总是输出各个文件名；--help：显示指令的帮助信息；--version：显示指令的版本信息。 示例显示文件末尾内容1$ tail -n 5 log2014.log 循环查看文件内容1$ tail -f test.log 从第5行开始显示文件1$ tail -n +5 log2014.log]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>shell</tag>
        <tag>tail</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LN命令]]></title>
    <url>%2Flinux%2Flinux%2Fshell-ln%2F</url>
    <content type="text"><![CDATA[简介ln命令用来为文件创件连接，连接类型分为硬连接和符号连接两种，默认的连接类型是硬连接。如果要创建符号连接必须使用-s选项。 注意：符号链接文件不是一个独立的文件，它的许多属性依赖于源文件，所以给符号链接文件设置存取权限是没有意义的。 参数说明1234567891011-b: 删除，覆盖以前建立的链接-d: 允许超级用户制作目录的硬链接-f: 强制执行-i: 交互模式，文件存在则提示用户是否覆盖-n: 把符号链接视为一般目录-s: 软链接(符号链接)-v: 显示详细的处理过程-S: “-S&lt;字尾备份字符串&gt; ”或 “--suffix=&lt;字尾备份字符串&gt;”-V: “-V&lt;备份方式&gt;”或“--version-control=&lt;备份方式&gt;”--help: 显示帮助信息--version: 显示版本信息 示例建立一个符号链接 在目录/usr/liu下建立一个符号链接文件abc，使它指向目录/usr/mengqc/mub1 1$ ln -s /usr/mengqc/mub1 /usr/liu/abc 删除一个符号链接 将目录/usr/liu/下的abc链接删除，注意不是rm -rf symbolic_name/ 12$ cd /usr/liu/$ rm -rf symbolic_name]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>shell</tag>
        <tag>ln</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LS命令]]></title>
    <url>%2Flinux%2Flinux%2Fshell-ls%2F</url>
    <content type="text"><![CDATA[简介ls命令用来显示目标列表，在Linux中是使用率较高的命令。ls命令的输出信息可以进行彩色加亮显示，以分区不同类型的文件。 参数说明12345678910111213141516171819202122-a：显示所有档案及目录（ls内定将档案名或目录名称为“.”的视为影藏，不会列出）；-A：显示除影藏文件“.”和“..”以外的所有文件列表；-C：多列显示输出结果。这是默认选项；-l：与“-C”选项功能相反，所有输出信息用单列格式输出，不输出为多列；-F：在每个输出项后追加文件的类型标识符，具体含义：“*”表示具有可执行权限的普通文件，“/”表示目录，“@”表示符号链接，“|”表示命令管道FIFO，“=”表示sockets套接字。当文件为普通文件时，不输出任何标识符；-b：将文件中的不可输出的字符以反斜线“”加字符编码的方式输出；-c：与“-lt”选项连用时，按照文件状态时间排序输出目录内容，排序的依据是文件的索引节点中的ctime字段。与“-l”选项连用时，则排序的一句是文件的状态改变时间；-d：仅显示目录名，而不显示目录下的内容列表。显示符号链接文件本身，而不显示其所指向的目录列表；-f：此参数的效果和同时指定“aU”参数相同，并关闭“lst”参数的效果；-i：显示文件索引节点号（inode）。一个索引节点代表一个文件；--file-type：与“-F”选项的功能相同，但是不显示“*”；-k：以KB（千字节）为单位显示文件大小；-l：以长格式显示目录下的内容列表。输出的信息从左到右依次包括文件名，文件类型、权限模式、硬连接数、所有者、组、文件大小和文件的最后修改时间等；-m：用“,”号区隔每个文件和目录的名称；-n：以用户识别码和群组识别码替代其名称；-r：以文件名反序排列并输出目录内容列表；-s：显示文件和目录的大小，以区块为单位；-t：用文件和目录的更改时间排序；-L：如果遇到性质为符号链接的文件或目录，直接列出该链接所指向的原始文件或目录；-R：递归处理，将指定目录下的所有文件及子目录一并处理；--full-time：列出完整的日期与时间；--color[=WHEN]：使用不同的颜色高亮显示不同类型的。 示例使用长清单模式1$ ls -l 显示文件大小1$ ls -lh 排序文件大小1$ ls -lhS 测量大小1$ ls -l --block-size=M 显示隐藏文件1$ ls -a 只列出目录条目1$ ls -d */ 不打印所有者信息1$ ls -g 不打印组信息1$ ls -lG 打印UID和GID1$ ls -n 不带颜色打印1$ ls --color=never 打印每个文件的索引号1$ ls -li 增加 / (斜线) 标记目录1$ ls -p 排序时反转顺序1$ ls -r 递归列出子目录1$ ls -R 扩展名排序1$ ls -lX 通过修改时间列出1$ ls -lt 列出你的主目录1$ ls ~ 打印ls命令版本1$ ls --version]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>shell</tag>
        <tag>ls</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP响应头和请求头信息对照表]]></title>
    <url>%2Fhttp%2Fhttp%2Fheader%2F</url>
    <content type="text"><![CDATA[简介HTTP请求头提供了关于请求，响应或者其他的发送实体的信息。HTTP的头信息包括通用头、请求头、响应头和实体头四个部分。每个头域由一个域名，冒号（:）和域值三部分组成。 通用头标：即可用于请求，也可用于响应，是作为一个整体而不是特定资源与事务相关联。 请求头标：允许客户端传递关于自身的信息和希望的响应形式。 响应头标：服务器和于传递自身信息的响应。 实体头标：定义被传送资源的信息。即可用于请求，也可用于响应。 根据以上分类的HTTP请求头介绍可以参考此文，本工具根据请求和输出分为Request和Response两部分。 HTTP Request Header Header 解释 示例 Accept 指定客户端能够接收的内容类型 Accept: text/html Accept-Charset 浏览器可以接受的字符编码集。 Accept-Charset: iso-8859-5 Accept-Encoding 指定浏览器可以支持的web服务器返回内容压缩编码类型。 Accept-Encoding: gzip, deflate Accept-Language 浏览器可接受的语言 Accept-Language: en,zh Accept-Ranges 可以请求网页实体的一个或者多个子范围字段 Accept-Ranges: bytes Authorization HTTP授权的授权证书 Authorization: Basic QWxhZGRpbjpvcGVuIHNlc2FtZQ== Cache-Control 指定请求和响应遵循的缓存机制 Cache-Control: no-cache Connection 表示是否需要持久连接。（HTTP 1.1默认进行持久连接） Connection: keep-alive Cookie HTTP请求发送时，会把保存在该请求域名下的所有cookie值一起发送给web服务器。 Cookie: admckid=1607301350151121825; Content-Length 请求的内容长度 Content-Length: 348 Content-Type 请求的与实体对应的MIME信息 Content-Type: application/x-www-form-urlencoded Date 请求发送的日期和时间 Date: Tue, 15 Nov 2010 08:12:31 GMT Expect 请求的特定的服务器行为 Expect: 100-continue From 发出请求的用户的Email From: user@email.com Host 指定请求的服务器的域名和端口号 c.xxx.com.cn If-Match 只有请求内容与实体相匹配才有效 If-Match: “737060cd8c284d8af7ad3082f209582d” If-Modified-Since 如果请求的部分在指定时间之后被修改则请求成功，未被修改则返回304代码 If-Modified-Since: Sat, 29 Oct 2010 19:43:31 GMT If-None-Match 如果内容未改变返回304代码，参数为服务器先前发送的Etag，与服务器回应的Etag比较判断是否改变 If-None-Match: “737060cd8c284d8af7ad3082f209582d” If-Range 如果实体未改变，服务器发送客户端丢失的部分，否则发送整个实体。参数也为Etag If-Range: “737060cd8c284d8af7ad3082f209582d” If-Unmodified-Since 只在实体在指定时间之后未被修改才请求成功 If-Unmodified-Since: Sat, 29 Oct 2010 19:43:31 GMT Max-Forwards 限制信息通过代理和网关传送的时间 Max-Forwards: 10 Pragma 用来包含实现特定的指令 Pragma: no-cache Proxy-Authorization 连接到代理的授权证书 Proxy-Authorization: Basic QWxhZGRpbjpvcGVuIHNlc2FtZQ== Range 只请求实体的一部分，指定范围 Range: bytes=500-999 Referer 先前网页的地址，当前请求网页紧随其后,即来路 Referer: http://www.iqiyi.com/a_19rrhahmi9.html TE 客户端愿意接受的传输编码，并通知服务器接受接受尾加头信息 TE: trailers,deflate;q=0.5 Upgrade 向服务器指定某种传输协议以便服务器进行转换（如果支持） Upgrade: HTTP/2.0, SHTTP/1.3, IRC/6.9, RTA/x11 User-Agent 内容包含发出请求的用户信息 User-Agent: Mozilla/5.0 Via 通知中间网关或代理服务器地址，通信协议 Via: 1.0 fred, 1.1 nowhere.com (Apache/1.1) Warning 关于消息实体的警告信息 Warn: 199 Miscellaneous warning HTTP Responses Header Header 解释 示例 Accept-Ranges 表明服务器是否支持指定范围请求及哪种类型的分段请求 Accept-Ranges: bytes Age 从原始服务器到代理缓存形成的估算时间（以秒计，非负） Age: 12 Allow 对某网络资源的有效的请求行为，不允许则返回405 Allow: GET, HEAD Cache-Control 告诉所有的缓存机制是否可以缓存及哪种类型 Cache-Control: max-age=31536000 Content-Encoding web服务器支持的返回内容压缩编码类型。 Content-Encoding: gzip Content-Language 响应体的语言 Content-Language: en,zh Content-Length 响应体的长度 Content-Length: 348 Content-Location 请求资源可替代的备用的另一地址 Content-Location: /index.htm Content-MD5 返回资源的MD5校验值 Content-MD5: Q2hlY2sgSW50ZWdyaXR5IQ== Content-Range 在整个返回体中本部分的字节位置 Content-Range: bytes 21010-47021/47022 Content-Type 返回内容的MIME类型 Content-Type: application/octet-stream Date 原始服务器消息发出的时间 Mon, 15 Aug 2016 02:43:15 GMT ETag 请求变量的实体标签的当前值 ETag: “737060cd8c284d8af7ad3082f209582d” Expires 响应过期的日期和时间 Expires: Tue, 15 Aug 2017 02:42:58 GMT Last-Modified 请求资源的最后修改时间 Last-Modified: Fri, 29 Jul 2016 16:26:41 GMT Location 用来重定向接收方到非请求URL的位置来完成请求或标识新的资源 Location: http://www.xxx.com/act/RdFHgKt4WyNYv27z.html P3P P3P提供的个人隐私保护策略 P3P: CP=CURa ADMa DEVa PSAo PSDo OUR BUS UNI PUR INT DEM STA PRE COM NAV OTC NOI DSP COR Pragma 包括实现特定的指令，它可应用到响应链上的任何接收方 Pragma: no-cache Proxy-Authenticate 它指出认证方案和可应用到代理的该URL上的参数 Proxy-Authenticate: Basic refresh 应用于重定向或一个新的资源被创造，在5秒之后重定向（由网景提出，被大部分浏览器支持） Refresh: http://www.xxx.com/ Retry-After 如果实体暂时不可取，通知客户端在指定时间之后再次尝试 Retry-After: 120 Server web服务器软件名称 Server: nginx/7ed94dd662ffeb274371f6f0438ac6587fed8d84 Set-Cookie 设置Http Cookie Set-Cookie: UserID=JohnDoe; Max-Age=3600; Version=1 Trailer 指出头域在分块传输编码的尾部存在 Trailer: Max-Forwards Transfer-Encoding 文件传输编码 Transfer-Encoding:chunked Vary 告诉下游代理是使用缓存响应还是从原始服务器请求 Vary: * Via 告知代理客户端响应是通过哪里发送的 Via: 1.0 fred, 1.1 nowhere.com (Apache/1.1) Warning 警告实体可能存在的问题 Warning: 199 Miscellaneous warning WWW-Authenticate 表明客户端请求实体应该使用的授权方案 WWW-Authenticate: Basic]]></content>
      <categories>
        <category>http</category>
      </categories>
      <tags>
        <tag>http</tag>
        <tag>协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MIME类型列表]]></title>
    <url>%2Fhttp%2Fhttp%2Fmime%2F</url>
    <content type="text"><![CDATA[简介MIME (Multipurpose Internet Mail Extensions) 是描述消息内容类型的因特网标准。MIME 消息能包含文本、图像、音频、视频以及其他应用程序专用的数据。官方的 MIME 信息是由 Internet Engineering Task Force (IETF) 在下面的文档中提供的： RFC-822 Standard for ARPA Internet text messagesRFC-2045 MIME Part 1: Format of Internet Message BodiesRFC-2046 MIME Part 2: Media TypesRFC-2047 MIME Part 3: Header Extensions for Non-ASCII TextRFC-2048 MIME Part 4: Registration ProceduresRFC-2049 MIME Part 5: Conformance Criteria and Examples 不同的应用程序支持不同的 MIME 类型。下面的参考手册是由 Microsoft Internet Information Server version 5 所支持的 MIME 类型列表。 按照内容类型排列的 Mime 类型列表 类型/子类型 扩展名 application/envoy evy application/fractals fif application/futuresplash spl application/hta hta application/internet-property-stream acx application/mac-binhex40 hqx application/msword doc application/msword dot application/octet-stream * application/octet-stream bin application/octet-stream class application/octet-stream dms application/octet-stream exe application/octet-stream lha application/octet-stream lzh application/oda oda application/olescript axs application/pdf pdf application/pics-rules prf application/pkcs10 p10 application/pkix-crl crl application/postscript ai application/postscript eps application/postscript ps application/rtf rtf application/set-payment-initiation setpay application/set-registration-initiation setreg application/vnd.ms-excel xla application/vnd.ms-excel xlc application/vnd.ms-excel xlm application/vnd.ms-excel xls application/vnd.ms-excel xlt application/vnd.ms-excel xlw application/vnd.ms-outlook msg application/vnd.ms-pkicertstore sst application/vnd.ms-pkiseccat cat application/vnd.ms-pkistl stl application/vnd.ms-powerpoint pot application/vnd.ms-powerpoint pps application/vnd.ms-powerpoint ppt application/vnd.ms-project mpp application/vnd.ms-works wcm application/vnd.ms-works wdb application/vnd.ms-works wks application/vnd.ms-works wps application/winhlp hlp application/x-bcpio bcpio application/x-cdf cdf application/x-compress z application/x-compressed tgz application/x-cpio cpio application/x-csh csh application/x-director dcr application/x-director dir application/x-director dxr application/x-dvi dvi application/x-gtar gtar application/x-gzip gz application/x-hdf hdf application/x-internet-signup ins application/x-internet-signup isp application/x-iphone iii application/x-javascript js application/x-latex latex application/x-msaccess mdb application/x-mscardfile crd application/x-msclip clp application/x-msdownload dll application/x-msmediaview m13 application/x-msmediaview m14 application/x-msmediaview mvb application/x-msmetafile wmf application/x-msmoney mny application/x-mspublisher pub application/x-msschedule scd application/x-msterminal trm application/x-mswrite wri application/x-netcdf cdf application/x-netcdf nc application/x-perfmon pma application/x-perfmon pmc application/x-perfmon pml application/x-perfmon pmr application/x-perfmon pmw application/x-pkcs12 p12 application/x-pkcs12 pfx application/x-pkcs7-certificates p7b application/x-pkcs7-certificates spc application/x-pkcs7-certreqresp p7r application/x-pkcs7-mime p7c application/x-pkcs7-mime p7m application/x-pkcs7-signature p7s application/x-sh sh application/x-shar shar application/x-shockwave-flash swf application/x-stuffit sit application/x-sv4cpio sv4cpio application/x-sv4crc sv4crc application/x-tar tar application/x-tcl tcl application/x-tex tex application/x-texinfo texi application/x-texinfo texinfo application/x-troff roff application/x-troff t application/x-troff tr application/x-troff-man man application/x-troff-me me application/x-troff-ms ms application/x-ustar ustar application/x-wais-source src application/x-x509-ca-cert cer application/x-x509-ca-cert crt application/x-x509-ca-cert der application/ynd.ms-pkipko pko application/zip zip audio/basic au audio/basic snd audio/mid mid audio/mid rmi audio/mpeg mp3 audio/x-aiff aif audio/x-aiff aifc audio/x-aiff aiff audio/x-mpegurl m3u audio/x-pn-realaudio ra audio/x-pn-realaudio ram audio/x-wav wav image/bmp bmp image/cis-cod cod image/gif gif image/ief ief image/jpeg jpe image/jpeg jpeg image/jpeg jpg image/pipeg jfif image/svg+xml svg image/tiff tif image/tiff tiff image/x-cmu-raster ras image/x-cmx cmx image/x-icon ico image/x-portable-anymap pnm image/x-portable-bitmap pbm image/x-portable-graymap pgm image/x-portable-pixmap ppm image/x-rgb rgb image/x-xbitmap xbm image/x-xpixmap xpm image/x-xwindowdump xwd message/rfc822 mht message/rfc822 mhtml message/rfc822 nws text/css css text/h323 323 text/html htm text/html html text/html stm text/iuls uls text/plain bas text/plain c text/plain h text/plain txt text/richtext rtx text/scriptlet sct text/tab-separated-values tsv text/webviewhtml htt text/x-component htc text/x-setext etx text/x-vcard vcf video/mpeg mp2 video/mpeg mpa video/mpeg mpe video/mpeg mpeg video/mpeg mpg video/mpeg mpv2 video/quicktime mov video/quicktime qt video/x-la-asf lsf video/x-la-asf lsx video/x-ms-asf asf video/x-ms-asf asr video/x-ms-asf asx video/x-msvideo avi video/x-sgi-movie movie x-world/x-vrml flr x-world/x-vrml vrml x-world/x-vrml wrl x-world/x-vrml wrz x-world/x-vrml xaf x-world/x-vrml xof]]></content>
      <categories>
        <category>http</category>
      </categories>
      <tags>
        <tag>http</tag>
        <tag>协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PS命令]]></title>
    <url>%2Flinux%2Flinux%2Fshell-ps%2F</url>
    <content type="text"><![CDATA[简介要对进程进行监测和控制,首先必须要了解当前进程的情况,也就是需要查看当前进程,而ps命令就是最基本同时也是非常强大的进程查看命令.使用该命令可以确定有哪些进程正在运行和运行的状态、进程是否结束、进程有没有僵尸、哪些进程占用了过多的资源等等.总之大部分信息都是可以通过执行该命令得到的. 参数说明12345678-a：显示现行终端机下的所有进程，包括其他用户的进程-A：所有的进程均显示出来，与 -e 具有同样的效用-u：以用户为主的进程状态-f：做一个更为完整的输出-e：等于“-A”x： 通常与 a 这个参数一起使用，可列出较完整信息l： 较长、较详细的将该 PID 的的信息列出j： 工作的格式 (jobs format) PS L字段说明 F ：代表这个程序的旗标 (flag)， 4 代表使用者为 super userS ：代表这个程序的状态 (STAT)，关于各 STAT 的意义将在内文介绍UID ：程序被该 UID 所拥有PID ：就是这个程序的 ID ！PPID ：则是其上级父程序的IDC ：CPU 使用的资源百分比PRI ：这个是 Priority (优先执行序) 的缩写，详细后面介绍NI ：这个是 Nice 值，在下一小节我们会持续介绍ADDR ：这个是 kernel function，指出该程序在内存的那个部分。如果是个 running的程序，一般就是 “-“SZ ：使用掉的内存大小WCHAN ：目前这个程序是否正在运作当中，若为 - 表示正在运作TTY ：登入者的终端机位置TIME ：使用掉的 CPU 时间。CMD ：所下达的指令为何 PS AUX字段说明 USER：该进程属于那个使用者账号的？PID ：该进程的进程ID号。%CPU：该进程使用掉的 CPU 资源百分比；%MEM：该进程所占用的物理内存百分比；VSZ ：该进程使用掉的虚拟内存量 (Kbytes)RSS ：该进程占用的固定的内存量 (Kbytes)TTY ：该进程是在那个终端机上面运作，若与终端机无关，则显示 ?，另外， tty1-tty6 是本机上面的登入者程序，若为 pts/0 等等的，则表示为由网络连接进主机的程序。START：该进程被触发启动的时间；TIME ：该进程实际使用 CPU 运作的时间。COMMAND：该程序的实际指令为什么？ STAT：该程序目前的状态，主要的状态有： R ：该程序目前正在运作，或者是可被运作； S ：该程序目前正在睡眠当中 (可说是 idle 状态啦！)，但可被某些讯号(signal) 唤醒。 T ：该程序目前正在侦测或者是停止了； Z ：该程序应该已经终止，但是其父程序却无法正常的终止他，造成 zombie (疆尸) 程序的状态 示例显示所有进程信息1$ ps -A 显示指定用户信息1$ ps -u root 显示所有进程信息，连同命令行1$ ps -ef ps 与grep 常用组合用法，查找特定进程1$ ps -ef|grep ssh 列出类似程序树的程序显示1$ ps -axjf 找出与 cron 与 syslog 这两个服务有关的 PID 号码1$ ps aux | egrep '(cron|syslog)']]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>shell</tag>
        <tag>ps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WGET命令]]></title>
    <url>%2Flinux%2Flinux%2Fshell-wget%2F</url>
    <content type="text"><![CDATA[简介wget命令用来从指定的URL下载文件。wget非常稳定，它在带宽很窄的情况下和不稳定网络中有很强的适应性，如果是由于网络的原因下载失败，wget会不断的尝试，直到整个文件下载完毕。如果是服务器打断下载过程，它会再次联到服务器上从停止的地方继续下载。这对从那些限定了链接时间的服务器上下载大文件非常有用。 参数说明12345678910111213141516171819202122-a&lt;日志文件&gt;：在指定的日志文件中记录资料的执行过程；-A&lt;后缀名&gt;：指定要下载文件的后缀名，多个后缀名之间使用逗号进行分隔；-b：进行后台的方式运行wget；-B&lt;连接地址&gt;：设置参考的连接地址的基地地址；-c：继续执行上次终端的任务；-C&lt;标志&gt;：设置服务器数据块功能标志on为激活，off为关闭，默认值为on；-d：调试模式运行指令；-D&lt;域名列表&gt;：设置顺着的域名列表，域名之间用“，”分隔；-e&lt;指令&gt;：作为文件“.wgetrc”中的一部分执行指定的指令；-h：显示指令帮助信息；-i&lt;文件&gt;：从指定文件获取要下载的URL地址；-l&lt;目录列表&gt;：设置顺着的目录列表，多个目录用“，”分隔；-L：仅顺着关联的连接；-r：递归下载方式；-nc：文件存在时，下载文件不覆盖原有文件；-nv：下载时只显示更新和出错信息，不显示指令的详细执行过程；-q：不显示指令执行过程；-nh：不查询主机名称；-v：显示详细执行过程；-V：显示版本信息；--passive-ftp：使用被动模式PASV连接FTP服务器；--follow-ftp：从HTML文件中下载FTP连接文件。 示例下载单个文件 以下的例子是从网络下载一个文件并保存在当前目录，在下载的过程中会显示进度条，包含（下载完成百分比，已经下载的字节，当前下载速度，剩余下载时间）。 1$ wget http://www.xxx.net/xxx.zip 下载并以不同的文件名保存 wget默认会以最后一个符合/的后面的字符来命令，对于动态链接的下载通常文件名会不正确。 1$ wget -O wordpress.zip http://www.xxx.net/xxx.aspx?id=1080 wget限速下载 当你执行wget的时候，它默认会占用全部可能的宽带下载。但是当你准备下载一个大文件，而你还需要下载其它文件时就有必要限速了。 1$ wget --limit-rate=300k http://www.xxx.net/testfile.zip 使用wget断点续传 使用wget -c重新启动下载中断的文件，对于我们下载大文件时突然由于网络等原因中断非常有帮助，我们可以继续接着下载而不是重新下载一个文件。需要继续中断的下载时可以使用-c参数。 1$ wget -c http://www.xxx.net/testfile.zip 使用wget后台下载1$ wget -b http://www.xxx.net/testfile.zip 伪装代理名称下载1$ wget --user-agent="Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.204 Safari/534.16" http://www.xxx.net/testfile.zip 测试下载链接 当你打算进行定时下载，你应该在预定时间测试下载链接是否有效。我们可以增加–spider参数进行检查。 1$ wget --spider URL 增加重试次数 如果网络有问题或下载一个大文件也有可能失败。wget默认重试20次连接下载文件。如果需要，你可以使用–tries增加重试次数。 1$ wget --tries=40 URL 下载多个文件 首先，保存一份下载链接文件：cat &gt; filelist.txturl1url2url3url4接着使用这个文件和参数-i下载。 1$ wget -i filelist.txt 镜像网站 下载整个网站到本地。–miror开户镜像下载。-p下载所有为了html页面显示正常的文件。–convert-links下载后，转换成本地的链接。-P ./LOCAL保存所有文件和目录到本地指定目录。 1$ wget --mirror -p --convert-links -P ./LOCAL URL 过滤指定格式下载 下载一个网站，但你不希望下载图片，可以使用这条命令。 1$ wget --reject=gif ur 把下载信息存入日志文件 不希望下载信息直接显示在终端而是在一个日志文件，可以使用。 1$ wget -o download.log URL 限制总下载文件大小 当你想要下载的文件超过5M而退出下载，你可以使用。注意：这个参数对单个文件下载不起作用，只能递归下载时才有效。 1$ wget -Q5m -i filelist.txt 下载指定格式文件 可以在以下情况使用该功能：下载一个网站的所有图片。下载一个网站的所有视频。下载一个网站的所有PDF文件。 1$ wget -r -A.pdf url FTP下载12$ wget ftp-url$ wget --ftp-user=USERNAME --ftp-password=PASSWORD url Https下载1$ wget -r -np -nd --accept=gz --no-check-certificate https://www.xxx.com/dir/ --http-user=username --http-password=password]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>shell</tag>
        <tag>wget</tag>
      </tags>
  </entry>
</search>
